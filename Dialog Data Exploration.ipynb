{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, nltk, random, os, json\n",
    "from nltk.corpus import stopwords\n",
    "from rank_bm25 import BM25Okapi, BM25L, BM25Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 7537684720 acquired on /Users/jarana/.cache/torch/transformers/e88f38f2c8bc669ef7873de68f36bf764d4f64b9833ca8401efe271aab476745.0f15800a5b4c30725c555e054e3d0262e9916635f0de9d397c30acd86c21dc73.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bd52e81f9046c9a1e55b1f078d4c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=451.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 7537684720 released on /Users/jarana/.cache/torch/transformers/e88f38f2c8bc669ef7873de68f36bf764d4f64b9833ca8401efe271aab476745.0f15800a5b4c30725c555e054e3d0262e9916635f0de9d397c30acd86c21dc73.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.question_answering import QuestionAnsweringModel\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Create the QuestionAnsweringModel\n",
    "model = QuestionAnsweringModel('distilbert', 'distilbert-base-uncased-distilled-squad', use_cuda=False, args={'reprocess_input_data': True, 'overwrite_output_dir': True, 'use_cuda': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 0\n",
    "# to_predict = []\n",
    "# for context in context_sentences:\n",
    "#     qas = []\n",
    "#     for prop in cand_properties:\n",
    "#         qas.append({'question': cand_properties[prop], 'id': idx})\n",
    "#         idx += 1\n",
    "#     to_predict.append({'context': context, 'qas':qas})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "regex = re.compile('[^a-zA-Z0-9.,]')\n",
    "#First parameter is the replacement, second parameter is your input string\n",
    "def preprocessingText(doc,enableStopword=False):\n",
    "#     doc = regex.sub(' ', doc)\n",
    "    if enableStopword:\n",
    "        doc = \" \".join([w for w in doc.split() if not w in stop_words])\n",
    "    return doc.lower().replace(\"\\n\",\"\")\n",
    "\n",
    "df_wiki = pd.read_csv(\"data/wikihowSep.csv\")\n",
    "df_wiki['title'] = df_wiki['title'].str.replace(\"How to \", \"\")\n",
    "for col in ['overview', 'headline', 'text', 'sectionLabel', 'title']:\n",
    "    df_wiki[col] = [preprocessingText(str(i), False) for i in df_wiki[col]]\n",
    "    \n",
    "df_wiki['title'] = [i if not i[-1].isdigit() else i[:-1] for i in df_wiki['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/jarana/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, re, math\n",
    "\n",
    "def camel_case_split(str): \n",
    "    words = [[str[0]]] \n",
    "    for c in str[1:]: \n",
    "        if words[-1][-1].islower() and c.isupper(): \n",
    "            words.append(list(c)) \n",
    "        else: \n",
    "            words[-1].append(c) \n",
    "\n",
    "    return \" \".join([i.lower() for i in [''.join(word) for word in words]])    \n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext.lower()\n",
    "\n",
    "def simplifyDomainName(oldName):\n",
    "    special = {\"buse\": \"bus\", \"rentalcar\": \"rent car\", \"ridesharing\": \"taxi\", \"messaging\": \"message\"}\n",
    "    newName = oldName.split(\"_\")[0].lower()\n",
    "    if newName[-1] == \"s\":\n",
    "        newName = newName[:-1]\n",
    "    if newName in special:\n",
    "        newName = special[newName]\n",
    "    return newName\n",
    "\n",
    "schema = pd.read_json(\"data/dialog/dstc8-schema-guided-dialogue/train/schema.json\")\n",
    "schema = pd.concat([schema, pd.read_json(\"data/dialog/dstc8-schema-guided-dialogue/dev/schema.json\")])\n",
    "schema = pd.concat([schema, pd.read_json(\"data/dialog/dstc8-schema-guided-dialogue/test/schema.json\")])\n",
    "\n",
    "schema['service_name'] = [simplifyDomainName(i) for i in schema.service_name.tolist()]\n",
    "domains = schema.service_name.unique().tolist()\n",
    "\n",
    "orgType = pd.read_csv(\"data/dialog/all-layers-types.csv\")\n",
    "orgProp = pd.read_csv(\"data/dialog/all-layers-properties.csv\")\n",
    "\n",
    "schemaCorpus = [camel_case_split(i[0]) + \" #SEP# \" + cleanhtml(i[1]).lower() for i in orgType[['label', 'comment']].values]\n",
    "bm25s = BM25L([doc.split() for doc in schemaCorpus])\n",
    "\n",
    "sProp2Desc = {camel_case_split(i[0]):cleanhtml(i[1]).lower() for i in np.concatenate([orgProp[['label', 'comment']].values, orgType[['label', 'comment']].values])}\n",
    "sType2Prop = {}\n",
    "sType2Subtype = {}\n",
    "for i,j,k in orgType[['label', 'subTypeOf', 'properties']].values:\n",
    "    if type(j) == str:\n",
    "        sType2Subtype[camel_case_split(i)] = [camel_case_split(z) for z in j.replace(\"http://schema.org/\", \"\").split(\", \")]\n",
    "                           \n",
    "    if type(k) == str:\n",
    "        sType2Prop[camel_case_split(i)] = [camel_case_split(z) for z in k.replace(\"http://schema.org/\", \"\").split(\", \")]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "doc2title = {}\n",
    "for name, group in df_wiki.groupby(\"title\"):\n",
    "    doc = []\n",
    "    for col in [\"overview\", \"sectionLabel\", \"headline\", \"text\"]:\n",
    "        doc.append(\" \".join(group[col].unique().tolist()))\n",
    "        \n",
    "    doc = name+\" #SEP# \"+ \" \".join(doc)\n",
    "    corpus.append(doc)\n",
    "    doc2title[doc] = name\n",
    "bm25w = BM25L([doc.split() for doc in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distant\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "unknown_services = ['alarm', 'weather']\n",
    "matches = []\n",
    "retrieved_properties = []\n",
    "for service, desc in schema[['service_name', 'description']].values:\n",
    "#   ignore weather and alarm\n",
    "    if service in unknown_services:\n",
    "        continue\n",
    "        \n",
    "    types = [i.split(\" #SEP# \")[0] for i in bm25s.get_top_n(service.split(), schemaCorpus, n=5)]\n",
    "    wiki2desc = dict(i.split(\" #SEP# \") for i in bm25w.get_top_n(preprocessingText(desc).split(), corpus, n=5))\n",
    "\n",
    "    #   get subtypes\n",
    "    subtypes = []\n",
    "    for t in types:\n",
    "        if t in sType2Subtype:\n",
    "            subtypes.extend(sType2Subtype[t])\n",
    "    subtypes = list(set(subtypes))\n",
    "    \n",
    "    cand_props = []\n",
    "    for t in types:\n",
    "        if t in sType2Prop:\n",
    "            cand_props.extend(sType2Prop[t])\n",
    "            retrieved_properties.extend(sType2Prop[t])\n",
    "    cand_props += types\n",
    "    \n",
    "    seen_prop = [\"\"]\n",
    "    for prop in cand_props:\n",
    "        if prop in schema.service_name.unique().tolist():\n",
    "            continue\n",
    "        propLast = prop.split()[-1]\n",
    "        #   check if nous\n",
    "        tokenized = nltk.word_tokenize(propLast)\n",
    "        nouns = [word  for (word, pos) in nltk.pos_tag(tokenized) if(pos[:2] == 'NN')]\n",
    "        if len(nouns) == 0:\n",
    "            propLast = \"\"\n",
    "            \n",
    "        for wiki in wiki2desc:\n",
    "            for sentence in wiki2desc[wiki].split(\". \"):\n",
    "                found_exact_match = False\n",
    "                exact_match_start = sentence.find(prop)\n",
    "                if exact_match_start > -1:\n",
    "                    matches.append([sProp2Desc[prop], prop, exact_match_start, sentence])\n",
    "                    found_exact_match = True\n",
    "                    \n",
    "                if propLast not in seen_prop and not found_exact_match:\n",
    "\n",
    "                    exact_match_start = sentence.find(propLast)\n",
    "                    if exact_match_start > -1:\n",
    "                        matches.append([sProp2Desc[prop], propLast, exact_match_start, sentence])\n",
    "        seen_prop.append(propLast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "trainData = []\n",
    "for i in matches:\n",
    "    question, text, answer_start, context = i\n",
    "    neg_question = random.choice(list(sProp2Desc.values()))\n",
    "    while neg_question == question:\n",
    "        neg_question = random.choice(list(sProp2Desc.values()))\n",
    "                        \n",
    "    tmp = {\"context\": context, 'qas':[{'id':str(idx+1), 'is_impossible': True, 'question': neg_question, 'answers':[]},{'id':str(idx), 'is_impossible': False, 'question': question, 'answers':[{'text': i[1], 'answer_start': i[2]}]}]}\n",
    "    trainData.append(tmp)\n",
    "    idx += 2\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "with open('data/dialog/distantWithNegativeTrain.json', 'w') as f:\n",
    "    json.dump(trainData, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train_model('data/dialog/distantWithNegativeTrain.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue_id</th>\n",
       "      <th>services</th>\n",
       "      <th>turns</th>\n",
       "      <th>service_name</th>\n",
       "      <th>description</th>\n",
       "      <th>slots</th>\n",
       "      <th>intents</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1100000.0</td>\n",
       "      <td>[Events_2]</td>\n",
       "      <td>[{'frames': [{'service': 'Events_2', 'slots': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Events_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1100001.0</td>\n",
       "      <td>[Events_2]</td>\n",
       "      <td>[{'frames': [{'service': 'Events_2', 'slots': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Events_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1100002.0</td>\n",
       "      <td>[Events_2]</td>\n",
       "      <td>[{'frames': [{'service': 'Events_2', 'slots': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Events_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1100003.0</td>\n",
       "      <td>[Events_2]</td>\n",
       "      <td>[{'frames': [{'service': 'Events_2', 'slots': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Events_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1100004.0</td>\n",
       "      <td>[Events_2]</td>\n",
       "      <td>[{'frames': [{'service': 'Events_2', 'slots': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Events_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dialogue_id    services                                              turns  \\\n",
       "0    1100000.0  [Events_2]  [{'frames': [{'service': 'Events_2', 'slots': ...   \n",
       "1    1100001.0  [Events_2]  [{'frames': [{'service': 'Events_2', 'slots': ...   \n",
       "2    1100002.0  [Events_2]  [{'frames': [{'service': 'Events_2', 'slots': ...   \n",
       "3    1100003.0  [Events_2]  [{'frames': [{'service': 'Events_2', 'slots': ...   \n",
       "4    1100004.0  [Events_2]  [{'frames': [{'service': 'Events_2', 'slots': ...   \n",
       "\n",
       "  service_name description slots intents    domain  \n",
       "0          NaN         NaN   NaN     NaN  Events_2  \n",
       "1          NaN         NaN   NaN     NaN  Events_2  \n",
       "2          NaN         NaN   NaN     NaN  Events_2  \n",
       "3          NaN         NaN   NaN     NaN  Events_2  \n",
       "4          NaN         NaN   NaN     NaN  Events_2  "
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_domain_dialogs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dstc8-schema-guided-dialogue\n",
    "# Train and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import glob\n",
    "\n",
    "# read dialog datasets\n",
    "txtfiles = []\n",
    "dialog = None\n",
    "for i in ['train', 'dev', 'test']:\n",
    "    for file in glob.glob(\"data/dialog/dstc8-schema-guided-dialogue/%s/*.json\" % i):\n",
    "        if \"schema.json\" in file:\n",
    "            continue\n",
    "        txtfiles.append(file)\n",
    "        dialog = pd.concat([dialog, pd.read_json(file)])\n",
    "\n",
    "# # extract single domains\n",
    "unique_single_domains = list(set([simplifyDomainName(i[0]) for i in dialog[dialog.services.str.len() == 1]['services']]))\n",
    "# split domains to train and test sets\n",
    "trainIds, testIds = train_test_split(list(set(unique_single_domains)), test_size=0.5, random_state=2020)\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "prop2desc = {}\n",
    "schemaT = pd.read_json(\"data/dialog/dstc8-schema-guided-dialogue/train/schema.json\")\n",
    "schemaT = pd.concat([schemaT, pd.read_json(\"data/dialog/dstc8-schema-guided-dialogue/dev/schema.json\")])\n",
    "schemaT = pd.concat([schemaT, pd.read_json(\"data/dialog/dstc8-schema-guided-dialogue/test/schema.json\")])\n",
    "for idx, s in schemaT.iterrows():\n",
    "    service = s.service_name\n",
    "    for i in s.slots:\n",
    "        prop2desc[service+\"_\"+i['name']] = i['description']\n",
    "\n",
    "\n",
    "single_domain_dialogs = dialog[dialog.services.str.len() == 1]\n",
    "# single_domain_dialogs['domain'] = [i[0] for i in single_domain_dialogs['services']]\n",
    "\n",
    "qid = 1\n",
    "# for idx, row in dialog.iterrows():\n",
    "# for idx, row in tmp.groupby('domain',as_index = False).apply(lambda x: x.sample(20)).iterrows():\n",
    "for idx, row in single_domain_dialogs.iterrows():\n",
    "    frames = row['turns']\n",
    "    context = []\n",
    "    \n",
    "    if str(frames)=='nan':\n",
    "        continue\n",
    "        \n",
    "    for f in frames:\n",
    "        service = f['frames'][0]['service']\n",
    "#         service = service.split(\"_\")[0].lower()\n",
    "        if len(f['frames']) == 1:\n",
    "            utterance = f['utterance']\n",
    "            if len(f['frames'][0]['slots']) != 0:\n",
    "                terms = []\n",
    "                for s in f['frames'][0]['slots']:\n",
    "                    \n",
    "#                   replace text with slot name\n",
    "#                     _context = \" \".join(context)\n",
    "#                     answer_start = len(_context) + s[\"start\"] + 1\n",
    "#                     new_answer = utterance[:s[\"start\"]] + s['slot'].replace(\"_\", \" \") + utterance[s[\"exclusive_end\"]+1:]\n",
    "#                     _context += \" \" + new_answer\n",
    "#                     _json = {'context': _context, 'qas':[{'id': str(qid), 'is_impossible': False, \n",
    "#                                                           'question': prop2desc[service+\"_\"+s['slot']], 'answers':[{'text':s['slot'].replace(\"_\",\" \"), \n",
    "#                                                            'answer_start': answer_start}]}]}\n",
    "                \n",
    "                    _context = \" \".join(context)\n",
    "                    answer_start = len(_context) + s[\"start\"] + 1\n",
    "#                     new_answer = utterance[:s[\"start\"]] + s['slot'].replace(\"_\", \" \") + utterance[s[\"exclusive_end\"]+1:]\n",
    "                    _context += \" \" + utterance\n",
    "                    question = prop2desc[service+\"_\"+s['slot']]\n",
    "#                   random negative question\n",
    "                    neg_question = random.choice(list(prop2desc.values()))\n",
    "                    while neg_question == question:\n",
    "                        neg_question = random.choice(list(prop2desc.values()))\n",
    "                        \n",
    "                    _json = {'context': _context, 'qas':[{'id': str(qid), 'is_impossible': False, \n",
    "                                                          'question': question, 'answers':[{'text':utterance[s['start']: s['exclusive_end']], \n",
    "                                                           'answer_start': answer_start}]}, {'id': str(qid+1), 'is_impossible': True, 'question': neg_question, 'answers':[] }]}\n",
    "    \n",
    "                    if simplifyDomainName(service) in trainIds:\n",
    "                        train_data.append(_json)\n",
    "                    else:\n",
    "                        test_data.append(_json)\n",
    "                    qid += 2\n",
    "            context.append(utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34461, 38913)"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save as a JSON file\n",
    "import json\n",
    "with open('data/dialog/defaultWithNegativeTrain.json', 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "import json\n",
    "with open('data/dialog/defaultWithNegativeTest.json', 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train_model('data/dialog/defaultWithNegativeTrain.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

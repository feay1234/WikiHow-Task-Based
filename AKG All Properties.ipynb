{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "import re, nltk, random, os, json\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "from nltk.corpus import stopwords\n",
    "from pyNTCIREVAL import Labeler\n",
    "from pyNTCIREVAL.metrics import MSnDCG, nERR, nDCG\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# keras\n",
    "from keras.layers import Input, LSTM, Concatenate, Embedding, Multiply, Dot, Dense, Subtract, Activation, SimpleRNN, Flatten, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectors(queries):\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'application/json',\n",
    "        'Ocp-Apim-Subscription-Key': '924c1505854b4da4a6144a1cce92937f',\n",
    "    }\n",
    "    \n",
    "    queries = [str(i).replace(\"\\'\", \"\") for i in queries]\n",
    "\n",
    "    params = urllib.parse.urlencode({})\n",
    "    \n",
    "    try:\n",
    "        conn = http.client.HTTPSConnection('api.msturing.org')\n",
    "#         conn.request(\"POST\", \"/gen/encode?%s\" % params, '{\"queries\": [\"how to make gingerbread people (in grams)\", \"test AI\"]}', headers)\n",
    "        conn.request(\"POST\", \"/gen/encode?%s\" % params, str({\"queries\": queries}).replace(\"\\'\", \"\\\"\"), headers)\n",
    "        response = conn.getresponse()\n",
    "        data = response.read()\n",
    "        data = json.loads(data)\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "#         print(data)\n",
    "        print(e)\n",
    "#         print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "    \n",
    "    return {data[i]['query']:data[i]['vector'] for i in range(len(data))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "regex = re.compile('[^a-zA-Z0-9.]')\n",
    "#First parameter is the replacement, second parameter is your input string\n",
    "def preprocessingText(doc):\n",
    "    doc = regex.sub(' ', doc)\n",
    "#     doc = \" \".join([w for w in doc.split() if not w in stop_words])\n",
    "    return doc.lower()\n",
    "\n",
    "def evaluate(qrels, ranked_list):\n",
    "    res = []\n",
    "    grades = [1,2,3,4] # a grade for relevance levels 1 and 2 (Note that level 0 is excluded)\n",
    "    labeler = Labeler(qrels)\n",
    "    labeled_ranked_list = labeler.label(ranked_list)\n",
    "    rel_level_num = 5\n",
    "    xrelnum = labeler.compute_per_level_doc_num(rel_level_num)\n",
    "    metric = MSnDCG(xrelnum, grades, cutoff=10)\n",
    "    result = metric.compute(labeled_ranked_list)\n",
    "    return result\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return \" \".join([m.group(0) for m in matches]).lower()\n",
    "\n",
    "def getTermMSvec(all_properties):\n",
    "    tmp = {}\n",
    "    for i in range(0, len(all_properties), 20):\n",
    "        data = getVectors(all_properties[i:i+20])\n",
    "        for i in data:\n",
    "            tmp[i] = data[i]\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/AKG/Test Collection/AKGG/akg_standard_akgg_property_rele.csv\")\n",
    "df_action = pd.read_csv(\"data/AKG/Test Collection/AM/akg_standard_am_verb_object_rele.csv\")\n",
    "\n",
    "with open(\"data/AKG/Formal Run Topics/AKGG_Formal_Run_Topic.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    qid, query, entity, entityType, action = [], [], [], [], []\n",
    "    for p in data['queries']:\n",
    "        qid.append(p['queryId'])\n",
    "        query.append(p['query'])   \n",
    "        entity.append(p['entity'])\n",
    "        entityType.append(' '.join(p['entityTypes']))    \n",
    "        action.append(p['action'])\n",
    "topic = pd.DataFrame({\"query_id\": qid, \"query\": query, \"entity\": entity, \"entityType\": entityType, \"action\":action})\n",
    "for c in [\"query\", \"entityType\", \"action\", \"entity\"]:\n",
    "    topic[c] = topic[c].str.lower().replace(\"\\'\", \"\")\n",
    "    \n",
    "df = df.merge(topic, how=\"inner\", on=\"query_id\")\n",
    "# df['query'] = df[['action', 'entity', 'entityType']].astype(str).apply(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIds, testIds = [], []\n",
    "for name, group in df.groupby(\"entityType\"):\n",
    "    if group.query_id.nunique() > 1:\n",
    "        ids = list(group.query_id.unique())\n",
    "        mid = int(group.query_id.nunique() / 2)\n",
    "        trainIds.extend(ids[:mid])\n",
    "        testIds.extend(ids[mid:])\n",
    "    else:\n",
    "        ids = list(group.query_id.unique())\n",
    "        trainIds.extend(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/AKG/Formal Run Topics/AM_Formal_Run_Topic.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    qid, entityurl, entity, entityType = [], [], [], []\n",
    "    for p in data['queries'][0]:\n",
    "        qid.append(p['queryId'])\n",
    "        entity.append(p['entity'])\n",
    "        entityType.append(' '.join(p['entityTypes']))    \n",
    "        entityurl.append(p['entityurl'])\n",
    "am_topic = pd.DataFrame({\"query_id\": qid, \"url\": entityurl, \"entity\": entity, \"entityType\": entityType})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = pd.read_csv(\"data/wikihowSep.csv\")\n",
    "df_wiki['headline'] = df_wiki['headline'].str.replace(\"\\n\", \"\")\n",
    "df_wiki['title'] = df_wiki['title'].str.replace(\"How to\", \"\")\n",
    "\n",
    "df_wiki['overview'] = [preprocessingText(str(i)) for i in df_wiki['overview']]\n",
    "df_wiki['headline'] = [preprocessingText(str(i)) for i in df_wiki['headline']]\n",
    "df_wiki['text'] = [preprocessingText(str(i)) for i in df_wiki['text']]\n",
    "df_wiki['sectionLabel'] = [preprocessingText(str(i)) for i in df_wiki['sectionLabel']]\n",
    "df_wiki['title'] = [preprocessingText(str(i)) for i in df_wiki['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoVivification(dict):\n",
    "    \"\"\"Implementation of perl's autovivification feature.\"\"\"\n",
    "    def __getitem__(self, item):\n",
    "        try:\n",
    "            return dict.__getitem__(self, item)\n",
    "        except KeyError:\n",
    "            value = self[item] = type(self)()\n",
    "            return value\n",
    "with open(\"data/AKG/Participants Runs/AKGG/akgg-formalrun-cuis.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    run = AutoVivification()\n",
    "    for p in data['runs']:\n",
    "        for res in p['results']:\n",
    "            for prop in res['properties']:\n",
    "                run[p['runid']][str(res['queryid'])][str(prop['property'])] = prop['rank']\n",
    "\n",
    "qids = []\n",
    "props = []\n",
    "for qid in run['1']:\n",
    "    tmp = list(run['1'][str(qid)].keys())\n",
    "    qids.extend([int(qid)] * len(tmp))\n",
    "    props.extend(tmp)\n",
    "df_run = pd.DataFrame({\"query_id\": qids, \"property\": props})\n",
    "df_run = df_run.merge(topic, how=\"left\", on=\"query_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "qrel = collections.defaultdict(dict)\n",
    "for qid, prop, label in df[['query_id', 'property', 'rele_label']].values:\n",
    "    qrel[str(qid)][str(prop)] = int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = df[[\"query_id\", \"entityType\", \"property\"]].append(df_run[[\"query_id\", \"entityType\", \"property\"]])\n",
    "dfp = dfp[dfp.query_id.isin(trainIds)]\n",
    "type2prop = dfp.groupby(\"entityType\")['property'].unique().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop2popularity = dfp.groupby(\"property\").size().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "type2prop2popularity = dfp.groupby([\"entityType\", \"property\"]).size().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = []\n",
    "for c in [ 'entity', 'entityType', 'action']:\n",
    "    for i in df[c].unique().tolist():\n",
    "        terms.append(i.replace(\"\\'\", \"\"))\n",
    "terms.extend([camel_case_split(i) for i in df.property.unique()])\n",
    "term2MSvec = getTermMSvec(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 993 unique tokens.\n",
      "Indexing word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1209 14:28:57.823349 4570330560 deprecation_wrapper.py:119] From /Users/jarana/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Preparing embedding matrix.\n",
      "Token num: 993, Found Tokens: 963\n"
     ]
    }
   ],
   "source": [
    "def get_pretrain_embeddings(path, word_index, EMBEDDING_DIM=100):\n",
    "    MAX_NUM_WORDS = len(word_index)\n",
    "    BASE_DIR = path + 'data/'\n",
    "    GLOVE_DIR = os.path.join(BASE_DIR, 'w2v')\n",
    "    print('Indexing word vectors.')\n",
    "\n",
    "    embeddings_index = {}\n",
    "    with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    print('Preparing embedding matrix.')\n",
    "\n",
    "    # prepare embedding matrix\n",
    "    num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    found = 0\n",
    "    for word, i in word_index.items():\n",
    "        if i > MAX_NUM_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if embedding_vector.shape[0] == 0:\n",
    "                continue\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            found += 1\n",
    "\n",
    "    print(\"Token num: %d, Found Tokens: %d\" % (len(word_index), found))\n",
    "\n",
    "    # load pre-trained word embeddings into an Embedding layer\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                embeddings_initializer=Constant(embedding_matrix))\n",
    "\n",
    "    return embedding_layer\n",
    "MAX_NUM_WORDS = 1000\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(list(term2MSvec.keys()))\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "path = \"/Users/jarana/workspace/WikiHow-Task-Based/\"\n",
    "max_words = len(word_index)\n",
    "embedding_layer = get_pretrain_embeddings(path, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0baff49b3149>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mMAX_NUM_WORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_NUM_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mqueries\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found %s unique tokens.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "queries = []\n",
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "    queries.append(row['action'] + \" \" + row['entity'] + \" \" + row['entityType'])\n",
    "properties = [camel_case_split(i) for i in df['property'].unique()]\n",
    "\n",
    "MAX_NUM_WORDS = 100000\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(corpus + queries + properties)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f94bbc2546eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/jarana/workspace/WikiHow-Task-Based/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmax_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0membedding_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pretrain_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_index' is not defined"
     ]
    }
   ],
   "source": [
    "path = \"/Users/jarana/workspace/WikiHow-Task-Based/\"\n",
    "max_words = len(word_index)\n",
    "embedding_layer = get_pretrain_embeddings(path, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWord2Vec(embedding_layer, MAX_SEQUENCE_LENGTH=10000):\n",
    "    q_inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    q_emb = GlobalMaxPooling1D()(embedding_layer(q_inp))\n",
    "    model = Model(q_inp, q_emb)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "word2vec = getWord2Vec(embedding_layer)\n",
    "prop2Glovevec = {p: word2vec.predict(pad_sequences(tokenizer.texts_to_sequences([camel_case_split(p)]), maxlen=10000))[0] for p in dfp.property.unique()}\n",
    "qid2Glovevec = {}\n",
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "    q = row['action'] + \" \" + row['entity'] + \" \" + row['entityType']\n",
    "    qid2Glovevec[row['query_id']] = word2vec.predict(pad_sequences(tokenizer.texts_to_sequences([q]), maxlen=10000))[0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = []\n",
    "# for name, group in df_wiki2.groupby(\"title\"):\n",
    "#     title = name\n",
    "#     overview = group['overview'].unique().tolist()\n",
    "#     text = group['text'].unique().tolist()\n",
    "#     headline = group['headline'].unique().tolist()\n",
    "#     sectionLabel = group['sectionLabel'].unique().tolist()\n",
    "# #     doc = \" \\t \".join([title] + overview + sectionLabel + headline + text)\n",
    "#     doc = \". \".join([title] + [str(i) for i in overview])\n",
    "#     corpus.append(doc)\n",
    "#     break\n",
    "# corpus = [preprocessingText(i) for i in corpus]\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# qid2doc2MSvec = {}\n",
    "# for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "#     query = row['action'] + \" \" + row['entity'] + \" \" + row['entityType']\n",
    "#     query = query.replace(\"thing\", \"\")\n",
    "#     tokenized_query = query.split(\" \")\n",
    "#     doc = bm25.get_top_n(tokenized_query, corpus, n=1)\n",
    "#     data = getVectors(doc)\n",
    "#     qid2doc2MSvec[row['query_id']] = data[doc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid2MSvec = {}\n",
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "    query = row['action'] + \" \" + row['entity'] + \" \" + row['entityType']\n",
    "    tokenized_query = query.split(\" \")\n",
    "    doc = bm25.get_top_n(tokenized_query, corpus, n=1)\n",
    "    data = getVectors(doc[0].split(\". \"))\n",
    "    qid2MSvec[row['query_id']] = [data[i] for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = []\n",
    "for i in run[\"1\"]:\n",
    "    properties.extend(run[\"1\"][i].keys())\n",
    "properties = list(set(properties + df.property.unique().tolist()))\n",
    "prop2MSvec = getTermMSvec(properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type2prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGHT = np.max([len(qid2MSvec[i]) for i in qid2MSvec])\n",
    "\n",
    "def runDocEval(qid2vec, prop2vec):\n",
    "    class CLF():\n",
    "        def __init__(self):\n",
    "\n",
    "            self.queryInput = Input(shape=(MAX_SEQUENCE_LENGHT, 100,))\n",
    "            self.propPosInput = Input(shape=(100,))\n",
    "            lstm = LSTM(100)\n",
    "            \n",
    "            queryEmbeddingLayer = Dense(100, name=\"uEmb\")\n",
    "            propEmbeddingLayer = Dense(100, name=\"iEmb\")\n",
    "\n",
    "            qEmb = queryEmbeddingLayer(lstm(self.queryInput))\n",
    "            pEmb = propEmbeddingLayer(self.propPosInput)\n",
    "\n",
    "            dense = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "            pred = Multiply()([qEmb, pEmb])\n",
    "            self.pred = dense(pred)\n",
    "\n",
    "            self.model = Model(inputs=[self.queryInput, self.propPosInput], outputs=self.pred)\n",
    "            self.model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "            \n",
    "        def generate_train_data(self, df):\n",
    "            x_query, x_pos_prop, y = [], [], []\n",
    "            for name, group in df.groupby(\"query_id\"):\n",
    "                cand_pos_prop = group.property.tolist()\n",
    "                rele2prop = group[['rele_label', 'property']].groupby(\"rele_label\")[\"property\"].apply(list).to_dict()\n",
    "                for idx, row in group.iterrows():\n",
    "\n",
    "                    x_query.append(qid2vec[name])\n",
    "                    x_pos_prop.append(prop2vec[row['property']])\n",
    "                    y.append(1)\n",
    "\n",
    "                cand_neg_prop = list(set.difference(set(type2prop[row['entityType']]), set(cand_pos_prop)))\n",
    "    #             cand_neg_prop = list(set.difference(set(df.property.unique()), set(cand_pos_prop)))\n",
    "                for neg_prop in cand_neg_prop:\n",
    "                    x_query.append(qid2vec[name])\n",
    "                    x_pos_prop.append(prop2vec[neg_prop])\n",
    "                    y.append(0)\n",
    "                    \n",
    "            x_query = pad_sequences(x_query, maxlen=MAX_SEQUENCE_LENGHT)\n",
    "            x_pos_prop = np.array(x_pos_prop)\n",
    "            return [x_query, x_pos_prop], np.array(y)\n",
    "\n",
    "    df_train = df[df.query_id.isin(trainIds)]\n",
    "    bpr = CLF()\n",
    "\n",
    "    for i in range(5):\n",
    "        x_train, y_train = bpr.generate_train_data(df_train)\n",
    "        history = bpr.model.fit(x_train, y_train, verbose=1)\n",
    "        res = []\n",
    "        for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "            if row['query_id'] not in testIds:\n",
    "                continue\n",
    "            qrels = qrel[str(row['query_id'])]\n",
    "            cand_properties = type2prop[row['entityType']]\n",
    "\n",
    "            rank = {}\n",
    "            for p in cand_properties:\n",
    "                score = bpr.model.predict([[pad_sequences(qid2vec[row['query_id']], maxlen=MAX_SEQUENCE_LENGHT)], [prop2vec[p]]])[0][0]\n",
    "                rank[p] = score\n",
    "            rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "            our = evaluate(qrels, rank)\n",
    "            res.append(our)\n",
    "        print(history.history[\"loss\"][0], np.mean(res))\n",
    "runDocEval(qid2MSvec, prop2MSvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runEval(qid2vec, prop2vec):\n",
    "    class CLF():\n",
    "        def _Y_init__(self):\n",
    "\n",
    "            self.queryInput = Input(shape=(100,))\n",
    "            self.propPosInput = Input(shape=(100,))\n",
    "\n",
    "            queryEmbeddingLayer = Dense(100, name=\"uEmb\")\n",
    "            propEmbeddingLayer = Dense(100, name=\"iEmb\")\n",
    "\n",
    "            qEmb = queryEmbeddingLayer(self.queryInput)\n",
    "            pEmb = propEmbeddingLayer(self.propPosInput)\n",
    "\n",
    "            dense = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "            pred = Multiply()([qEmb, pEmb])\n",
    "            self.pred = dense(pred)\n",
    "\n",
    "            self.model = Model(inputs=[self.queryInput, self.propPosInput], outputs=self.pred)\n",
    "            self.model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "    #         self.predictor = Model([self.queryInput, self.propPosInput], [pDot])\n",
    "    #         self.predictor = Model([self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput], [pDot])\n",
    "        def generate_train_data(self, df):\n",
    "            x_query, x_entity, x_type, x_action, x_pos_prop, x_neg_prop, y = [], [], [], [], [], [], []\n",
    "            for name, group in df.groupby(\"query_id\"):\n",
    "                cand_pos_prop = group.property.tolist()\n",
    "                rele2prop = group[['rele_label', 'property']].groupby(\"rele_label\")[\"property\"].apply(list).to_dict()\n",
    "\n",
    "                for idx, row in group.iterrows():\n",
    "\n",
    "                    x_query.append(qid2vec[name])\n",
    "                    x_pos_prop.append(prop2vec[row['property']])\n",
    "                    y.append(1)\n",
    "\n",
    "                cand_neg_prop = list(set.difference(set(type2prop[row['entityType']]), set(cand_pos_prop)))\n",
    "    #             cand_neg_prop = list(set.difference(set(df.property.unique()), set(cand_pos_prop)))\n",
    "                for neg_prop in cand_neg_prop:\n",
    "                    x_query.append(qid2vec[name])\n",
    "                    x_pos_prop.append(prop2vec[neg_prop])\n",
    "                    y.append(0)\n",
    "\n",
    "            x_query = np.array(x_query)\n",
    "            x_entity = np.array(x_entity)\n",
    "            x_type = np.array(x_type)\n",
    "            x_action = np.array(x_action)\n",
    "            x_pos_prop = np.array(x_pos_prop)\n",
    "            x_neg_prop = np.array(x_neg_prop)\n",
    "    #         return [x_entity, x_type, x_action, x_pos_prop, x_neg_prop], np.ones(len(x_query))\n",
    "            return [x_query, x_pos_prop], np.array(y)\n",
    "\n",
    "    df_train = df[df.query_id.isin(trainIds)]\n",
    "    bpr = CLF()\n",
    "\n",
    "    for i in range(5):\n",
    "        x_train, y_train = bpr.generate_train_data(df_train)\n",
    "        print(x_train[0].shape)\n",
    "        history = bpr.model.fit(x_train, y_train, verbose=1)\n",
    "        res = []\n",
    "        for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "            if row['query_id'] not in testIds:\n",
    "                continue\n",
    "            qrels = qrel[str(row['query_id'])]\n",
    "            cand_properties = type2prop[row['entityType']]\n",
    "\n",
    "            rank = {}\n",
    "            for p in cand_properties:\n",
    "                score = bpr.model.predict([[qid2vec[row['query_id']]], [prop2vec[p]]])[0][0]\n",
    "                rank[p] = score\n",
    "            rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "            our = evaluate(qrels, rank)\n",
    "            res.append(our)\n",
    "        print(history.history[\"loss\"][0], np.mean(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qid2Glovevec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c0ef31ef22b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrunEval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqid2Glovevec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop2Glovevec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'qid2Glovevec' is not defined"
     ]
    }
   ],
   "source": [
    "runEval(qid2Glovevec, prop2Glovevec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40586, 100)\n",
      "Epoch 1/1\n",
      "40586/40586 [==============================] - 6s 156us/step - loss: 1.5421\n",
      "1.542116905747302 0.5190515531894443\n",
      "(40586, 100)\n",
      "Epoch 1/1\n",
      "40586/40586 [==============================] - 4s 91us/step - loss: 1.4715\n",
      "1.4715179094971944 0.5091338674472682\n",
      "(40586, 100)\n",
      "Epoch 1/1\n",
      "40586/40586 [==============================] - 4s 90us/step - loss: 1.4650\n",
      "1.4650341861028195 0.5086894400063741\n",
      "(40586, 100)\n",
      "Epoch 1/1\n",
      "40586/40586 [==============================] - 4s 93us/step - loss: 1.4662\n",
      "1.4661579954437958 0.5094041594885643\n",
      "(40586, 100)\n",
      "Epoch 1/1\n",
      "40586/40586 [==============================] - 4s 91us/step - loss: 1.4626\n",
      "1.4626264764412813 0.5140373406751466\n",
      "(40586, 100)\n",
      "Epoch 1/1\n",
      "40586/40586 [==============================] - 4s 91us/step - loss: 1.4630\n",
      "1.4630207869639176 0.5046157484529858\n",
      "(40586, 100)\n",
      "Epoch 1/1\n",
      "40586/40586 [==============================] - 4s 96us/step - loss: 1.4636\n",
      "1.463594130606659 0.4950205701944584\n",
      "(40586, 100)\n",
      "Epoch 1/1\n",
      "14432/40586 [=========>....................] - ETA: 2s - loss: 1.4633"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-458-4231cd5fbfde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'entity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'action'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'entityType'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`inputs` should be a list or tuple.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m         \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def bpr_triplet_loss(X):\n",
    "    positive_item_latent, negative_item_latent = X\n",
    "\n",
    "    loss = 1 - K.log(K.sigmoid(positive_item_latent - negative_item_latent))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred - 0 * y_true)\n",
    "\n",
    "\n",
    "class BPR():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.queryInput = Input(shape=(100,))\n",
    "#         self.entityTypeInput = Input(shape=(100,))\n",
    "#         self.actionInput = Input(shape=(100,))\n",
    "        \n",
    "        self.propPosInput = Input(shape=(100,))\n",
    "        self.propNegInput = Input(shape=(100,))\n",
    "\n",
    "        queryEmbeddingLayer = Dense(100, name=\"uEmb\")\n",
    "        propEmbeddingLayer = Dense(100, name=\"iEmb\")\n",
    "\n",
    "        self.qEmb = queryEmbeddingLayer(self.queryInput)\n",
    "        self.pEmb = propEmbeddingLayer(self.propPosInput)\n",
    "        self.nEmb = propEmbeddingLayer(self.propNegInput)\n",
    "\n",
    "        pDot = Dot(axes=-1)([self.qEmb, self.pEmb])\n",
    "        nDot = Dot(axes=-1)([self.qEmb, self.nEmb])\n",
    "\n",
    "#         peDot = Dot(axes=-1)([self.dentityInput, self.propPosInput])\n",
    "#         ptDot = Dot(axes=-1)([self.entityTypeInput, self.propPosInput])\n",
    "#         paDot = Dot(axes=-1)([self.actionInput, self.propPosInput])\n",
    "\n",
    "#         neDot = Dot(axes=-1)([self.entityInput, self.propNegInput])\n",
    "#         ntDot = Dot(axes=-1)([self.entityTypeInput, self.propNegInput])\n",
    "#         naDot = Dot(axes=-1)([self.actionInput, self.propNegInput])\n",
    "        \n",
    "#         pDot = Concatenate()([self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput])\n",
    "#         nDot = Concatenate()([self.entityInput, self.entityTypeInput, self.actionInput, self.propNegInput])\n",
    "        \n",
    "        dense = Dense(1, activation=\"sigmoid\")\n",
    "        \n",
    "#         pDot = Multiply()([self.queryInput, self.propPosInput])\n",
    "#         nDot = Multiply()([self.queryInput, self.propNegInput])\n",
    "        \n",
    "#         pDot = Dot(axes=-1)([self.queryInput, self.propPosInput])\n",
    "#         nDot = Dot(axes=-1)([self.queryInput, self.propNegInput])\n",
    "        \n",
    "        pDot = dense(pDot)\n",
    "        nDot = dense(nDot)\n",
    "#         pred = Multiply()([q_emb, t_emb])\n",
    "        #\n",
    "        # diff = Subtract()([pDot, nDot])\n",
    "        #\n",
    "        lammbda_output = Lambda(bpr_triplet_loss, output_shape=(1,))\n",
    "        self.pred = lammbda_output([pDot, nDot])\n",
    "\n",
    "        self.model = Model(inputs=[self.queryInput, self.propPosInput, self.propNegInput], outputs=self.pred)\n",
    "#         self.model = Model(inputs=[self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput, self.propNegInput], outputs=self.pred)\n",
    "#         self.model = Model(inputs=[self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput, self.propNegInput], outputs=self.pred)\n",
    "\n",
    "        self.model.compile(optimizer=\"adam\", loss=identity_loss)\n",
    "        self.predictor = Model([self.queryInput, self.propPosInput], [pDot])\n",
    "#         self.predictor = Model([self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput], [pDot])\n",
    "    def generate_train_data(self, df):\n",
    "        x_query, x_entity, x_type, x_action, x_pos_prop, x_neg_prop, y = [], [], [], [], [], [], []\n",
    "        for name, group in df.groupby(\"query_id\"):\n",
    "            cand_pos_prop = group.property.tolist()\n",
    "            rele2prop = group[['rele_label', 'property']].groupby(\"rele_label\")[\"property\"].apply(list).to_dict()\n",
    "\n",
    "            \n",
    "            for idx, row in group.iterrows():\n",
    "                \n",
    "                propLabel = row['rele_label']\n",
    "                for rele in rele2prop:\n",
    "                    if propLabel > rele:\n",
    "                        for neg_prop in rele2prop[rele]:\n",
    "                            x_query.append(qid2MSvec[name])\n",
    "                            x_pos_prop.append(prop2MSvec[row['property']])\n",
    "                            x_neg_prop.append(prop2MSvec[neg_prop])\n",
    "                \n",
    "                cand_neg_prop = list(set.difference(set(type2prop[row['entityType']]), set(cand_pos_prop)))\n",
    "                for neg_prop in cand_neg_prop:\n",
    "                    x_query.append(qid2MSvec[name])\n",
    "                    x_pos_prop.append(prop2MSvec[row['property']])\n",
    "                    x_neg_prop.append(prop2MSvec[neg_prop])\n",
    "                \n",
    "#                 cand_neg_prop = list(set.difference(set(df.property.unique()), set(cand_pos_prop)))\n",
    "#                 for neg_prop in cand_neg_prop:\n",
    "#                     x_query.append(qid2MSvec[name])\n",
    "                    x_pos_prop.append(prop2MSvec[row['property']])\n",
    "                    x_neg_prop.append(prop2MSvec[neg_prop])\n",
    "                            \n",
    "#                 cand_neg_prop = type2prop[row['entityType']]\n",
    "#                 cand_neg_prop = df.property.unique().tolist()\n",
    "\n",
    "#                 for n in range(int(row['rele_label'])):\n",
    "#                 for n in range(1):\n",
    "#                     if int(row['rele_label']) < 3:\n",
    "#                         break\n",
    "#                     x_entity.append(entity2MSvec[row['entity']])\n",
    "#                     x_type.append(type2MSvec[row['entityType']])\n",
    "#                     x_action.append(action2MSvec[row['action'].replace(\"\\'\", \"\")])\n",
    "#                     x_query.append(qid2MSvec[name])\n",
    "#                     x_pos_prop.append(prop2MSvec[row['property']])\n",
    "#                     neg_prop = random.choice(cand_neg_prop)\n",
    "#                     while neg_prop in cand_pos_prop:\n",
    "#                         neg_prop = random.choice(cand_neg_prop)\n",
    "#                     x_neg_prop.append(prop2MSvec[neg_prop])\n",
    "        x_query = np.array(x_query)\n",
    "        x_entity = np.array(x_entity)\n",
    "        x_type = np.array(x_type)\n",
    "        x_action = np.array(x_action)\n",
    "        x_pos_prop = np.array(x_pos_prop)\n",
    "        x_neg_prop = np.array(x_neg_prop)\n",
    "#         return [x_entity, x_type, x_action, x_pos_prop, x_neg_prop], np.ones(len(x_query))\n",
    "        return [x_query, x_pos_prop, x_neg_prop], np.ones(len(x_query))\n",
    "        \n",
    "        \n",
    "# print(x_query)\n",
    "df_train = df[df.query_id.isin(trainIds)]\n",
    "bpr = BPR()\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    x_train, y_train = bpr.generate_train_data(df_train)\n",
    "    print(x_train[0].shape)\n",
    "    history = bpr.model.fit(x_train, y_train, verbose=1)\n",
    "    res = []\n",
    "    for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "        if row['query_id'] not in testIds:\n",
    "            continue\n",
    "        qrels = qrel[str(row['query_id'])]\n",
    "        cand_properties = type2prop[row['entityType']]\n",
    "\n",
    "        rank = {}\n",
    "        for p in cand_properties:\n",
    "#             score = bpr.predictor.predict([[type2MSvec[row['entityType']]], [prop2MSvec[p]]])[0][0]\n",
    "            score = bpr.predictor.predict([[qid2MSvec[row['query_id']]], [prop2MSvec[p]]])[0][0]\n",
    "#             score = bpr.predictor.predict([[x_query], [x_property]])[0][0]\n",
    "            rank[p] = score\n",
    "        rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "        our = evaluate(qrels, rank)\n",
    "\n",
    "        res.append(our)\n",
    "    print(history.history[\"loss\"][0], np.mean(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, GlobalAveragePooling1D, GlobalMaxPooling1D, Concatenate, Embedding, Multiply, Dot, Dense, Subtract, Activation, SimpleRNN, Flatten, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "def bpr_triplet_loss(X):\n",
    "    positive_item_latent, negative_item_latent = X\n",
    "\n",
    "    loss = 1 - K.log(K.sigmoid(positive_item_latent - negative_item_latent))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred - 0 * y_true)\n",
    "\n",
    "\n",
    "class BPR_Emb():\n",
    "    def __init__(self, embedding_layer, query_len, property_len):\n",
    "        self.query_len = query_len\n",
    "        self.property_len =property_len\n",
    "        \n",
    "        queryInput = Input(shape=(query_len,))\n",
    "        propPosInput = Input(shape=(property_len,))\n",
    "        propNegInput = Input(shape=(property_len,))\n",
    "        \n",
    "        q_emb = GlobalMaxPooling1D()(embedding_layer(queryInput))\n",
    "        pp_emb = GlobalMaxPooling1D()(embedding_layer(propPosInput))\n",
    "        np_emb = GlobalMaxPooling1D()(embedding_layer(propNegInput))\n",
    "\n",
    "        pDot = Dot(axes=-1)([q_emb, pp_emb])\n",
    "        nDot = Dot(axes=-1)([q_emb, np_emb])\n",
    "        \n",
    "        lammbda_output = Lambda(bpr_triplet_loss, output_shape=(1,))\n",
    "        self.pred = lammbda_output([pDot, nDot])\n",
    "\n",
    "        self.model = Model(inputs=[queryInput, propPosInput, propNegInput], outputs=self.pred)\n",
    "\n",
    "        self.model.compile(optimizer=\"adam\", loss=identity_loss)\n",
    "        self.predictor = Model([queryInput, propPosInput], [pDot])\n",
    "        \n",
    "    def generate_train_data(self, df):\n",
    "        x_query, x_pos_prop, x_neg_prop, y = [], [], [], []\n",
    "        for name, group in df.groupby(\"query_id\"):\n",
    "            cand_pos_prop = group.property.tolist()\n",
    "            for idx, row in group.iterrows():\n",
    "                cand_neg_prop = type2prop[row['entityType']]\n",
    "#                 cand_neg_prop = df.property.unique().tolist()\n",
    "            \n",
    "                for n in range(int(row['rele_label'])):\n",
    "                    x_query.append(pad_sequences(tokenizer.texts_to_sequences([row['query']]), maxlen=self.query_len)[0])\n",
    "                    x_pos_prop.append(pad_sequences(tokenizer.texts_to_sequences([camel_case_split(row['property'])]), maxlen=self.property_len)[0])\n",
    "                    neg_prop = random.choice(cand_neg_prop)\n",
    "                    while neg_prop in cand_pos_prop:\n",
    "                        neg_prop = random.choice(cand_neg_prop)\n",
    "                    x_neg_prop.append(pad_sequences(tokenizer.texts_to_sequences([neg_prop]), maxlen=self.property_len)[0])\n",
    "                    \n",
    "        x_query = np.array(x_query)\n",
    "        x_pos_prop = np.array(x_pos_prop)\n",
    "        x_neg_prop = np.array(x_neg_prop)\n",
    "        return [x_query, x_pos_prop, x_neg_prop], np.ones(len(x_query))\n",
    "        \n",
    "query_len = np.max([len(camel_case_split(i).split()) for i in df['property'].tolist()])\n",
    "property_len = np.max([len(camel_case_split(i).split()) for i in df['property'].tolist()])\n",
    "# print(x_query)\n",
    "df_train = df[df.query_id.isin(trainIds)]\n",
    "bpr = BPR_Emb(embedding_layer, query_len, property_len)\n",
    "x_train, y_train = bpr.generate_train_data(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4498/4498 [==============================] - 2s 497us/step - loss: 1.0832\n",
      "1.0831895288545432 0.3826144596488156\n",
      "Epoch 1/1\n",
      "4498/4498 [==============================] - 1s 139us/step - loss: 1.0694\n",
      "1.0694032606944979 0.38042030857845655\n",
      "Epoch 1/1\n",
      "4498/4498 [==============================] - 1s 123us/step - loss: 1.0648\n",
      "1.0648053308336827 0.38235602146305087\n",
      "Epoch 1/1\n",
      "4498/4498 [==============================] - 1s 124us/step - loss: 1.0574\n",
      "1.0574256314760635 0.38222651509533356\n",
      "Epoch 1/1\n",
      "4498/4498 [==============================] - 1s 125us/step - loss: 1.0541\n",
      "1.054137928743477 0.3831956303053043\n",
      "Epoch 1/1\n",
      "4498/4498 [==============================] - 1s 124us/step - loss: 1.0501\n",
      "1.0500986490741524 0.38398216903759963\n",
      "Epoch 1/1\n",
      "4498/4498 [==============================] - 1s 125us/step - loss: 1.0462\n",
      "1.0462187009686732 0.38367446082679935\n",
      "Epoch 1/1\n",
      "4498/4498 [==============================] - 1s 125us/step - loss: 1.0426\n",
      "1.0426163518624922 0.38324473165330797\n",
      "Epoch 1/1\n",
      "4498/4498 [==============================] - 1s 132us/step - loss: 1.0412\n",
      "1.0411913765436176 0.38459720682594656\n",
      "Epoch 1/1\n",
      "4498/4498 [==============================] - 1s 126us/step - loss: 1.0391\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-423-3b37bb0eec5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mx_property\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcamel_case_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproperty_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#             score = bpr.predictor.predict([[type2MSvec[row['entityType']]], [prop2MSvec[p]]])[0][0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_query\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_property\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mrank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    x_train, y_train = bpr.generate_train_data(df_train)\n",
    "    history = bpr.model.fit(x_train, y_train, verbose=1)\n",
    "    res = []\n",
    "    for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "        if row['query_id'] not in testIds:\n",
    "            continue\n",
    "        qrels = qrel[str(row['query_id'])]\n",
    "        cand_properties = type2prop[row['entityType']]\n",
    "\n",
    "        rank = {}\n",
    "        for p in cand_properties:\n",
    "            x_query = pad_sequences(tokenizer.texts_to_sequences([row['query']]), maxlen=query_len)[0]\n",
    "            x_property = pad_sequences(tokenizer.texts_to_sequences([camel_case_split(p)]), maxlen=property_len)[0]\n",
    "#             score = bpr.predictor.predict([[type2MSvec[row['entityType']]], [prop2MSvec[p]]])[0][0]\n",
    "            score = bpr.predictor.predict([[x_query], [x_property]])[0][0]\n",
    "            rank[p] = score\n",
    "        rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "        our = evaluate(qrels, rank)\n",
    "\n",
    "        res.append(our)\n",
    "    print(history.history[\"loss\"][0], np.mean(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4636621146037568\n",
      "0.529892936164067\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "res2 = []\n",
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "    if row['query_id'] not in testIds:\n",
    "        continue\n",
    "    qrels = qrel[str(row['query_id'])]\n",
    "    cand_properties = type2prop[row['entityType']]\n",
    "    \n",
    "    rank = {}\n",
    "    for p in cand_properties:\n",
    "#         score = bpr.predictor.predict([[type2MSvec[row['entityType']]], [prop2MSvec[p]]])[0][0]\n",
    "#         rank[p] = score\n",
    "#         score = cosine_similarity([type2MSvec[row['entityType']]], [prop2MSvec[p]])[0][0]\n",
    "#         rank[p] = score\n",
    "#         if (row['entityType'], p) not in type2prop2popularity:\n",
    "#             rank[p] = -99999\n",
    "#         else:\n",
    "#             rank[p] = type2prop2popularity[(row['entityType'], p)]\n",
    "#         rank[p] = prop2popularity[p]\n",
    "        rank[p] = 1\n",
    "    rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "    our = evaluate(qrels, rank)\n",
    "#     our = evaluate(qrels, cand_properties)\n",
    "    base = evaluate(qrels, list(run[\"1\"][str(row[\"query_id\"])].keys()))\n",
    "\n",
    "    res.append(our)\n",
    "    res2.append(base)\n",
    "print(np.mean(res))\n",
    "print(np.mean(res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "import re, nltk, random, os, json\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "from nltk.corpus import stopwords\n",
    "from pyNTCIREVAL import Labeler\n",
    "from pyNTCIREVAL.metrics import MSnDCG, nERR, nDCG\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# keras\n",
    "from keras.layers import Input, LSTM, GlobalMaxPooling1D, GlobalAveragePooling1D, Concatenate, Embedding, Multiply, Dot, Dense, Subtract, Activation, SimpleRNN, Flatten, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectors(queries):\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'application/json',\n",
    "        'Ocp-Apim-Subscription-Key': '924c1505854b4da4a6144a1cce92937f',\n",
    "    }\n",
    "    \n",
    "    queries = [str(i).replace(\"\\'\", \"\") for i in queries]\n",
    "\n",
    "    params = urllib.parse.urlencode({})\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            conn = http.client.HTTPSConnection('api.msturing.org')\n",
    "    #         conn.request(\"POST\", \"/gen/encode?%s\" % params, '{\"queries\": [\"how to make gingerbread people (in grams)\", \"test AI\"]}', headers)\n",
    "            conn.request(\"POST\", \"/gen/encode?%s\" % params, str({\"queries\": queries}).replace(\"\\'\", \"\\\"\"), headers)\n",
    "            response = conn.getresponse()\n",
    "            data = response.read()\n",
    "            data = json.loads(data)\n",
    "            conn.close()\n",
    "            break\n",
    "        except Exception as e:\n",
    "    #         print(data)\n",
    "            print(\"delay\")\n",
    "            time.sleep(5)\n",
    "#         print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "    \n",
    "    return {data[i]['query']:data[i]['vector'] for i in range(len(data))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "regex = re.compile('[^a-zA-Z0-9.]')\n",
    "#First parameter is the replacement, second parameter is your input string\n",
    "def preprocessingText(doc):\n",
    "    doc = regex.sub(' ', doc)\n",
    "#     doc = \" \".join([w for w in doc.split() if not w in stop_words])\n",
    "    return doc.lower()\n",
    "\n",
    "def evaluate(qrels, ranked_list):\n",
    "    res = []\n",
    "    grades = [1,2,3,4] # a grade for relevance levels 1 and 2 (Note that level 0 is excluded)\n",
    "    labeler = Labeler(qrels)\n",
    "    labeled_ranked_list = labeler.label(ranked_list)\n",
    "    rel_level_num = 5\n",
    "    xrelnum = labeler.compute_per_level_doc_num(rel_level_num)\n",
    "    result = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in [5, 10 ,20]:\n",
    "        metric = MSnDCG(xrelnum, grades, cutoff=i)\n",
    "        result[\"ndcg@%d\" % i] = metric.compute(labeled_ranked_list)\n",
    "        \n",
    "        _ranked_list = ranked_list[:i]\n",
    "        result[\"p@%d\" % i] = len(set.intersection(set(qrels.keys()), set(_ranked_list))) / len(_ranked_list)\n",
    "        result[\"r@%d\" % i] = len(set.intersection(set(qrels.keys()), set(_ranked_list))) / len(qrels)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return \" \".join([m.group(0) for m in matches]).lower()\n",
    "\n",
    "def getTermMSvec(all_properties):\n",
    "    tmp = {}\n",
    "    for i in range(0, len(all_properties), 20):\n",
    "        data = getVectors(all_properties[i:i+20])\n",
    "        for i in data:\n",
    "            tmp[i] = data[i]\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/AKG/Test Collection/AKGG/akg_standard_akgg_property_rele.csv\")\n",
    "df_action = pd.read_csv(\"data/AKG/Test Collection/AM/akg_standard_am_verb_object_rele.csv\")\n",
    "\n",
    "with open(\"data/AKG/Formal Run Topics/AKGG_Formal_Run_Topic.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    qid, query, entity, entityType, action = [], [], [], [], []\n",
    "    for p in data['queries']:\n",
    "        qid.append(p['queryId'])\n",
    "        query.append(p['query'])   \n",
    "        entity.append(p['entity'])\n",
    "        entityType.append(' '.join(p['entityTypes']))    \n",
    "        action.append(p['action'])\n",
    "topic = pd.DataFrame({\"query_id\": qid, \"query\": query, \"entity\": entity, \"entityType\": entityType, \"action\":action})\n",
    "for c in [\"query\", \"entityType\", \"action\", \"entity\"]:\n",
    "    topic[c] = topic[c].str.lower().replace(\"\\'\", \"\")\n",
    "    \n",
    "df = df.merge(topic, how=\"inner\", on=\"query_id\")\n",
    "# df['query'] = df[['action', 'entity', 'entityType']].astype(str).apply(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIds, testIds = [], []\n",
    "for name, group in df.groupby(\"entityType\"):\n",
    "    if group.query_id.nunique() > 1:\n",
    "        ids = list(group.query_id.unique())\n",
    "        mid = int(group.query_id.nunique() / 2)\n",
    "        trainIds.extend(ids[:mid])\n",
    "        testIds.extend(ids[mid:])\n",
    "    else:\n",
    "        ids = list(group.query_id.unique())\n",
    "        trainIds.extend(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/AKG/Formal Run Topics/AM_Formal_Run_Topic.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    qid, entityurl, entity, entityType = [], [], [], []\n",
    "    for p in data['queries'][0]:\n",
    "        qid.append(p['queryId'])\n",
    "        entity.append(p['entity'])\n",
    "        entityType.append(' '.join(p['entityTypes']))    \n",
    "        entityurl.append(p['entityurl'])\n",
    "am_topic = pd.DataFrame({\"query_id\": qid, \"url\": entityurl, \"entity\": entity, \"entityType\": entityType})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = pd.read_csv(\"data/wikihowSep.csv\")\n",
    "df_wiki['headline'] = df_wiki['headline'].str.replace(\"\\n\", \"\")\n",
    "df_wiki['title'] = df_wiki['title'].str.replace(\"How to\", \"\")\n",
    "\n",
    "df_wiki['overview'] = [preprocessingText(str(i)) for i in df_wiki['overview']]\n",
    "df_wiki['headline'] = [preprocessingText(str(i)) for i in df_wiki['headline']]\n",
    "df_wiki['text'] = [preprocessingText(str(i)) for i in df_wiki['text']]\n",
    "df_wiki['sectionLabel'] = [preprocessingText(str(i)) for i in df_wiki['sectionLabel']]\n",
    "df_wiki['title'] = [preprocessingText(str(i)) for i in df_wiki['title']]\n",
    "df_wiki['title'] = [i if not i[-1].isdigit() else i[:-1] for i in df_wiki['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoVivification(dict):\n",
    "    \"\"\"Implementation of perl's autovivification feature.\"\"\"\n",
    "    def __getitem__(self, item):\n",
    "        try:\n",
    "            return dict.__getitem__(self, item)\n",
    "        except KeyError:\n",
    "            value = self[item] = type(self)()\n",
    "            return value\n",
    "with open(\"data/AKG/Participants Runs/AKGG/akgg-formalrun-cuis.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    run = AutoVivification()\n",
    "    for p in data['runs']:\n",
    "        for res in p['results']:\n",
    "            for prop in res['properties']:\n",
    "                run[p['runid']][str(res['queryid'])][str(prop['property'])] = prop['rank']\n",
    "\n",
    "qids = []\n",
    "props = []\n",
    "for qid in run['1']:\n",
    "    tmp = list(run['1'][str(qid)].keys())\n",
    "    qids.extend([int(qid)] * len(tmp))\n",
    "    props.extend(tmp)\n",
    "df_run = pd.DataFrame({\"query_id\": qids, \"property\": props})\n",
    "df_run = df_run.merge(topic, how=\"left\", on=\"query_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "qrel = collections.defaultdict(dict)\n",
    "for qid, prop, label in df[['query_id', 'property', 'rele_label']].values:\n",
    "    qrel[str(qid)][str(prop)] = int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = df[[\"query_id\", \"entityType\", \"property\"]].append(df_run[[\"query_id\", \"entityType\", \"property\"]])\n",
    "dfp = dfp[dfp.query_id.isin(trainIds)]\n",
    "type2prop = dfp.groupby(\"entityType\")['property'].unique().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop2popularity = dfp.groupby(\"property\").size().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "type2prop2popularity = dfp.groupby([\"entityType\", \"property\"]).size().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = []\n",
    "for c in [ 'entity', 'entityType', 'action']:\n",
    "    for i in df[c].unique().tolist():\n",
    "        terms.append(i.replace(\"\\'\", \"\"))\n",
    "terms.extend([camel_case_split(i) for i in df.property.unique()])\n",
    "term2MSvec = getTermMSvec(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 993 unique tokens.\n",
      "Indexing word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1209 14:28:57.823349 4570330560 deprecation_wrapper.py:119] From /Users/jarana/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Preparing embedding matrix.\n",
      "Token num: 993, Found Tokens: 963\n"
     ]
    }
   ],
   "source": [
    "def get_pretrain_embeddings(path, word_index, EMBEDDING_DIM=100):\n",
    "    MAX_NUM_WORDS = len(word_index)\n",
    "    BASE_DIR = path + 'data/'\n",
    "    GLOVE_DIR = os.path.join(BASE_DIR, 'w2v')\n",
    "    print('Indexing word vectors.')\n",
    "\n",
    "    embeddings_index = {}\n",
    "    with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    print('Preparing embedding matrix.')\n",
    "\n",
    "    # prepare embedding matrix\n",
    "    num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    found = 0\n",
    "    for word, i in word_index.items():\n",
    "        if i > MAX_NUM_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if embedding_vector.shape[0] == 0:\n",
    "                continue\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            found += 1\n",
    "\n",
    "    print(\"Token num: %d, Found Tokens: %d\" % (len(word_index), found))\n",
    "\n",
    "    # load pre-trained word embeddings into an Embedding layer\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                embeddings_initializer=Constant(embedding_matrix))\n",
    "\n",
    "    return embedding_layer\n",
    "MAX_NUM_WORDS = 1000\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(list(term2MSvec.keys()))\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "path = \"/Users/jarana/workspace/WikiHow-Task-Based/\"\n",
    "max_words = len(word_index)\n",
    "embedding_layer = get_pretrain_embeddings(path, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0baff49b3149>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mMAX_NUM_WORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_NUM_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mqueries\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found %s unique tokens.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "queries = []\n",
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "    queries.append(row['action'] + \" \" + row['entity'] + \" \" + row['entityType'])\n",
    "properties = [camel_case_split(i) for i in df['property'].unique()]\n",
    "\n",
    "MAX_NUM_WORDS = 100000\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(corpus + queries + properties)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f94bbc2546eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/jarana/workspace/WikiHow-Task-Based/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmax_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0membedding_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pretrain_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_index' is not defined"
     ]
    }
   ],
   "source": [
    "path = \"/Users/jarana/workspace/WikiHow-Task-Based/\"\n",
    "max_words = len(word_index)\n",
    "embedding_layer = get_pretrain_embeddings(path, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWord2Vec(embedding_layer, MAX_SEQUENCE_LENGTH=10000):\n",
    "    q_inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    q_emb = GlobalMaxPooling1D()(embedding_layer(q_inp))\n",
    "    model = Model(q_inp, q_emb)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "word2vec = getWord2Vec(embedding_layer)\n",
    "prop2Glovevec = {p: word2vec.predict(pad_sequences(tokenizer.texts_to_sequences([camel_case_split(p)]), maxlen=10000))[0] for p in dfp.property.unique()}\n",
    "qid2Glovevec = {}\n",
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "    q = row['action'] + \" \" + row['entity'] + \" \" + row['entityType']\n",
    "    qid2Glovevec[row['query_id']] = word2vec.predict(pad_sequences(tokenizer.texts_to_sequences([q]), maxlen=10000))[0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfw = df_wiki[df_wiki.title.str.contains(\"|\".join(df.entity.unique().tolist()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "entityInWiki = []\n",
    "for e in df.entity.unique():\n",
    "    tmp = dfw[dfw.title.str.contains(e)]\n",
    "    if len(tmp) > 0:\n",
    "        entityInWiki.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = []\n",
    "# for name, group in df_wiki2.groupby(\"title\"):\n",
    "#     title = name\n",
    "#     overview = group['overview'].unique().tolist()\n",
    "#     text = group['text'].unique().tolist()\n",
    "#     headline = group['headline'].unique().tolist()\n",
    "#     sectionLabel = group['sectionLabel'].unique().tolist()\n",
    "# #     doc = \" \\t \".join([title] + overview + sectionLabel + headline + text)\n",
    "#     doc = \". \".join([title] + [str(i) for i in headline])\n",
    "#     corpus.append(doc)\n",
    "# corpus = [preprocessingText(i) for i in corpus]\n",
    "corpus = df_wiki.title.unique().tolist()\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# qid2doc2MSvec = {}\n",
    "# for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "#     query = row['action'] + \" \" + row['entity'] + \" \" + row['entityType']\n",
    "#     query = query.replace(\"thing\", \"\")\n",
    "#     tokenized_query = query.split(\" \")\n",
    "#     doc = bm25.get_top_n(tokenized_query, corpus, n=1)\n",
    "#     data = getVectors(doc)\n",
    "#     qid2doc2MSvec[row['query_id']] = data[doc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "    query = row['action'] + \" \" + row['entity'] + \" \" + row['entityType']\n",
    "    query = query.replace(\"thing\", \"\")\n",
    "    tokenized_query = query.split(\" \")\n",
    "    doc = bm25.get_top_n(tokenized_query, corpus, n=5)\n",
    "#     data = getVectors(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid2MSvec = {}\n",
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "    query = row['action'] + \" \" + row['entity']\n",
    "    if row['query_id'] in qid2MSvec:\n",
    "        continue\n",
    "    if row['entity'] not in entityInWiki:\n",
    "        data = getVectors([query])\n",
    "        qid2MSvec[row['query_id']] = [data[i] for i in data]\n",
    "        continue        \n",
    "#     titles = [str(i).replace(\"\\'\", \"\") for i in dfw[dfw.title.str.contains(row['entity'])].text.unique().tolist()]\n",
    "    titles = dfw[dfw.title.str.contains(row['entity'])].title.unique().tolist()\n",
    "    data = getTermMSvec([query] + titles)\n",
    "    rank = {}\n",
    "    for i in data:\n",
    "        if i == query:\n",
    "            continue\n",
    "        rank[i] = cosine_similarity([data[query]], [data[i]])[0][0]\n",
    "    best_title = sorted(rank.items(), key=lambda x: x[1])[-1][0]\n",
    "    sentences = []\n",
    "    tokens = \" \".join(dfw[dfw.title == best_title].text.tolist()).split()\n",
    "    for i in range(0, len(tokens), 10):\n",
    "        sentences.append(\" \".join(tokens[i:i+10]))\n",
    "#     print(sentences)\n",
    "    data = getTermMSvec([query] + sentences)\n",
    "    qid2MSvec[row['query_id']] = list(data.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid2query2MSvec = {}\n",
    "queries = []\n",
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "    query = preprocessingText(row['action'] + \" \" + row['entity'] + \" \" + row['entityType'])\n",
    "    qid2query2MSvec[row['query_id']] = getVectors([query])[query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = []\n",
    "for i in run[\"1\"]:\n",
    "    properties.extend(run[\"1\"][i].keys())\n",
    "properties = list(set(properties + df.property.unique().tolist()))\n",
    "prop2MSvec = getTermMSvec(properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 5s 1ms/step - loss: 0.6783\n",
      "p@5 0.6415094339622641\n",
      "p@10 0.6452830188679245\n",
      "p@20 0.6097085110111571\n",
      "r@5 0.21295392806141095\n",
      "r@10 0.4215522536540119\n",
      "r@20 0.7399442286851015\n",
      "ndcg@5 0.4147554424780503\n",
      "ndcg@10 0.4934425418859435\n",
      "ndcg@20 0.6034604259609319\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 3s 895us/step - loss: 0.6151\n",
      "p@5 0.7056603773584906\n",
      "p@10 0.6566037735849056\n",
      "p@20 0.6276330393130439\n",
      "r@5 0.23388207031527314\n",
      "r@10 0.42968920500610436\n",
      "r@20 0.7619390921693918\n",
      "ndcg@5 0.4926087025163004\n",
      "ndcg@10 0.5323958724753819\n",
      "ndcg@20 0.6479589975385036\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 3s 925us/step - loss: 0.5842\n",
      "p@5 0.7207547169811319\n",
      "p@10 0.6698113207547168\n",
      "p@20 0.6248028506337987\n",
      "r@5 0.23208312666297945\n",
      "r@10 0.43407584284723716\n",
      "r@20 0.7597716256846463\n",
      "ndcg@5 0.5089250104313318\n",
      "ndcg@10 0.5596897273777569\n",
      "ndcg@20 0.6570091495135245\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 4s 976us/step - loss: 0.5576\n",
      "p@5 0.7018867924528301\n",
      "p@10 0.6698113207547169\n",
      "p@20 0.6191424732753081\n",
      "r@5 0.23057452304560777\n",
      "r@10 0.437109077955944\n",
      "r@20 0.7529050362213806\n",
      "ndcg@5 0.48766165837806585\n",
      "ndcg@10 0.5453815261623465\n",
      "ndcg@20 0.6449277402957959\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 4s 1ms/step - loss: 0.5366\n",
      "p@5 0.7018867924528301\n",
      "p@10 0.6735849056603773\n",
      "p@20 0.6257462468602139\n",
      "r@5 0.23030202106374778\n",
      "r@10 0.44120784330843316\n",
      "r@20 0.7595402215765555\n",
      "ndcg@5 0.5037316698328989\n",
      "ndcg@10 0.5509400629807996\n",
      "ndcg@20 0.6507699292722887\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 4s 1ms/step - loss: 0.5135\n",
      "p@5 0.6981132075471698\n",
      "p@10 0.6471698113207548\n",
      "p@20 0.6163122845960629\n",
      "r@5 0.23732610038804913\n",
      "r@10 0.42491411304002574\n",
      "r@20 0.7477371433338489\n",
      "ndcg@5 0.5095626369527279\n",
      "ndcg@10 0.5486724676484516\n",
      "ndcg@20 0.650887250507925\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 4s 1ms/step - loss: 0.4960\n",
      "p@5 0.7660377358490565\n",
      "p@10 0.6943396226415094\n",
      "p@20 0.6115953034639874\n",
      "r@5 0.2512243785091206\n",
      "r@10 0.4526824657127244\n",
      "r@20 0.7422162121747379\n",
      "ndcg@5 0.543072600456856\n",
      "ndcg@10 0.5740385475178458\n",
      "ndcg@20 0.6574921410127578\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 4s 1ms/step - loss: 0.4741\n",
      "p@5 0.7056603773584905\n",
      "p@10 0.6754716981132074\n",
      "p@20 0.6106519072375723\n",
      "r@5 0.23077477518230483\n",
      "r@10 0.44011568628164227\n",
      "r@20 0.7417372003997975\n",
      "ndcg@5 0.5100751087605425\n",
      "ndcg@10 0.562584973298916\n",
      "ndcg@20 0.6491419674109762\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 4s 1ms/step - loss: 0.4559\n",
      "p@5 0.7283018867924529\n",
      "p@10 0.6792452830188679\n",
      "p@20 0.6210292657281384\n",
      "r@5 0.24244297759149674\n",
      "r@10 0.4446773805019029\n",
      "r@20 0.7560837930818654\n",
      "ndcg@5 0.510556671744999\n",
      "ndcg@10 0.5614628093393371\n",
      "ndcg@20 0.6557044089439757\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 4s 1ms/step - loss: 0.4373\n",
      "p@5 0.7320754716981132\n",
      "p@10 0.6716981132075472\n",
      "p@20 0.6191424732753081\n",
      "r@5 0.23895647704062334\n",
      "r@10 0.4377325207715418\n",
      "r@20 0.7490283324946564\n",
      "ndcg@5 0.5153955494463379\n",
      "ndcg@10 0.561685141655819\n",
      "ndcg@20 0.6526552089017696\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGHT = int(np.max([len(qid2MSvec[i]) for i in qid2MSvec])) + 1\n",
    "\n",
    "# def runDocEval(qid2MSvec, prop2MSvec):\n",
    "class CLF():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.queryInput = Input(shape=(MAX_SEQUENCE_LENGHT, 100,))\n",
    "        self.propPosInput = Input(shape=(100,))\n",
    "        lstm = LSTM(100)\n",
    "\n",
    "        queryEmbeddingLayer = Dense(100, name=\"uEmb\")\n",
    "        propEmbeddingLayer = Dense(100, name=\"iEmb\")\n",
    "\n",
    "#         qEmb = queryEmbeddingLayer(lstm(self.queryInput))\n",
    "        qEmb = queryEmbeddingLayer(self.queryInput)\n",
    "        pEmb = propEmbeddingLayer(self.propPosInput)\n",
    "\n",
    "        dense = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "        pred = Multiply()([qEmb, pEmb])\n",
    "        \n",
    "        self.pred = dense(GlobalMaxPooling1D()(pred))\n",
    "#         self.pred = dense(pred)\n",
    "\n",
    "        self.model = Model(inputs=[self.queryInput, self.propPosInput], outputs=self.pred)\n",
    "        self.model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "    def generate_train_data(self, df):\n",
    "        x_query, x_pos_prop, y = [], [], []\n",
    "        for name, group in df.groupby(\"query_id\"):\n",
    "            cand_pos_prop = group.property.tolist()\n",
    "            rele2prop = group[['rele_label', 'property']].groupby(\"rele_label\")[\"property\"].apply(list).to_dict()\n",
    "            for idx, row in group.iterrows():\n",
    "                x_query.append(qid2MSvec[name] + [qid2query2MSvec[name]])\n",
    "#                 x_query.append([qid2query2MSvec[name]])\n",
    "                x_pos_prop.append(prop2MSvec[row['property']])\n",
    "                y.append(1)\n",
    "\n",
    "            cand_neg_prop = list(set.difference(set(type2prop[row['entityType']]), set(cand_pos_prop)))\n",
    "#             cand_neg_prop = list(set.difference(set(df.property.unique()), set(cand_pos_prop)))\n",
    "            for neg_prop in cand_neg_prop:\n",
    "                x_query.append(qid2MSvec[name] + [qid2query2MSvec[name]])\n",
    "#                 x_query.append([qid2query2MSvec[name]])\n",
    "                x_pos_prop.append(prop2MSvec[neg_prop])\n",
    "                y.append(0)\n",
    "\n",
    "        x_query = pad_sequences(x_query,dtype='float32', maxlen=MAX_SEQUENCE_LENGHT)\n",
    "        x_pos_prop = np.array(x_pos_prop)\n",
    "        return [x_query, x_pos_prop], np.array(y)\n",
    "\n",
    "df_train = df[df.query_id.isin(trainIds)]\n",
    "bpr = CLF()\n",
    "x_train, y_train = bpr.generate_train_data(df_train)\n",
    "for i in range(10):\n",
    "    test_num = 0\n",
    "    history = bpr.model.fit(x_train, y_train, verbose=1)\n",
    "    res = {\"%s@%d\" %( i,j): [] for i in [\"p\", \"r\", \"ndcg\"] for j in [5, 10 ,20]}\n",
    "    for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "        if row['query_id'] not in testIds:\n",
    "            continue\n",
    "        if row['entity'] not in entityInWiki:\n",
    "                continue\n",
    "        test_num += 1\n",
    "        qrels = qrel[str(row['query_id'])]\n",
    "        cand_properties = type2prop[row['entityType']]\n",
    "#         cand_properties = list(prop2MSvec.keys())\n",
    "#         cand_properties = list(set(type2prop[row['entityType']].tolist() + list(qrels.keys())))\n",
    "#         cand_properties = list(set(type2prop[row['entityType']].tolist() + list(qrels.keys())))\n",
    "\n",
    "\n",
    "        rank = {}\n",
    "        for p in cand_properties:\n",
    "            score = bpr.model.predict([pad_sequences([qid2MSvec[row['query_id']] + [qid2query2MSvec[row['query_id']]]], dtype='float32', maxlen=MAX_SEQUENCE_LENGHT), np.array([prop2MSvec[p]])])[0][0]\n",
    "#             score = bpr.model.predict([pad_sequences([[qid2query2MSvec[row['query_id']]]], dtype='float32', maxlen=MAX_SEQUENCE_LENGHT), np.array([prop2MSvec[p]])])[0][0]\n",
    "            rank[p] = score\n",
    "#         rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "        rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "        our = evaluate(qrels, rank)\n",
    "        for key in res:\n",
    "            res[key].append(our[key])\n",
    "    for key in res:\n",
    "        print(key, np.mean(res[key]))\n",
    "# runDocEval(qid2MSvec, prop2MSvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p@5 0.6377358490566039\n",
      "p@10 0.6121593291404612\n",
      "p@20 0.5825995807127883\n",
      "r@5 0.21082034063385194\n",
      "r@10 0.40143147962953435\n",
      "r@20 0.700012686961253\n",
      "ndcg@5 0.4251717300658441\n",
      "ndcg@10 0.487037580220878\n",
      "ndcg@20 0.5898301992301582\n"
     ]
    }
   ],
   "source": [
    "res = {\"%s@%d\" %( i,j): [] for i in [\"p\", \"r\", \"ndcg\"] for j in [5, 10 ,20]}\n",
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "    if row['query_id'] not in testIds:\n",
    "        continue\n",
    "    if row['entity'] not in entityInWiki:\n",
    "            continue\n",
    "    test_num += 1\n",
    "    qrels = qrel[str(row['query_id'])]\n",
    "    cand_properties = type2prop[row['entityType']].tolist()\n",
    "    \n",
    "#     top_cat = \" \".join(row['entityType'].split()[:-1])\n",
    "#     if top_cat in type2prop:\n",
    "#         cand_properties = cand_properties + type2prop[top_cat].tolist()\n",
    "#     cand_properties = list(qrels.keys())|\n",
    "    \n",
    "#         cand_properties = list(prop2MSvec.keys())\n",
    "#     cand_properties = list(set(type2prop[row['entityType']].tolist() + list(qrels.keys())))\n",
    "\n",
    "#     rank = {}\n",
    "#     for p in cand_properties:\n",
    "#         score = bpr.model.predict([pad_sequences([qid2MSvec[row['query_id']] + [qid2query2MSvec[row['query_id']]]], dtype='float32', maxlen=MAX_SEQUENCE_LENGHT), np.array([prop2MSvec[p]])])[0][0]\n",
    "# #             score = bpr.model.predict([pad_sequences([[qid2query2MSvec[row['query_id']]]], dtype='float32', maxlen=MAX_SEQUENCE_LENGHT), np.array([prop2MSvec[p]])])[0][0]\n",
    "#         rank[p] = score\n",
    "# #         rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "#     rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "    our = evaluate(qrels, cand_properties)\n",
    "    for key in res:\n",
    "        res[key].append(our[key])\n",
    "for key in res:\n",
    "    print(key, np.mean(res[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thing'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p@5 0.6377358490566039\n",
    "p@10 0.6113207547169811\n",
    "p@20 0.569954761116628\n",
    "r@5 0.21082034063385194\n",
    "r@10 0.40143147962953435\n",
    "r@20 0.7037862718669133\n",
    "ndcg@5 0.4251717300658441\n",
    "ndcg@10 0.48841889205988737\n",
    "ndcg@20 0.6132699993873993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r@20 0.700012686961253\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['thing', 'thing action', 'thing creativework', 'thing creativework book', 'thing creativework movie', 'thing creativework musicrecording', 'thing creativework tvseries', 'thing event', 'thing event educationevent', 'thing event sportsevent', 'thing intangible', 'thing intangible computerlanguage', 'thing intangible language', 'thing intangible service', 'thing intangible service financialproduct', 'thing medicalentity anatomicalsystem', 'thing medicalentity medicalcondition', 'thing medicalentity medicalintangible', 'thing medicalentity medicalprocedure', 'thing medicalentity parasiticdisease', 'thing medicalentity substance', 'thing medicalentity substance drug', 'thing organization airline', 'thing organization broadcastchannel', 'thing organization company', 'thing organization corporation', 'thing organization educationalorganization collegeoruniversity', 'thing organization governmentorganization', 'thing organization ngo', 'thing organization performinggroup musicgroup', 'thing organization sportsorganization', 'thing person', 'thing place', 'thing place accommodation', 'thing place landform bodyofwater', 'thing place landform mountain', 'thing place localbusiness entertainmentbusiness', 'thing place touristattraction', 'thing product', 'thing product vehicle', 'thing product vehicle car'])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in type2prop:\n",
    "    if k == \"thing\":\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thing': array(['potentialAction', 'url', 'description', 'name', 'alternateName',\n",
       "        'identifier', 'image', 'additionalType'], dtype=object),\n",
       " 'thing action': array(['startTime', 'result', 'target', 'name', 'actionStatus', 'object',\n",
       "        'instrument', 'disambiguatingDescription', 'description',\n",
       "        'participant', 'agent', 'potentialAction', 'endTime',\n",
       "        'additionalType', 'image', 'location', 'alternateName', 'url',\n",
       "        'identifier', 'error'], dtype=object),\n",
       " 'thing creativework': array(['releasedEvent', 'version', 'author', 'workExample', 'award',\n",
       "        'review', 'offers', 'video', 'timeRequired', 'exampleOfWork',\n",
       "        'isPartOf', 'mentions', 'recordedAt', 'inLanguage', 'isBasedOn',\n",
       "        'publisher', 'name', 'character', 'educationalUse', 'audience',\n",
       "        'accessibilityFeature', 'copyrightYear', 'publication', 'producer',\n",
       "        'hasPart', 'text', 'translator'], dtype=object),\n",
       " 'thing creativework book': array(['locationCreated', 'review', 'recordedAt', 'publisher',\n",
       "        'publication', 'educationalUse', 'additionalType', 'provider',\n",
       "        'character', 'position', 'dateModified', 'video', 'hasPart',\n",
       "        'isPartOf', 'isBasedOn', 'name', 'version', 'material', 'offers',\n",
       "        'comment', 'inLanguage', 'producer', 'image', 'identifier'],\n",
       "       dtype=object),\n",
       " 'thing creativework movie': array(['recordedAt', 'musicBy', 'character', 'workExample',\n",
       "        'educationalUse', 'releasedEvent', 'actor', 'author', 'version',\n",
       "        'isFamilyFriendly', 'material', 'publisher', 'producer',\n",
       "        'accountablePerson', 'commentCount', 'alternativeHeadline',\n",
       "        'headline', 'timeRequired', 'potentialAction', 'isPartOf',\n",
       "        'exampleOfWork', 'hasPart', 'isBasedOn', 'position', 'video',\n",
       "        'accessibilityHazard', 'mentions', 'provider', 'name', 'audio',\n",
       "        'copyrightYear', 'contentRating', 'creator', 'award', 'director',\n",
       "        'productionCompany', 'genre', 'schemaVersion', 'trailer',\n",
       "        'publication', 'audience', 'countryOfOrigin', 'comment',\n",
       "        'alternateName', 'inLanguage', 'offers', 'identifier',\n",
       "        'publishingPrinciples', 'text', 'contentLocation',\n",
       "        'locationCreated', 'translator', 'review', 'image'], dtype=object),\n",
       " 'thing creativework musicrecording': array(['producer', 'inAlbum', 'video', 'byArtist', 'character',\n",
       "        'provider', 'releasedEvent', 'author', 'recordedAt', 'position',\n",
       "        'additionalType', 'offers', 'award', 'audience', 'educationalUse',\n",
       "        'recordingOf', 'version', 'hasPart', 'isPartOf', 'name', 'comment',\n",
       "        'isBasedOn', 'publication'], dtype=object),\n",
       " 'thing creativework tvseries': array(['isBasedOn', 'image', 'version', 'award', 'position', 'musicBy',\n",
       "        'countryOfOrigin', 'educationalUse', 'timeRequired', 'workExample',\n",
       "        'isPartOf', 'hasPart', 'name', 'inLanguage', 'provider',\n",
       "        'publication', 'recordedAt', 'material', 'comment', 'publisher',\n",
       "        'episode', 'offers', 'author', 'producer'], dtype=object),\n",
       " 'thing event': array(['offers', 'location', 'identifier', 'translator', 'performer',\n",
       "        'previousStartDate', 'endDate', 'attendee', 'duration',\n",
       "        'recordedIn', 'startDate', 'eventStatus', 'director',\n",
       "        'workFeatured', 'description', 'image', 'potentialAction',\n",
       "        'organizer', 'funder', 'contributor', 'actor', 'subEvent',\n",
       "        'sponsor', 'remainingAttendeeCapacity', 'workPerformed', 'review',\n",
       "        'audience', 'isAccessibleForFree', 'inLanguage', 'typicalAgeRange',\n",
       "        'name', 'additionalType', 'composer'], dtype=object),\n",
       " 'thing event educationevent': array(['organizer', 'image', 'duration', 'audience', 'doorTime',\n",
       "        'director', 'previousStartDate', 'endDate', 'workPerformed',\n",
       "        'location', 'workFeatured', 'eventStatus', 'sponsor', 'subEvent',\n",
       "        'composer', 'inLanguage', 'attendee', 'name', 'actor',\n",
       "        'recordedIn', 'translator', 'offers', 'performer', 'review',\n",
       "        'identifier', 'description', 'contributor'], dtype=object),\n",
       " 'thing event sportsevent': array(['review', 'director', 'competitor', 'duration', 'name', 'endDate',\n",
       "        'audience', 'attendee', 'offers', 'recordedIn', 'inLanguage',\n",
       "        'awayTeam', 'sponsor', 'remainingAttendeeCapacity', 'description',\n",
       "        'typicalAgeRange', 'homeTeam', 'actor', 'performer', 'organizer',\n",
       "        'location', 'composer', 'identifier', 'translator', 'image',\n",
       "        'contributor'], dtype=object),\n",
       " 'thing intangible': array(['potentialAction', 'additionalType', 'description', 'name',\n",
       "        'mainEntityOfPage', 'disambiguatingDescription', 'image',\n",
       "        'alternateName', 'identifier', 'url'], dtype=object),\n",
       " 'thing intangible computerlanguage': array(['image', 'mainEntityOfPage', 'url', 'additionalType',\n",
       "        'alternateName', 'identifier', 'name', 'description',\n",
       "        'potentialAction'], dtype=object),\n",
       " 'thing intangible language': array(['potentialAction', 'identifier', 'name', 'image', 'description',\n",
       "        'url', 'alternateName', 'additionalType'], dtype=object),\n",
       " 'thing intangible service': array(['category', 'logo', 'url', 'provider', 'image', 'name',\n",
       "        'serviceType', 'description', 'identifier', 'isRelatedTo', 'brand',\n",
       "        'serviceOutput', 'availableChannel', 'additionalType',\n",
       "        'alternateName', 'isSimilarTo', 'offers', 'review', 'award',\n",
       "        'audience', 'broker', 'areaServed'], dtype=object),\n",
       " 'thing intangible service financialproduct': array(['image', 'annualPercentageRate', 'potentialAction', 'description',\n",
       "        'name', 'interestRate', 'review', 'serviceOutput',\n",
       "        'aggregateRating', 'hasOfferCatalog', 'providerMobility',\n",
       "        'identifier', 'alternateName', 'serviceType', 'category', 'offers',\n",
       "        'award', 'audience', 'feesAndCommissionsSpecification', 'provider',\n",
       "        'hoursAvailable', 'isRelatedTo', 'brand', 'isSimilarTo', 'broker',\n",
       "        'logo', 'url', 'areaServed'], dtype=object),\n",
       " 'thing medicalentity anatomicalsystem': array(['comprisedOf', 'relevantSpeciality', 'legalStatus',\n",
       "        'associatedPathophysiology', 'recognizingAuthority', 'study',\n",
       "        'image', 'medicineSystem', 'relatedCondition', 'identifier',\n",
       "        'relatedTherapy', 'potentialAction', 'additionalType',\n",
       "        'description', 'relatedStructure', 'url', 'name', 'code',\n",
       "        'guideline', 'alternateName'], dtype=object),\n",
       " 'thing medicalentity medicalcondition': array(['associatedAnatomy', 'primaryPrevention', 'description', 'study',\n",
       "        'pathophysiology', 'expectedPrognosis', 'name',\n",
       "        'differentialDiagnosis', 'riskFactor', 'epidemiology', 'stage',\n",
       "        'naturalProgression', 'cause', 'drug', 'possibleComplication',\n",
       "        'potentialAction', 'medicineSystem', 'guideline', 'identifier',\n",
       "        'signOrSymptom', 'image', 'recognizingAuthority', 'subtype',\n",
       "        'alternateName', 'possibleTreatment', 'status',\n",
       "        'secondaryPrevention', 'url', 'typicalTest', 'additionalType',\n",
       "        'code'], dtype=object),\n",
       " 'thing medicalentity medicalintangible': array(['image', 'potentialAction', 'description', 'code', 'study',\n",
       "        'legalStatus', 'recognizingAuthority', 'relevantSpeciality',\n",
       "        'name', 'identifier', 'guideline', 'url', 'medicineSystem',\n",
       "        'additionalType', 'alternateName'], dtype=object),\n",
       " 'thing medicalentity medicalprocedure': array(['relevantSpeciality', 'anatomicalSystem', 'medicalSpecimen',\n",
       "        'preparation', 'medicineSystem', 'newborn', 'medical Report',\n",
       "        'anatomicalStructure', 'medicalUnit', 'url', 'study', 'image',\n",
       "        'identifier', 'medicalRoom', 'name', 'howPerformed', 'careReason',\n",
       "        'physicalExamination', 'description', 'medicalDepartment',\n",
       "        'medicalClinic', 'careProvider', 'indication', 'guideline', 'code',\n",
       "        'followup', 'adverseEvent'], dtype=object),\n",
       " 'thing medicalentity parasiticdisease': array(['potentialAction', 'medicineSystem', 'relevantSpeciality', 'study',\n",
       "        'guideline', 'url', 'legalStatus', 'identifier', 'additionalType',\n",
       "        'name', 'code', 'image', 'description', 'alternateName',\n",
       "        'recognizingAuthority'], dtype=object),\n",
       " 'thing medicalentity substance': array(['study', 'maximumIntake', 'potentialAction', 'name', 'identifier',\n",
       "        'description', 'activeIngredient', 'alternateName',\n",
       "        'recognizingAuthority', 'disambiguatingDescription',\n",
       "        'medicineSystem', 'code', 'image', 'guideline', 'url',\n",
       "        'additionalType', 'legalStatus', 'relevantSpeciality'],\n",
       "       dtype=object),\n",
       " 'thing medicalentity substance drug': array(['alcoholWarning', 'dosageForm', 'maximumIntake',\n",
       "        'isAvailableGenerically', 'medicineSystem', 'warning', 'drugUnit',\n",
       "        'prescriptionStatus', 'study', 'overdosage', 'cost',\n",
       "        'activeIngredient', 'relatedDrug', 'image', 'manufacturer',\n",
       "        'identifier', 'description', 'isProprietary', 'guideline',\n",
       "        'doseSchedule', 'interactingDrug', 'name', 'code', 'url',\n",
       "        'additionalType', 'drugClass'], dtype=object),\n",
       " 'thing organization airline': array(['parentOrganization', 'numberOfEmployees', 'address', 'name',\n",
       "        'hasOfferCatalog', 'areaServed', 'globalLocationNumber', 'seeks',\n",
       "        'location', 'member', 'identifier', 'event', 'brand', 'award',\n",
       "        'contactPoint', 'owns', 'telephone', 'alternateName', 'iataCode',\n",
       "        'boardingPolicy', 'memberOf', 'makesOffer', 'review', 'sponsor',\n",
       "        'image', 'department', 'taxID', 'logo', 'employee', 'description'],\n",
       "       dtype=object),\n",
       " 'thing organization broadcastchannel': array(['review', 'inBroadcastLineup', 'logo', 'broadcastChannelId',\n",
       "        'makesOffer', 'member', 'employee', 'award', 'address',\n",
       "        'alternateName', 'hasPOS', 'owns', 'providesBroadcastService',\n",
       "        'location', 'event', 'identifier', 'department', 'memberOf',\n",
       "        'broadcastServiceTier', 'sponsor', 'aggregateRating', 'brand',\n",
       "        'image', 'name', 'alumni', 'email', 'telephone', 'seeks'],\n",
       "       dtype=object),\n",
       " 'thing organization company': array(['sponsor', 'founder', 'address', 'legalName', 'alternateName',\n",
       "        'brand', 'location', 'department', 'logo', 'identifier',\n",
       "        'globalLocationNumber', 'employee', 'review', 'description',\n",
       "        'owns', 'foundingLocation', 'member', 'hasOfferCatalog', 'image',\n",
       "        'makesOffer', 'telephone', 'name', 'memberOf', 'award', 'event',\n",
       "        'taxID'], dtype=object),\n",
       " 'thing organization corporation': array(['globalLocationNumber', 'name', 'brand', 'makesOffer',\n",
       "        'numberOfEmployees', 'owns', 'logo', 'sponsor', 'isicV4',\n",
       "        'additionalType', 'review', 'founder', 'department', 'location',\n",
       "        'description', 'employee', 'parentOrganization', 'memberOf',\n",
       "        'taxID', 'email', 'telephone', 'event', 'address',\n",
       "        'hasOfferCatalog', 'award', 'foundingDate', 'foundingLocation',\n",
       "        'image', 'areaServed', 'legalName', 'vatID', 'member', 'seeks',\n",
       "        'identifier', 'faxNumber', 'subOrganization', 'hasPOS', 'url',\n",
       "        'contactPoint', 'alumni'], dtype=object),\n",
       " 'thing organization educationalorganization collegeoruniversity': array(['identifier', 'location', 'employee', 'event', 'areaServed',\n",
       "        'numberOfEmployees', 'foundingLocation', 'address',\n",
       "        'hasOfferCatalog', 'taxID', 'makesOffer', 'award', 'department',\n",
       "        'email', 'review', 'founder', 'memberOf', 'alumni', 'legalName',\n",
       "        'member', 'name', 'owns', 'sponsor', 'brand', 'seeks'],\n",
       "       dtype=object),\n",
       " 'thing organization governmentorganization': array(['founder', 'location', 'parentOrganization', 'employee',\n",
       "        'potentialAction', 'legalName', 'identifier', 'department',\n",
       "        'email', 'memberOf', 'owns', 'sponsor', 'numberOfEmployees',\n",
       "        'foundingDate', 'event', 'faxNumber', 'telephone', 'logo', 'name',\n",
       "        'subOrganization', 'award', 'address', 'member', 'taxID', 'review',\n",
       "        'brand', 'seeks'], dtype=object),\n",
       " 'thing organization ngo': array(['member', 'employee', 'location', 'identifier', 'brand', 'seeks',\n",
       "        'potentialAction', 'funder', 'review', 'contactPoint', 'name',\n",
       "        'globalLocationNumber', 'email', 'owns', 'hasOfferCatalog',\n",
       "        'department', 'event', 'founder', 'award', 'subOrganization',\n",
       "        'address', 'taxID', 'memberOf', 'image', 'description', 'alumni'],\n",
       "       dtype=object),\n",
       " 'thing organization performinggroup musicgroup': array(['album', 'name', 'employee', 'event', 'owns', 'genre',\n",
       "        'description', 'additionalType', 'brand', 'address', 'alumni',\n",
       "        'track', 'award', 'member', 'memberOf', 'location', 'logo',\n",
       "        'review', 'image', 'department', 'identifier', 'sponsor'],\n",
       "       dtype=object),\n",
       " 'thing organization sportsorganization': array(['award', 'member', 'logo', 'sport', 'employee', 'owns', 'name',\n",
       "        'founder', 'taxID', 'makesOffer', 'identifier', 'location',\n",
       "        'sponsor', 'memberOf', 'review', 'alternateName', 'areaServed',\n",
       "        'address', 'event', 'foundingLocation', 'aggregateRating',\n",
       "        'alumni', 'image', 'brand', 'foundingDate', 'description',\n",
       "        'department', 'legalName', 'email', 'hasPOS', 'numberOfEmployees',\n",
       "        'isicV4', 'parentOrganization', 'contactPoint', 'telephone',\n",
       "        'seeks'], dtype=object),\n",
       " 'thing person': array(['children', 'gender', 'memberOf', 'hasOfferCatalog',\n",
       "        'homeLocation', 'potentialAction', 'email', 'knows', 'image',\n",
       "        'name', 'jobTitle', 'spouse', 'colleague', 'worksFor', 'sponsor',\n",
       "        'nationality', 'funder', 'deathPlace', 'height', 'identifier',\n",
       "        'faxNumber', 'follows', 'hasPOS', 'relatedTo', 'duns', 'owns',\n",
       "        'workLocation', 'description', 'seeks', 'sibling', 'givenName',\n",
       "        'contactPoint', 'award', 'performerIn', 'affiliation', 'parent',\n",
       "        'alternateName', 'familyName', 'brand', 'deathDate', 'birthPlace',\n",
       "        'taxID', 'makesOffer', 'globalLocationNumber', 'weight',\n",
       "        'birthDate', 'address', 'alumniOf'], dtype=object),\n",
       " 'thing place': array(['image', 'logo', 'event', 'telephone', 'review',\n",
       "        'containedInPlace', 'additionalType', 'openingHoursSpecification',\n",
       "        'specialOpeningHoursSpecification', 'geo', 'description',\n",
       "        'containsPlace', 'potentialAction', 'address', 'hasMap',\n",
       "        'identifier', 'globalLocationNumber', 'faxNumber',\n",
       "        'maximumAttendeeCapacity', 'aggregateRating', 'smokingAllowed',\n",
       "        'alternateName', 'url', 'name', 'additionalProperty', 'isicV4',\n",
       "        'branchCode', 'amenityFeature', 'photo'], dtype=object),\n",
       " 'thing place accommodation': array(['address', 'floorSize', 'specialOpeningHoursSpecification',\n",
       "        'isicV4', 'geo', 'telephone', 'smokingAllowed',\n",
       "        'globalLocationNumber', 'review', 'openingHoursSpecification',\n",
       "        'amenityFeature', 'containedInPlace', 'petsAllowed',\n",
       "        'permittedUsage', 'branchCode', 'image', 'maximumAttendeeCapacity',\n",
       "        'description', 'additionalProperty', 'name', 'identifier', 'event',\n",
       "        'hasMap', 'logo', 'photo', 'url', 'numberOfRooms',\n",
       "        'additionalType', 'containsPlace'], dtype=object),\n",
       " 'thing place landform bodyofwater': array(['photo', 'openingHoursSpecification', 'amenityFeature',\n",
       "        'maximumAttendeeCapacity', 'containsPlace', 'description', 'logo',\n",
       "        'image', 'globalLocationNumber', 'hasMap', 'telephone',\n",
       "        'additionalProperty', 'containedInPlace', 'faxNumber', 'review',\n",
       "        'geo', 'potentialAction', 'additionalType', 'event', 'identifier',\n",
       "        'name', 'address', 'url', 'alternateName', 'branchCode'],\n",
       "       dtype=object),\n",
       " 'thing place landform mountain': array(['geo', 'description', 'hasMap', 'review',\n",
       "        'specialOpeningHoursSpecification', 'openingHoursSpecification',\n",
       "        'maximumAttendeeCapacity', 'containedInPlace', 'containsPlace',\n",
       "        'alternateName', 'address', 'photo', 'isicV4', 'amenityFeature',\n",
       "        'url', 'event', 'name', 'image', 'identifier', 'telephone', 'logo',\n",
       "        'additionalProperty', 'aggregateRating', 'additionalType',\n",
       "        'branchCode'], dtype=object),\n",
       " 'thing place localbusiness entertainmentbusiness': array(['containedInPlace', 'geo', 'image', 'openingHours', 'url',\n",
       "        'address', 'description', 'alternateName',\n",
       "        'specialOpeningHoursSpecification', 'logo', 'additionalType',\n",
       "        'additionalProperty', 'hasMap', 'photo', 'telephone',\n",
       "        'openingHoursSpecification', 'name', 'event', 'identifier',\n",
       "        'review', 'containsPlace', 'priceRange'], dtype=object),\n",
       " 'thing place touristattraction': array(['amenityFeature', 'address', 'specialOpeningHoursSpecification',\n",
       "        'additionalProperty', 'aggregateRating', 'hasMap', 'geo',\n",
       "        'globalLocationNumber', 'photo', 'review', 'url', 'additionalType',\n",
       "        'maximumAttendeeCapacity', 'name', 'event', 'identifier', 'image',\n",
       "        'description', 'logo', 'telephone', 'containedInPlace',\n",
       "        'containsPlace', 'alternateName', 'potentialAction'], dtype=object),\n",
       " 'thing product': array(['productID', 'color', 'alternateName', 'review', 'itemCondition',\n",
       "        'offers', 'isAccessoryOrSparePartFor', 'additionalType', 'model',\n",
       "        'material', 'audience', 'isSimilarTo', 'brand', 'manufacturer',\n",
       "        'award', 'isRelatedTo', 'category', 'releaseDate',\n",
       "        'productionDate', 'name', 'weight', 'image', 'logo',\n",
       "        'isConsumableFor', 'height', 'identifier', 'additionalProperty',\n",
       "        'purchaseDate', 'depth', 'description', 'url', 'aggregateRating',\n",
       "        'potentialAction', 'mpn', 'width'], dtype=object),\n",
       " 'thing product vehicle': array(['vehicleEngine', 'vehicleModelDate', 'color',\n",
       "        'knownVehicleDamages', 'fuelType', 'weight', 'name',\n",
       "        'isConsumableFor', 'mileageFromOdometer', 'vehicleInteriorColor',\n",
       "        'height', 'fuelEfficiency', 'fuelConsumption',\n",
       "        'driveWheelConfiguration', 'offers', 'award',\n",
       "        'vehicleIdentificationNumber', 'image', 'productID', 'brand',\n",
       "        'vehicleInteriorType', 'category', 'isSimilarTo',\n",
       "        'dateVehicleFirstRegistered', 'vehicleSpecialUsage',\n",
       "        'vehicleTransmission', 'identifier', 'vehicleSeatingCapacity',\n",
       "        'manufacturer', 'description', 'model', 'material',\n",
       "        'vehicleConfiguration', 'isRelatedTo', 'review'], dtype=object),\n",
       " 'thing product vehicle car': array(['releaseDate', 'review', 'vehicleInteriorColor', 'model', 'brand',\n",
       "        'manufacturer', 'vehicleTransmission', 'identifier',\n",
       "        'vehicleEngine', 'offers', 'weight', 'image', 'vehicleModelDate',\n",
       "        'driveWheelConfiguration', 'isSimilarTo', 'productID', 'height',\n",
       "        'material', 'sku', 'color', 'logo', 'dateVehicleFirstRegistered',\n",
       "        'vehicleSeatingCapacity', 'vehicleInteriorType', 'purchaseDate',\n",
       "        'knownVehicleDamages', 'name', 'isRelatedTo', 'award',\n",
       "        'isConsumableFor', 'audience', 'category'], dtype=object)}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type2prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type2lowprop = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p@5 1.0\n",
    "p@10 1.0\n",
    "p@20 1.0\n",
    "r@5 0.34775426477190596\n",
    "r@10 0.6410011920176065\n",
    "r@20 0.9743608592665196\n",
    "ndcg@5 0.6745476627692915\n",
    "ndcg@10 0.7743670055333471\n",
    "ndcg@20 0.8649422098938173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runEval(qid2vec, prop2vec):\n",
    "    class CLF():\n",
    "        def __init__(self):\n",
    "\n",
    "            self.queryInput = Input(shape=(100,))\n",
    "            self.propPosInput = Input(shape=(100,))\n",
    "\n",
    "            queryEmbeddingLayer = Dense(100, name=\"uEmb\")\n",
    "            propEmbeddingLayer = Dense(100, name=\"iEmb\")\n",
    "\n",
    "            qEmb = queryEmbeddingLayer(self.queryInput)\n",
    "            pEmb = propEmbeddingLayer(self.propPosInput)\n",
    "\n",
    "            dense = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "            pred = Multiply()([qEmb, pEmb])\n",
    "            self.pred = dense(pred)\n",
    "\n",
    "            self.model = Model(inputs=[self.queryInput, self.propPosInput], outputs=self.pred)\n",
    "            self.model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "    #         self.predictor = Model([self.queryInput, self.propPosInput], [pDot])\n",
    "    #         self.predictor = Model([self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput], [pDot])\n",
    "        def generate_train_data(self, df):\n",
    "            x_query, x_entity, x_type, x_action, x_pos_prop, x_neg_prop, y = [], [], [], [], [], [], []\n",
    "            for name, group in df.groupby(\"query_id\"):\n",
    "                cand_pos_prop = group.property.tolist()\n",
    "                rele2prop = group[['rele_label', 'property']].groupby(\"rele_label\")[\"property\"].apply(list).to_dict()\n",
    "\n",
    "                for idx, row in group.iterrows():\n",
    "\n",
    "                    x_query.append(qid2vec[name])\n",
    "                    x_pos_prop.append(prop2vec[row['property']])\n",
    "                    y.append(1)\n",
    "\n",
    "                cand_neg_prop = list(set.difference(set(type2prop[row['entityType']]), set(cand_pos_prop)))\n",
    "    #             cand_neg_prop = list(set.difference(set(df.property.unique()), set(cand_pos_prop)))\n",
    "                for neg_prop in cand_neg_prop:\n",
    "                    x_query.append(qid2vec[name])\n",
    "                    x_pos_prop.append(prop2vec[neg_prop])\n",
    "                    y.append(0)\n",
    "\n",
    "            x_query = np.array(x_query)\n",
    "            x_entity = np.array(x_entity)\n",
    "            x_type = np.array(x_type)\n",
    "            x_action = np.array(x_action)\n",
    "            x_pos_prop = np.array(x_pos_prop)\n",
    "            x_neg_prop = np.array(x_neg_prop)\n",
    "    #         return [x_entity, x_type, x_action, x_pos_prop, x_neg_prop], np.ones(len(x_query))\n",
    "            return [x_query, x_pos_prop], np.array(y)\n",
    "\n",
    "    df_train = df[df.query_id.isin(trainIds)]\n",
    "    bpr = CLF()\n",
    "\n",
    "    for i in range(5):\n",
    "        x_train, y_train = bpr.generate_train_data(df_train)\n",
    "        print(x_train[0].shape)\n",
    "        history = bpr.model.fit(x_train, y_train, verbose=1)\n",
    "        res = []\n",
    "        for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "            if row['query_id'] not in testIds:\n",
    "                continue\n",
    "            if row['entity'] not in entityInWiki:\n",
    "                continue\n",
    "            qrels = qrel[str(row['query_id'])]\n",
    "            cand_properties = type2prop[row['entityType']]\n",
    "#             cand_properties = list(set(type2prop[row['entityType']].tolist() + list(qrels.keys())))\n",
    "\n",
    "            rank = {}\n",
    "            for p in cand_properties:\n",
    "                score = bpr.model.predict([[qid2vec[row['query_id']]], [prop2vec[p]]])[0][0]\n",
    "                rank[p] = score\n",
    "            rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "            our = evaluate(qrels, rank)\n",
    "            res.append(our)\n",
    "        print(history.history[\"loss\"][0], np.mean(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3785, 100)\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 2s 461us/step - loss: 0.6766\n",
      "0.6765900413772704 0.5217868829295494\n",
      "(3785, 100)\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 0s 84us/step - loss: 0.6097\n",
      "0.6097327130316428 0.531291756179537\n",
      "(3785, 100)\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 0s 82us/step - loss: 0.5673\n",
      "0.5672527287091416 0.5340745098329324\n",
      "(3785, 100)\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 0s 84us/step - loss: 0.5329\n",
      "0.5329114211440243 0.5402163274635307\n",
      "(3785, 100)\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 0s 86us/step - loss: 0.5005\n",
      "0.500480217977748 0.5409132413335264\n"
     ]
    }
   ],
   "source": [
    "runEval(qid2query2MSvec, prop2MSvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40586, 100)\n",
      "Epoch 1/1\n",
      "40586/40586 [==============================] - 6s 156us/step - loss: 1.5421\n",
      "1.542116905747302 0.5190515531894443\n",
      "(40586, 100)\n",
      "Epoch 1/1\n",
      "40586/40586 [==============================] - 4s 91us/step - loss: 1.4715\n",
      "1.4715179094971944 0.5091338674472682\n",
      "(40586, 100)\n",
      "Epoch 1/1\n",
      "40586/40586 [==============================] - 4s 90us/step - loss: 1.4650\n",
      "1.4650341861028195 0.5086894400063741\n",
      "(40586, 100)\n",
      "Epoch 1/1\n",
      "40586/40586 [==============================] - 4s 93us/step - loss: 1.4662\n",
      "1.4661579954437958 0.5094041594885643\n",
      "(40586, 100)\n",
      "Epoch 1/1\n",
      "40586/40586 [==============================] - 4s 91us/step - loss: 1.4626\n",
      "1.4626264764412813 0.5140373406751466\n",
      "(40586, 100)\n",
      "Epoch 1/1\n",
      "40586/40586 [==============================] - 4s 91us/step - loss: 1.4630\n",
      "1.4630207869639176 0.5046157484529858\n",
      "(40586, 100)\n",
      "Epoch 1/1\n",
      "40586/40586 [==============================] - 4s 96us/step - loss: 1.4636\n",
      "1.463594130606659 0.4950205701944584\n",
      "(40586, 100)\n",
      "Epoch 1/1\n",
      "14432/40586 [=========>....................] - ETA: 2s - loss: 1.4633"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-458-4231cd5fbfde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'entity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'action'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'entityType'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`inputs` should be a list or tuple.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m         \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def bpr_triplet_loss(X):\n",
    "    positive_item_latent, negative_item_latent = X\n",
    "\n",
    "    loss = 1 - K.log(K.sigmoid(positive_item_latent - negative_item_latent))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred - 0 * y_true)\n",
    "\n",
    "\n",
    "class BPR():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.queryInput = Input(shape=(100,))\n",
    "#         self.entityTypeInput = Input(shape=(100,))\n",
    "#         self.actionInput = Input(shape=(100,))\n",
    "        \n",
    "        self.propPosInput = Input(shape=(100,))\n",
    "        self.propNegInput = Input(shape=(100,))\n",
    "\n",
    "        queryEmbeddingLayer = Dense(100, name=\"uEmb\")\n",
    "        propEmbeddingLayer = Dense(100, name=\"iEmb\")\n",
    "\n",
    "        self.qEmb = queryEmbeddingLayer(self.queryInput)\n",
    "        self.pEmb = propEmbeddingLayer(self.propPosInput)\n",
    "        self.nEmb = propEmbeddingLayer(self.propNegInput)\n",
    "\n",
    "        pDot = Dot(axes=-1)([self.qEmb, self.pEmb])\n",
    "        nDot = Dot(axes=-1)([self.qEmb, self.nEmb])\n",
    "\n",
    "#         peDot = Dot(axes=-1)([self.dentityInput, self.propPosInput])\n",
    "#         ptDot = Dot(axes=-1)([self.entityTypeInput, self.propPosInput])\n",
    "#         paDot = Dot(axes=-1)([self.actionInput, self.propPosInput])\n",
    "\n",
    "#         neDot = Dot(axes=-1)([self.entityInput, self.propNegInput])\n",
    "#         ntDot = Dot(axes=-1)([self.entityTypeInput, self.propNegInput])\n",
    "#         naDot = Dot(axes=-1)([self.actionInput, self.propNegInput])\n",
    "        \n",
    "#         pDot = Concatenate()([self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput])\n",
    "#         nDot = Concatenate()([self.entityInput, self.entityTypeInput, self.actionInput, self.propNegInput])\n",
    "        \n",
    "        dense = Dense(1, activation=\"sigmoid\")\n",
    "        \n",
    "#         pDot = Multiply()([self.queryInput, self.propPosInput])\n",
    "#         nDot = Multiply()([self.queryInput, self.propNegInput])\n",
    "        \n",
    "#         pDot = Dot(axes=-1)([self.queryInput, self.propPosInput])\n",
    "#         nDot = Dot(axes=-1)([self.queryInput, self.propNegInput])\n",
    "        \n",
    "        pDot = dense(pDot)\n",
    "        nDot = dense(nDot)\n",
    "#         pred = Multiply()([q_emb, t_emb])\n",
    "        #\n",
    "        # diff = Subtract()([pDot, nDot])\n",
    "        #\n",
    "        lammbda_output = Lambda(bpr_triplet_loss, output_shape=(1,))\n",
    "        self.pred = lammbda_output([pDot, nDot])\n",
    "\n",
    "        self.model = Model(inputs=[self.queryInput, self.propPosInput, self.propNegInput], outputs=self.pred)\n",
    "#         self.model = Model(inputs=[self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput, self.propNegInput], outputs=self.pred)\n",
    "#         self.model = Model(inputs=[self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput, self.propNegInput], outputs=self.pred)\n",
    "\n",
    "        self.model.compile(optimizer=\"adam\", loss=identity_loss)\n",
    "        self.predictor = Model([self.queryInput, self.propPosInput], [pDot])\n",
    "#         self.predictor = Model([self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput], [pDot])\n",
    "    def generate_train_data(self, df):\n",
    "        x_query, x_entity, x_type, x_action, x_pos_prop, x_neg_prop, y = [], [], [], [], [], [], []\n",
    "        for name, group in df.groupby(\"query_id\"):\n",
    "            cand_pos_prop = group.property.tolist()\n",
    "            rele2prop = group[['rele_label', 'property']].groupby(\"rele_label\")[\"property\"].apply(list).to_dict()\n",
    "\n",
    "            \n",
    "            for idx, row in group.iterrows():\n",
    "                \n",
    "                propLabel = row['rele_label']\n",
    "                for rele in rele2prop:\n",
    "                    if propLabel > rele:\n",
    "                        for neg_prop in rele2prop[rele]:\n",
    "                            x_query.append(qid2MSvec[name])\n",
    "                            x_pos_prop.append(prop2MSvec[row['property']])\n",
    "                            x_neg_prop.append(prop2MSvec[neg_prop])\n",
    "                \n",
    "                cand_neg_prop = list(set.difference(set(type2prop[row['entityType']]), set(cand_pos_prop)))\n",
    "                for neg_prop in cand_neg_prop:\n",
    "                    x_query.append(qid2MSvec[name])\n",
    "                    x_pos_prop.append(prop2MSvec[row['property']])\n",
    "                    x_neg_prop.append(prop2MSvec[neg_prop])\n",
    "                \n",
    "#                 cand_neg_prop = list(set.difference(set(df.property.unique()), set(cand_pos_prop)))\n",
    "#                 for neg_prop in cand_neg_prop:\n",
    "#                     x_query.append(qid2MSvec[name])\n",
    "                    x_pos_prop.append(prop2MSvec[row['property']])\n",
    "                    x_neg_prop.append(prop2MSvec[neg_prop])\n",
    "                            \n",
    "#                 cand_neg_prop = type2prop[row['entityType']]\n",
    "#                 cand_neg_prop = df.property.unique().tolist()\n",
    "\n",
    "#                 for n in range(int(row['rele_label'])):\n",
    "#                 for n in range(1):\n",
    "#                     if int(row['rele_label']) < 3:\n",
    "#                         break\n",
    "#                     x_entity.append(entity2MSvec[row['entity']])\n",
    "#                     x_type.append(type2MSvec[row['entityType']])\n",
    "#                     x_action.append(action2MSvec[row['action'].replace(\"\\'\", \"\")])\n",
    "#                     x_query.append(qid2MSvec[name])\n",
    "#                     x_pos_prop.append(prop2MSvec[row['property']])\n",
    "#                     neg_prop = random.choice(cand_neg_prop)\n",
    "#                     while neg_prop in cand_pos_prop:\n",
    "#                         neg_prop = random.choice(cand_neg_prop)\n",
    "#                     x_neg_prop.append(prop2MSvec[neg_prop])\n",
    "        x_query = np.array(x_query)\n",
    "        x_entity = np.array(x_entity)\n",
    "        x_type = np.array(x_type)\n",
    "        x_action = np.array(x_action)\n",
    "        x_pos_prop = np.array(x_pos_prop)\n",
    "        x_neg_prop = np.array(x_neg_prop)\n",
    "#         return [x_entity, x_type, x_action, x_pos_prop, x_neg_prop], np.ones(len(x_query))\n",
    "        return [x_query, x_pos_prop, x_neg_prop], np.ones(len(x_query))\n",
    "        \n",
    "        \n",
    "# print(x_query)\n",
    "df_train = df[df.query_id.isin(trainIds)]\n",
    "bpr = BPR()\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    x_train, y_train = bpr.generate_train_data(df_train)\n",
    "    print(x_train[0].shape)\n",
    "    history = bpr.model.fit(x_train, y_train, verbose=1)\n",
    "    res = []\n",
    "    for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "        if row['query_id'] not in testIds:\n",
    "            continue\n",
    "        qrels = qrel[str(row['query_id'])]\n",
    "        cand_properties = type2prop[row['entityType']]\n",
    "            cand_properties = type2prop[row['entityType']]\n",
    "        \n",
    "\n",
    "        rank = {}\n",
    "        for p in cand_properties:\n",
    "#             score = bpr.predictor.predict([[type2MSvec[row['entityType']]], [prop2MSvec[p]]])[0][0]\n",
    "            score = bpr.predictor.predict([[qid2MSvec[row['query_id']]], [prop2MSvec[p]]])[0][0]\n",
    "#             score = bpr.predictor.predict([[x_query], [x_property]])[0][0]\n",
    "            rank[p] = score\n",
    "        rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "        our = evaluate(qrels, rank)\n",
    "\n",
    "        res.append(our)\n",
    "    print(history.history[\"loss\"][0], np.mean(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p@5 0.6377358490566039\n",
      "p@10 0.6121593291404612\n",
      "p@20 0.5825995807127883\n",
      "r@5 0.21082034063385194\n",
      "r@10 0.40143147962953435\n",
      "r@20 0.700012686961253\n",
      "ndcg@5 0.4251717300658441\n",
      "ndcg@10 0.487037580220878\n",
      "ndcg@20 0.5898301992301582\n",
      "p@5 0.671698113207547\n",
      "p@10 0.6405660377358491\n",
      "p@20 0.6463500593882043\n",
      "r@5 0.21764968417489006\n",
      "r@10 0.4063436619523424\n",
      "r@20 0.7617817065067196\n",
      "ndcg@5 0.47091777226069426\n",
      "ndcg@10 0.5126074197810446\n",
      "ndcg@20 0.6381537946013381\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "res2 = []\n",
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "    if row['query_id'] not in testIds:\n",
    "        continue\n",
    "    if row['entity'] not in entityInWiki:\n",
    "        continue\n",
    "    qrels = qrel[str(row['query_id'])]\n",
    "    cand_properties = type2prop[row['entityType']]\n",
    "    \n",
    "    rank = {}\n",
    "    for p in cand_properties:\n",
    "#         score = bpr.predictor.predict([[type2MSvec[row['entityType']]], [prop2MSvec[p]]])[0][0]\n",
    "#         rank[p] = score\n",
    "#         score = cosine_similarity([type2MSvec[row['entityType']]], [prop2MSvec[p]])[0][0]\n",
    "#         rank[p] = score\n",
    "#         if (row['entityType'], p) not in type2prop2popularity:\n",
    "#             rank[p] = -99999\n",
    "#         else:\n",
    "#             rank[p] = type2prop2popularity[(row['entityType'], p)]\n",
    "#         rank[p] = prop2popularity[p]\n",
    "        rank[p] = 1\n",
    "    rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "    our = evaluate(qrels, rank)\n",
    "#     our = evaluate(qrels, cand_properties)\n",
    "    base = evaluate(qrels, list(run[\"1\"][str(row[\"query_id\"])].keys()))\n",
    "\n",
    "    res.append(our)\n",
    "    res2.append(base)\n",
    "# print(np.mean(res))\n",
    "# print(np.mean(res2))\n",
    "# for i in res2:\n",
    "#     print(key, np.mean(res2[key]))\n",
    "\n",
    "keys = {\"%s@%d\" %( i,j): [] for i in [\"p\", \"r\", \"ndcg\"] for j in [5, 10 ,20]}\n",
    "for r in [res, res2]:\n",
    "    for k in keys:\n",
    "        print(k, np.mean([i[k] for i in r]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.autonotebook import tqdm\n",
    "import re\n",
    "import nltk\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "from nltk.corpus import stopwords\n",
    "from pyNTCIREVAL import Labeler\n",
    "from pyNTCIREVAL.metrics import MSnDCG, nERR, nDCG\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectors(queries):\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'application/json',\n",
    "        'Ocp-Apim-Subscription-Key': '924c1505854b4da4a6144a1cce92937f',\n",
    "    }\n",
    "    \n",
    "    queries = [str(i).replace(\"\\'\", \"\") for i in queries]\n",
    "\n",
    "    params = urllib.parse.urlencode({})\n",
    "    \n",
    "    try:\n",
    "        conn = http.client.HTTPSConnection('api.msturing.org')\n",
    "#         conn.request(\"POST\", \"/gen/encode?%s\" % params, '{\"queries\": [\"how to make gingerbread people (in grams)\", \"test AI\"]}', headers)\n",
    "        conn.request(\"POST\", \"/gen/encode?%s\" % params, str({\"queries\": queries}).replace(\"\\'\", \"\\\"\"), headers)\n",
    "        response = conn.getresponse()\n",
    "        data = response.read()\n",
    "        data = json.loads(data)\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "#         print(data)\n",
    "        print(e)\n",
    "#         print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "    \n",
    "    return {data[i]['query']:data[i]['vector'] for i in range(len(data))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "regex = re.compile('[^a-zA-Z0-9]')\n",
    "#First parameter is the replacement, second parameter is your input string\n",
    "def preprocessingText(doc):\n",
    "    doc = regex.sub(' ', doc)\n",
    "    doc = \" \".join([w for w in doc.split() if not w in stop_words])\n",
    "    return doc.lower()\n",
    "\n",
    "def evaluate(qrels, ranked_list):\n",
    "    res = []\n",
    "    grades = [1,2,3,4] # a grade for relevance levels 1 and 2 (Note that level 0 is excluded)\n",
    "    labeler = Labeler(qrels)\n",
    "    labeled_ranked_list = labeler.label(ranked_list)\n",
    "    rel_level_num = 5\n",
    "    xrelnum = labeler.compute_per_level_doc_num(rel_level_num)\n",
    "    metric = MSnDCG(xrelnum, grades, cutoff=10)\n",
    "    result = metric.compute(labeled_ranked_list)\n",
    "    return result\n",
    "\n",
    "trainIds, testIds = [], []\n",
    "for name, group in df.groupby(\"entityType\"):\n",
    "    if group.query_id.nunique() > 1:\n",
    "        ids = list(group.query_id.unique())\n",
    "        mid = int(group.query_id.nunique() / 2)\n",
    "        trainIds.extend(ids[:mid])\n",
    "        testIds.extend(ids[mid:])\n",
    "    else:\n",
    "        ids = list(group.query_id.unique())\n",
    "        trainIds.extend(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/AKG/Test Collection/AKGG/akg_standard_akgg_property_rele.csv\")\n",
    "df_action = pd.read_csv(\"data/AKG/Test Collection/AM/akg_standard_am_verb_object_rele.csv\")\n",
    "\n",
    "with open(\"data/AKG/Formal Run Topics/AKGG_Formal_Run_Topic.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    qid, query, entity, entityType, action = [], [], [], [], []\n",
    "    for p in data['queries']:\n",
    "        qid.append(p['queryId'])\n",
    "        query.append(p['query'])   \n",
    "        entity.append(p['entity'])\n",
    "        entityType.append(' '.join(p['entityTypes']))    \n",
    "        action.append(p['action'])\n",
    "topic = pd.DataFrame({\"query_id\": qid, \"query\": query, \"entity\": entity, \"entityType\": entityType, \"action\":action})\n",
    "for c in [\"query\", \"entityType\", \"action\", \"entity\"]:\n",
    "    topic[c] = topic[c].str.lower().replace(\"\\'\", \"\")\n",
    "    \n",
    "df = df.merge(topic, how=\"inner\", on=\"query_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = pd.read_csv(\"data/wikihowSep.csv\")\n",
    "df_wiki['headline'] = df_wiki['headline'].str.replace(\"\\n\", \"\")\n",
    "df_wiki['title'] = df_wiki['title'].str.replace(\"How to\", \"\")\n",
    "\n",
    "df_wiki['overview'] = [preprocessingText(str(i)) for i in df_wiki['overview']]\n",
    "df_wiki['headline'] = [preprocessingText(str(i)) for i in df_wiki['headline']]\n",
    "df_wiki['text'] = [preprocessingText(str(i)) for i in df_wiki['text']]\n",
    "df_wiki['sectionLabel'] = [preprocessingText(str(i)) for i in df_wiki['sectionLabel']]\n",
    "df_wiki['title'] = [preprocessingText(str(i)) for i in df_wiki['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoVivification(dict):\n",
    "    \"\"\"Implementation of perl's autovivification feature.\"\"\"\n",
    "    def __getitem__(self, item):\n",
    "        try:\n",
    "            return dict.__getitem__(self, item)\n",
    "        except KeyError:\n",
    "            value = self[item] = type(self)()\n",
    "            return value\n",
    "with open(\"data/AKG/Participants Runs/AKGG/akgg-formalrun-cuis.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    run = AutoVivification()\n",
    "    for p in data['runs']:\n",
    "        for res in p['results']:\n",
    "            for prop in res['properties']:\n",
    "                run[p['runid']][str(res['queryid'])][str(prop['property'])] = prop['rank']\n",
    "\n",
    "qids = []\n",
    "props = []\n",
    "for qid in run['1']:\n",
    "    tmp = list(run['1'][str(qid)].keys())\n",
    "    qids.extend([int(qid)] * len(tmp))\n",
    "    props.extend(tmp)\n",
    "df_run = pd.DataFrame({\"query_id\": qids, \"property\": props})\n",
    "df_run = df_run.merge(topic, how=\"left\", on=\"query_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "qrel = collections.defaultdict(dict)\n",
    "for qid, prop, label in df[['query_id', 'property', 'rele_label']].values:\n",
    "    qrel[str(qid)][str(prop)] = int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = df[[\"query_id\", \"entityType\", \"property\"]].append(df_run[[\"query_id\", \"entityType\", \"property\"]])\n",
    "dfp = dfp[dfp.query_id.isin(trainIds)]\n",
    "type2prop = dfp.groupby(\"entityType\")['property'].unique().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(287, 175)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp.property.nunique(), df_run[df_run.query_id.isin(trainIds)].property.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop2popularity = dfp.groupby(\"property\").size().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "type2prop2popularity = dfp.groupby([\"entityType\", \"property\"]).size().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qid2MSvec = {}\n",
    "# for i, j, k, l in df[[\"query_id\", \"entity\", \"action\", \"entityType\"]].drop_duplicates().values:\n",
    "#     q = str(k +\" \" +j + \" \" + l).replace(\"\\'\", \"\")\n",
    "#     try:\n",
    "#         data = getVectors([q])\n",
    "#         qid2MSvec[i] = data[q]\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         print(q)\n",
    "# prop2MSvec = {}\n",
    "# all_properties = dfp.property.unique().tolist()\n",
    "# for i in range(0, len(all_properties), 20):\n",
    "#     data = getVectors(all_properties[i:i+20])\n",
    "#     for i in data:\n",
    "#         prop2MSvec[i] = data[i]\n",
    "\n",
    "# type2MSvec = {}\n",
    "# all_properties = dfp.entityType.unique().tolist()\n",
    "# for i in range(0, len(all_properties), 20):\n",
    "#     data = getVectors(all_properties[i:i+20])\n",
    "#     for i in data:\n",
    "#         type2MSvec[i] = data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2MSvec = {}\n",
    "all_properties = df.entity.unique().tolist()\n",
    "for i in range(0, len(all_properties), 20):\n",
    "    data = getVectors(all_properties[i:i+20])\n",
    "    for i in data:\n",
    "        entity2MSvec[i] = data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Multiply, Dot, Dense, Subtract, Activation, SimpleRNN, Flatten, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "def bpr_triplet_loss(X):\n",
    "    positive_item_latent, negative_item_latent = X\n",
    "\n",
    "    loss = 1 - K.log(K.sigmoid(positive_item_latent - negative_item_latent))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred - 0 * y_true)\n",
    "\n",
    "\n",
    "class BPR():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.queryInput = Input(shape=(100,))\n",
    "        self.propPosInput = Input(shape=(100,))\n",
    "        self.propNegInput = Input(shape=(100,))\n",
    "\n",
    "        queryEmbeddingLayer = Dense(10, name=\"uEmb\")\n",
    "        propEmbeddingLayer = Dense(10, name=\"iEmb\")\n",
    "\n",
    "#         self.qEmb = queryEmbeddingLayer(self.queryInput)\n",
    "#         self.pEmb = propEmbeddingLayer(self.propPosInput)\n",
    "#         self.nEmb = propEmbeddingLayer(self.propNegInput)\n",
    "\n",
    "#         pDot = Dot(axes=-1)([self.qEmb, self.pEmb])\n",
    "#         nDot = Dot(axes=-1)([self.qEmb, self.nEmb])\n",
    "        \n",
    "        dense = Dense(1, activation=\"linear\")\n",
    "        \n",
    "#         pDot = Multiply()([self.queryInput, self.propPosInput])\n",
    "#         nDot = Multiply()([self.queryInput, self.propNegInput])\n",
    "        \n",
    "        pDot = Dot(axes=-1)([self.queryInput, self.propPosInput])\n",
    "        nDot = Dot(axes=-1)([self.queryInput, self.propNegInput])\n",
    "        \n",
    "        pDot = dense(pDot)\n",
    "        nDot = dense(nDot)\n",
    "#         pred = Multiply()([q_emb, t_emb])\n",
    "        #\n",
    "        # diff = Subtract()([pDot, nDot])\n",
    "        #\n",
    "        lammbda_output = Lambda(bpr_triplet_loss, output_shape=(1,))\n",
    "        self.pred = lammbda_output([pDot, nDot])\n",
    "\n",
    "        self.model = Model(inputs=[self.queryInput, self.propPosInput, self.propNegInput], outputs=self.pred)\n",
    "\n",
    "        self.model.compile(optimizer=\"adam\", loss=identity_loss)\n",
    "        self.predictor = Model([self.queryInput, self.propPosInput], [pDot])\n",
    "    def generate_train_data(self, df):\n",
    "        x_query, x_pos_prop, x_neg_prop, y = [], [], [], []\n",
    "        for name, group in df.groupby(\"entityType\"):\n",
    "            cand_pos_prop = group.property.tolist()\n",
    "            for idx, row in group.iterrows():\n",
    "                cand_neg_prop = type2prop[row['entityType']]\n",
    "\n",
    "                for n in range(int(row['rele_label'])):\n",
    "#                 for n in range(1):\n",
    "#                     if int(row['rele_label']) < 3:\n",
    "#                         break\n",
    "                    x_query.append(type2MSvec[name])\n",
    "                    x_pos_prop.append(prop2MSvec[row['property']])\n",
    "                    neg_prop = random.choice(cand_neg_prop)\n",
    "                    while neg_prop in cand_pos_prop:\n",
    "                        neg_prop = random.choice(cand_neg_prop)\n",
    "                    x_neg_prop.append(prop2MSvec[neg_prop])\n",
    "        x_query = np.array(x_query)\n",
    "        x_pos_prop = np.array(x_pos_prop)\n",
    "        x_neg_prop = np.array(x_neg_prop)\n",
    "        return [x_query, x_pos_prop, x_neg_prop], np.ones(len(x_query))\n",
    "        \n",
    "        \n",
    "# print(x_query)\n",
    "df_train = df[df.query_id.isin(trainIds)]\n",
    "bpr = BPR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    x_train, y_train = bpr.generate_train_data(df_train)\n",
    "    bpr.model.fit(x_train, y_train, verbose=0)\n",
    "    res = []\n",
    "    for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "        if row['query_id'] not in testIds:\n",
    "            continue\n",
    "        qrels = qrel[str(row['query_id'])]\n",
    "        cand_properties = type2prop[row['entityType']]\n",
    "\n",
    "        rank = {}\n",
    "        for p in cand_properties:\n",
    "            score = bpr.predictor.predict([[type2MSvec[row['entityType']]], [prop2MSvec[p]]])[0][0]\n",
    "            rank[p] = score\n",
    "        rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "        our = evaluate(qrels, rank)\n",
    "\n",
    "        res.append(our)\n",
    "    print(np.mean(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4439482474153289\n",
      "0.5324041751716548\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "res2 = []\n",
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "#     if row['query_id'] not in testIds:\n",
    "#         continue\n",
    "    qrels = qrel[str(row['query_id'])]\n",
    "    cand_properties = type2prop[row['entityType']]\n",
    "    \n",
    "    rank = {}\n",
    "    for p in cand_properties:\n",
    "#         score = bpr.predictor.predict([[type2MSvec[row['entityType']]], [prop2MSvec[p]]])[0][0]\n",
    "#         rank[p] = score\n",
    "        score = cosine_similarity([type2MSvec[row['entityType']]], [prop2MSvec[p]])[0][0]\n",
    "        rank[p] = score\n",
    "#         if (row['entityType'], p) not in type2prop2popularity:\n",
    "#             rank[p] = -99999\n",
    "#         else:\n",
    "#             rank[p] = type2prop2popularity[(row['entityType'], p)]\n",
    "#         rank[p] = prop2popularity[p]\n",
    "    rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "    our = evaluate(qrels, rank)\n",
    "#     our = evaluate(qrels, cand_properties)\n",
    "    base = evaluate(qrels, list(run[\"1\"][str(row[\"query_id\"])].keys()))\n",
    "\n",
    "    res.append(our)\n",
    "    res2.append(base)\n",
    "print(np.mean(res))\n",
    "print(np.mean(res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

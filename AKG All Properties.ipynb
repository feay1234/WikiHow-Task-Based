{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "import re, nltk, random, os, json\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "from nltk.corpus import stopwords\n",
    "from pyNTCIREVAL import Labeler\n",
    "from pyNTCIREVAL.metrics import MSnDCG, nERR, nDCG\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from rank_bm25 import BM25Okapi\n",
    "import time, os, pickle\n",
    "# keras\n",
    "from keras.layers import Input, LSTM, GlobalMaxPooling1D, GlobalAveragePooling1D, Concatenate, Embedding, Multiply, Dot, Dense, Subtract, Activation, SimpleRNN, Flatten, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectors(queries):\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'application/json',\n",
    "        'Ocp-Apim-Subscription-Key': '924c1505854b4da4a6144a1cce92937f',\n",
    "    }\n",
    "    \n",
    "    queries = [str(i).replace(\"\\'\", \"\") for i in queries]\n",
    "\n",
    "    params = urllib.parse.urlencode({})\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            conn = http.client.HTTPSConnection('api.msturing.org')\n",
    "    #         conn.request(\"POST\", \"/gen/encode?%s\" % params, '{\"queries\": [\"how to make gingerbread people (in grams)\", \"test AI\"]}', headers)\n",
    "            conn.request(\"POST\", \"/gen/encode?%s\" % params, str({\"queries\": queries}).replace(\"\\'\", \"\\\"\"), headers)\n",
    "            response = conn.getresponse()\n",
    "            data = response.read()\n",
    "            data = json.loads(data)\n",
    "            conn.close()\n",
    "            break\n",
    "        except Exception as e:\n",
    "    #         print(data)\n",
    "            time.sleep(5)\n",
    "#         print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "    \n",
    "    return {data[i]['query']:data[i]['vector'] for i in range(len(data))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "regex = re.compile('[^a-zA-Z0-9.]')\n",
    "#First parameter is the replacement, second parameter is your input string\n",
    "def preprocessingText(doc):\n",
    "    doc = regex.sub(' ', doc)\n",
    "#     doc = \" \".join([w for w in doc.split() if not w in stop_words])\n",
    "    return doc.lower()\n",
    "\n",
    "def evaluate(qrels, ranked_list):\n",
    "    res = []\n",
    "    grades = [1,2,3,4] # a grade for relevance levels 1 and 2 (Note that level 0 is excluded)\n",
    "    labeler = Labeler(qrels)\n",
    "    labeled_ranked_list = labeler.label(ranked_list)\n",
    "    rel_level_num = 5\n",
    "    xrelnum = labeler.compute_per_level_doc_num(rel_level_num)\n",
    "    result = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in [5, 10 ,15]:\n",
    "        metric = MSnDCG(xrelnum, grades, cutoff=i)\n",
    "        result[\"ndcg@%d\" % i] = metric.compute(labeled_ranked_list)\n",
    "        \n",
    "        _ranked_list = ranked_list[:i]\n",
    "        result[\"p@%d\" % i] = len(set.intersection(set(qrels.keys()), set(_ranked_list))) / len(_ranked_list)\n",
    "        result[\"r@%d\" % i] = len(set.intersection(set(qrels.keys()), set(_ranked_list))) / len(qrels)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return \" \".join([m.group(0) for m in matches]).lower()\n",
    "\n",
    "def getTermMSvec(all_properties):\n",
    "    tmp = {}\n",
    "    for i in range(0, len(all_properties), 20):\n",
    "        data = getVectors(all_properties[i:i+20])\n",
    "        for i in data:\n",
    "            tmp[i] = data[i]\n",
    "    return tmp\n",
    "\n",
    "def saveDict(d, name):\n",
    "    f = open(name,\"wb\")\n",
    "    pickle.dump(d,f)\n",
    "    f.close()\n",
    "def loadDict(name):\n",
    "    return pickle.load( open(name, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/AKG/Test Collection/AKGG/akg_standard_akgg_property_rele.csv\")\n",
    "df_action = pd.read_csv(\"data/AKG/Test Collection/AM/akg_standard_am_verb_object_rele.csv\")\n",
    "\n",
    "with open(\"data/AKG/Formal Run Topics/AKGG_Formal_Run_Topic.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    qid, query, entity, entityType, action = [], [], [], [], []\n",
    "    for p in data['queries']:\n",
    "        qid.append(p['queryId'])\n",
    "        query.append(p['query'])   \n",
    "        entity.append(p['entity'])\n",
    "        entityType.append(' '.join(p['entityTypes']))    \n",
    "        action.append(p['action'])\n",
    "topic = pd.DataFrame({\"query_id\": qid, \"query\": query, \"entity\": entity, \"entityType\": entityType, \"action\":action})\n",
    "for c in [\"query\", \"entityType\", \"action\", \"entity\"]:\n",
    "    topic[c] = topic[c].str.lower().replace(\"\\'\", \"\")\n",
    "    \n",
    "df = df.merge(topic, how=\"inner\", on=\"query_id\")\n",
    "# df['query'] = df[['action', 'entity', 'entityType']].astype(str).apply(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIds, testIds = [], []\n",
    "for name, group in df.groupby(\"entityType\"):\n",
    "    if group.query_id.nunique() > 1:\n",
    "        ids = list(group.query_id.unique())\n",
    "        mid = int(group.query_id.nunique() / 2)\n",
    "        trainIds.extend(ids[:mid])\n",
    "        testIds.extend(ids[mid:])\n",
    "    else:\n",
    "        ids = list(group.query_id.unique())\n",
    "        trainIds.extend(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/AKG/Formal Run Topics/AM_Formal_Run_Topic.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    qid, entityurl, entity, entityType = [], [], [], []\n",
    "    for p in data['queries'][0]:\n",
    "        qid.append(p['queryId'])\n",
    "        entity.append(p['entity'])\n",
    "        entityType.append(' '.join(p['entityTypes']))    \n",
    "        entityurl.append(p['entityurl'])\n",
    "am_topic = pd.DataFrame({\"query_id\": qid, \"url\": entityurl, \"entity\": entity, \"entityType\": entityType})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = pd.read_csv(\"data/wikihowSep.csv\")\n",
    "df_wiki['headline'] = df_wiki['headline'].str.replace(\"\\n\", \"\")\n",
    "df_wiki['title'] = df_wiki['title'].str.replace(\"How to \", \"\")\n",
    "\n",
    "df_wiki['overview'] = [preprocessingText(str(i)) for i in df_wiki['overview']]\n",
    "df_wiki['headline'] = [preprocessingText(str(i)) for i in df_wiki['headline']]\n",
    "df_wiki['text'] = [preprocessingText(str(i)) for i in df_wiki['text']]\n",
    "df_wiki['sectionLabel'] = [preprocessingText(str(i)) for i in df_wiki['sectionLabel']]\n",
    "df_wiki['title'] = [preprocessingText(str(i)) for i in df_wiki['title']]\n",
    "df_wiki['title'] = [i if not i[-1].isdigit() else i[:-1] for i in df_wiki['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoVivification(dict):\n",
    "    \"\"\"Implementation of perl's autovivification feature.\"\"\"\n",
    "    def __getitem__(self, item):\n",
    "        try:\n",
    "            return dict.__getitem__(self, item)\n",
    "        except KeyError:\n",
    "            value = self[item] = type(self)()\n",
    "            return value\n",
    "with open(\"data/AKG/Participants Runs/AKGG/akgg-formalrun-cuis.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    run = AutoVivification()\n",
    "    for p in data['runs']:\n",
    "        for res in p['results']:\n",
    "            for prop in res['properties']:\n",
    "                run[p['runid']][str(res['queryid'])][str(prop['property'])] = prop['rank']\n",
    "\n",
    "qids = []\n",
    "props = []\n",
    "for qid in run['1']:\n",
    "    tmp = list(run['1'][str(qid)].keys())\n",
    "    qids.extend([int(qid)] * len(tmp))\n",
    "    props.extend(tmp)\n",
    "df_run = pd.DataFrame({\"query_id\": qids, \"property\": props})\n",
    "df_run = df_run.merge(topic, how=\"left\", on=\"query_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "qrel = collections.defaultdict(dict)\n",
    "for qid, prop, label in df[['query_id', 'property', 'rele_label']].values:\n",
    "    qrel[str(qid)][str(prop)] = int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = df[[\"query_id\", \"entityType\", \"property\"]].append(df_run[[\"query_id\", \"entityType\", \"property\"]])\n",
    "dfp = dfp[dfp.query_id.isin(trainIds)]\n",
    "type2prop = dfp.groupby(\"entityType\")['property'].unique().to_dict()\n",
    "prop2popularity = dfp.groupby(\"property\").size().to_dict()\n",
    "type2prop2popularity = dfp.groupby([\"entityType\", \"property\"]).size().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = []\n",
    "for c in [ 'entity', 'entityType', 'action']:\n",
    "    for i in df[c].unique().tolist():\n",
    "        terms.append(i.replace(\"\\'\", \"\"))\n",
    "terms.extend([camel_case_split(i) for i in df.property.unique()])\n",
    "term2MSvec = getTermMSvec(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 993 unique tokens.\n",
      "Indexing word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1209 14:28:57.823349 4570330560 deprecation_wrapper.py:119] From /Users/jarana/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Preparing embedding matrix.\n",
      "Token num: 993, Found Tokens: 963\n"
     ]
    }
   ],
   "source": [
    "def get_pretrain_embeddings(path, word_index, EMBEDDING_DIM=100):\n",
    "    MAX_NUM_WORDS = len(word_index)\n",
    "    BASE_DIR = path + 'data/'\n",
    "    GLOVE_DIR = os.path.join(BASE_DIR, 'w2v')\n",
    "    print('Indexing word vectors.')\n",
    "\n",
    "    embeddings_index = {}\n",
    "    with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    print('Preparing embedding matrix.')\n",
    "\n",
    "    # prepare embedding matrix\n",
    "    num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    found = 0\n",
    "    for word, i in word_index.items():\n",
    "        if i > MAX_NUM_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if embedding_vector.shape[0] == 0:\n",
    "                continue\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            found += 1\n",
    "\n",
    "    print(\"Token num: %d, Found Tokens: %d\" % (len(word_index), found))\n",
    "\n",
    "    # load pre-trained word embeddings into an Embedding layer\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                embeddings_initializer=Constant(embedding_matrix))\n",
    "\n",
    "    return embedding_layer\n",
    "MAX_NUM_WORDS = 1000\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(list(term2MSvec.keys()))\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "path = \"/Users/jarana/workspace/WikiHow-Task-Based/\"\n",
    "max_words = len(word_index)\n",
    "embedding_layer = get_pretrain_embeddings(path, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0baff49b3149>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mMAX_NUM_WORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_NUM_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mqueries\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found %s unique tokens.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "queries = []\n",
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "    queries.append(row['action'] + \" \" + row['entity'] + \" \" + row['entityType'])\n",
    "properties = [camel_case_split(i) for i in df['property'].unique()]\n",
    "\n",
    "MAX_NUM_WORDS = 100000\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(corpus + queries + properties)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f94bbc2546eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/jarana/workspace/WikiHow-Task-Based/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmax_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0membedding_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pretrain_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_index' is not defined"
     ]
    }
   ],
   "source": [
    "path = \"/Users/jarana/workspace/WikiHow-Task-Based/\"\n",
    "max_words = len(word_index)\n",
    "embedding_layer = get_pretrain_embeddings(path, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWord2Vec(embedding_layer, MAX_SEQUENCE_LENGTH=10000):\n",
    "    q_inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    q_emb = GlobalMaxPooling1D()(embedding_layer(q_inp))\n",
    "    model = Model(q_inp, q_emb)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "word2vec = getWord2Vec(embedding_layer)\n",
    "prop2Glovevec = {p: word2vec.predict(pad_sequences(tokenizer.texts_to_sequences([camel_case_split(p)]), maxlen=10000))[0] for p in dfp.property.unique()}\n",
    "qid2Glovevec = {}\n",
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "    q = row['action'] + \" \" + row['entity'] + \" \" + row['entityType']\n",
    "    qid2Glovevec[row['query_id']] = word2vec.predict(pad_sequences(tokenizer.texts_to_sequences([q]), maxlen=10000))[0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfw = df_wiki[df_wiki.title.str.contains(\"|\".join(df.entity.unique().tolist()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "entityInWiki = []\n",
    "for e in df.entity.unique():\n",
    "    tmp = dfw[dfw.title.str.contains(e)]\n",
    "    if len(tmp) > 0:\n",
    "        entityInWiki.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = []\n",
    "# for name, group in df_wiki2.groupby(\"title\"):\n",
    "#     title = name\n",
    "#     overview = group['overview'].unique().tolist()\n",
    "#     text = group['text'].unique().tolist()\n",
    "#     headline = group['headline'].unique().tolist()\n",
    "#     sectionLabel = group['sectionLabel'].unique().tolist()\n",
    "# #     doc = \" \\t \".join([title] + overview + sectionLabel + headline + text)\n",
    "#     doc = \". \".join([title] + [str(i) for i in headline])\n",
    "#     corpus.append(doc)\n",
    "# corpus = [preprocessingText(i) for i in corpus]\n",
    "corpus = df_wiki.title.unique().tolist()\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# qid2doc2MSvec = {}\n",
    "# for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "#     query = row['action'] + \" \" + row['entity'] + \" \" + row['entityType']\n",
    "#     query = query.replace(\"thing\", \"\")\n",
    "#     tokenized_query = query.split(\" \")\n",
    "#     doc = bm25.get_top_n(tokenized_query, corpus, n=1)\n",
    "#     data = getVectors(doc)\n",
    "#     qid2doc2MSvec[row['query_id']] = data[doc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "    query = row['action'] + \" \" + row['entity'] + \" \" + row['entityType']\n",
    "    query = query.replace(\"thing\", \"\")\n",
    "    tokenized_query = query.split(\" \")\n",
    "    doc = bm25.get_top_n(tokenized_query, corpus, n=5)\n",
    "#     data = getVectors(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load : qid2MSvec.pkl\n",
    "import pickle\n",
    "\n",
    "qid2MSvec = pickle.load( open( \"qid2MSvec.pkl\", \"rb\" ) )\n",
    "# qid2MSvec = {}\n",
    "# for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "#     query = row['action'] + \" \" + row['entity']\n",
    "#     if row['query_id'] in qid2MSvec:\n",
    "#         continue\n",
    "#     if row['entity'] not in entityInWiki:\n",
    "#         data = getVectors([query])\n",
    "#         qid2MSvec[row['query_id']] = list(data.values())\n",
    "#         continue        \n",
    "# #     titles = [str(i).replace(\"\\'\", \"\") for i in dfw[dfw.title.str.contains(row['entity'])].text.unique().tolist()]\n",
    "#     titles = dfw[dfw.title.str.contains(row['entity'])].title.unique().tolist()\n",
    "#     data = getTermMSvec([query] + titles)\n",
    "#     rank = {}\n",
    "#     for i in data:\n",
    "#         if i == query:\n",
    "#             continue\n",
    "#         rank[i] = cosine_similarity([data[query]],jj [data[i]])[0][0]\n",
    "#     best_title = sorted(rank.items(), key=lambda x: x[1])[-1][0]\n",
    "#     sentences = []\n",
    "#     tokens = \" \".join(dfw[dfw.title == best_title].text.tolist()).split()\n",
    "#     for i in range(0, len(tokens), 10):\n",
    "#         sentences.append(\" \".join(tokens[i:i+10]))\n",
    "#     data = getTermMSvec([query] + sentences)\n",
    "#     qid2MSvec[row['query_id']] = list(data.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0633824c8ea45dba9e7a750233f6a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=229), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get questions\n",
    "import glob\n",
    "from run_scraper import crawl\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# finish = []\n",
    "# for file in glob.glob(\"akg/*\"):\n",
    "#     finish.append(file.split(\"/\")[1].replace(\"_\", \" \").replace(\".csv\", \"\"))\n",
    "\n",
    "bestTitle2qid = {}\n",
    "for idx, row in tqdm(df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows(), total=229):\n",
    "    query = row['action'] + \" \" + row['entity']\n",
    "    if row['entity'] not in entityInWiki:\n",
    "        continue        \n",
    "    titles = dfw[dfw.title.str.contains(row['entity'])].title.unique().tolist()\n",
    "    data = getTermMSvec([query] + titles)\n",
    "    rank = {}\n",
    "    for i in data:\n",
    "        if i == query:\n",
    "            continue\n",
    "        rank[i] = cosine_similarity([data[query]], [data[i]])[0][0]\n",
    "    best_title = sorted(rank.items(), key=lambda x: x[1])[-1][0]\n",
    "    bestTitle2qid[best_title] = row['query_id']\n",
    "#     try:\n",
    "#         crawl(best_title, \"akg/\", \"no_question\")\n",
    "#     except:\n",
    "#         print(best_title)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# qid2questions2MSvec = {}\n",
    "# for file in glob.glob(\"akg/*\"):\n",
    "#     title = file.split(\"/\")[1].replace(\"_\", \" \").replace(\".csv\", \"\")\n",
    "#     lines = open(file).read().splitlines()\n",
    "#     questions = []\n",
    "#     for i in lines[1:]:\n",
    "#         questions.extend(i.split(\";\"))\n",
    "#     title2questions[title] = list(set(questions))\n",
    "#     data = getVectors([title] + list(set(questions)) )\n",
    "#     qid2questions2MSvec[bestTitle2qid[title]] = list(data.values())\n",
    "qid2questions2MSvec = loadDict(\"qid2questions2MSvec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qid2query2MSvec = {}\n",
    "# queries = []\n",
    "# for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "#     query = preprocessingText(row['action'] + \" \" + row['entity'] + \" \" + row['entityType'])\n",
    "#     qid2query2MSvec[row['query_id']] = getVectors([query])[query]\n",
    "qid2query2MSvec = pickle.load( open( \"qid2query2MSvec.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# properties = []\n",
    "# for i in run[\"1\"]:\n",
    "#     properties.extend(run[\"1\"][i].keys())\n",
    "# properties = list(set(properties + df.property.unique().tolist()))\n",
    "# prop2MSvec = getTermMSvec(properties)\n",
    "prop2MSvec = pickle.load( open( \"prop2MSvec.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLF with Question MS vectors\n",
    "MAX_SEQUENCE_LENGHT = int(np.max([len(qid2MSvec[i]) for i in qid2MSvec])) + 1\n",
    "# def runDocEval(qid2MSvec, prop2MSvec):\n",
    "class CLF():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.queryInput = Input(shape=(MAX_SEQUENCE_LENGHT, 100,))\n",
    "        self.propPosInput = Input(shape=(100,))\n",
    "        lstm = LSTM(100)\n",
    "\n",
    "        queryEmbeddingLayer = Dense(100, name=\"uEmb\")\n",
    "        propEmbeddingLayer = Dense(100, name=\"iEmb\")\n",
    "\n",
    "#         qEmb = queryEmbeddingLayer(lstm(self.queryInput))\n",
    "        qEmb = queryEmbeddingLayer(self.queryInput)\n",
    "        pEmb = propEmbeddingLayer(self.propPosInput)\n",
    "\n",
    "        dense = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "        pred = Multiply()([qEmb, pEmb])\n",
    "        \n",
    "        self.pred = dense(GlobalMaxPooling1D()(pred))\n",
    "#         self.pred = dense(pred)\n",
    "\n",
    "        self.model = Model(inputs=[self.queryInput, self.propPosInput], outputs=self.pred)\n",
    "        self.model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "    def generate_train_data(self, df):\n",
    "        x_query, x_pos_prop, y = [], [], []\n",
    "        for name, group in df.groupby(\"query_id\"):\n",
    "            cand_pos_prop = group.property.tolist()\n",
    "            rele2prop = group[['rele_label', 'property']].groupby(\"rele_label\")[\"property\"].apply(list).to_dict()\n",
    "            for idx, row in group.iterrows():\n",
    "                if name in qid2questions2MSvec:\n",
    "                    x_query.append(qid2questions2MSvec[name] + [qid2query2MSvec[name]])\n",
    "                else:\n",
    "                    x_query.append(qid2MSvec[name] + [qid2query2MSvec[name]])\n",
    "#                 x_query.append([qid2query2MSvec[name]])\n",
    "                x_pos_prop.append(prop2MSvec[row['property']])\n",
    "                y.append(1)\n",
    "        \n",
    "            cand_neg_prop = list(set.difference(set(type2prop[row['entityType']]), set(cand_pos_prop)))\n",
    "#             cand_neg_prop = list(set.difference(set(df.property.unique()), set(cand_pos_prop)))\n",
    "            for neg_prop in cand_neg_prop:\n",
    "                if name in qid2questions2MSvec:\n",
    "                    x_query.append(qid2questions2MSvec[name] + [qid2query2MSvec[name]])\n",
    "                else:\n",
    "                    x_query.append(qid2MSvec[name] + [qid2query2MSvec[name]])\n",
    "#                 x_query.append([qid2query2MSvec[name]])\n",
    "                x_pos_prop.append(prop2MSvec[neg_prop])\n",
    "                y.append(0)\n",
    "\n",
    "        x_query = pad_sequences(x_query,dtype='float32', maxlen=MAX_SEQUENCE_LENGHT)\n",
    "        x_pos_prop = np.array(x_pos_prop)\n",
    "        return [x_query, x_pos_prop], np.array(y)\n",
    "\n",
    "df_train = df[df.query_id.isin(trainIds)]\n",
    "bpr = CLF()\n",
    "x_train, y_train = bpr.generate_train_data(df_train)\n",
    "for i in range(10):\n",
    "    test_num = 0\n",
    "    history = bpr.model.fit(x_train, y_train, verbose=0)\n",
    "    res = {\"%s@%d\" %( i,j): [] for i in [\"p\", \"r\", \"ndcg\"] for j in [5, 10 ,15]}\n",
    "    for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "        if row['query_id'] not in testIds:\n",
    "            continue\n",
    "        if row['entity'] not in entityInWiki:\n",
    "                continue\n",
    "        test_num += 1\n",
    "        qrels = qrel[str(row['query_id'])]\n",
    "        cand_properties = type2prop[row['entityType']]\n",
    "#         cand_properties = list(prop2MSvec.keys())\n",
    "#         cand_properties = list(set(type2prop[row['entityType']].tolist() + list(qrels.keys())))\n",
    "#         cand_properties = list(set(type2prop[row['entityType']].tolist() + list(qrels.keys())))\n",
    "\n",
    "        rank = {}\n",
    "        for p in cand_properties:\n",
    "            if row['query_id'] in qid2questions2MSvec:\n",
    "                score = bpr.model.predict([pad_sequences([qid2questions2MSvec[row['query_id']] + [qid2query2MSvec[row['query_id']]]], dtype='float32', maxlen=MAX_SEQUENCE_LENGHT), np.array([prop2MSvec[p]])])[0][0]\n",
    "            else:    \n",
    "                score = bpr.model.predict([pad_sequences([qid2MSvec[row['query_id']] + [qid2query2MSvec[row['query_id']]]], dtype='float32', maxlen=MAX_SEQUENCE_LENGHT), np.array([prop2MSvec[p]])])[0][0]\n",
    "#             score = bpr.model.predict([pad_sequences([[qid2query2MSvec[row['query_id']]]], dtype='float32', maxlen=MAX_SEQUENCE_LENGHT), np.array([prop2MSvec[p]])])[0][0]\n",
    "            rank[p] = score\n",
    "#         rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "        rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "        our = evaluate(qrels, rank)\n",
    "        for key in res:\n",
    "            res[key].append(our[key])\n",
    "    for key in ['ndcg@15']:\n",
    "        print(key, np.mean(res[key]))\n",
    "# runDocEval(qid2MSvec, prop2MSvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 5s 1ms/step - loss: 0.6863\n",
      "ndcg@15 0.5716840359523024\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 4s 951us/step - loss: 0.6256\n",
      "ndcg@15 0.5819940098938364\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 4s 1ms/step - loss: 0.5937\n",
      "ndcg@15 0.5608354134879565\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 4s 1ms/step - loss: 0.5690\n",
      "ndcg@15 0.5877968688552294\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 4s 1ms/step - loss: 0.5461\n",
      "ndcg@15 0.5932514972455951\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 4s 1ms/step - loss: 0.5229\n",
      "ndcg@15 0.5888175164402596\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 4s 1ms/step - loss: 0.5063\n",
      "ndcg@15 0.5862739048630199\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 4s 1ms/step - loss: 0.4848\n",
      "ndcg@15 0.5835915711406858\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 4s 1ms/step - loss: 0.4604\n",
      "ndcg@15 0.6007156661227928\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 4s 1ms/step - loss: 0.4417\n",
      "ndcg@15 0.6069969299916321\n"
     ]
    }
   ],
   "source": [
    "# Most recent and best performing model\n",
    "MAX_SEQUENCE_LENGHT = int(np.max([len(qid2MSvec[i]) for i in qid2MSvec])) + 1\n",
    "# def runDocEval(qid2MSvec, prop2MSvec):\n",
    "class CLF():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.queryInput = Input(shape=(MAX_SEQUENCE_LENGHT, 100,))\n",
    "        self.propPosInput = Input(shape=(100,))\n",
    "        lstm = LSTM(100)\n",
    "\n",
    "        queryEmbeddingLayer = Dense(100, name=\"uEmb\")\n",
    "        propEmbeddingLayer = Dense(100, name=\"iEmb\")\n",
    "\n",
    "#         qEmb = queryEmbeddingLayer(lstm(self.queryInput))\n",
    "        qEmb = queryEmbeddingLayer(self.queryInput)\n",
    "        pEmb = propEmbeddingLayer(self.propPosInput)\n",
    "\n",
    "        dense = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "        pred = Multiply()([qEmb, pEmb])\n",
    "        \n",
    "        self.pred = dense(GlobalMaxPooling1D()(pred))\n",
    "#         self.pred = dense(pred)\n",
    "\n",
    "        self.model = Model(inputs=[self.queryInput, self.propPosInput], outputs=self.pred)\n",
    "        self.model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "    def generate_train_data(self, df):\n",
    "        x_query, x_pos_prop, y = [], [], []\n",
    "        for name, group in df.groupby(\"query_id\"):\n",
    "            cand_pos_prop = group.property.tolist()\n",
    "            rele2prop = group[['rele_label', 'property']].groupby(\"rele_label\")[\"property\"].apply(list).to_dict()\n",
    "            for idx, row in group.iterrows():\n",
    "                x_query.append(qid2MSvec[name] + [qid2query2MSvec[name]])\n",
    "#                 x_query.append([qid2query2MSvec[name]])\n",
    "                x_pos_prop.append(prop2MSvec[row['property']])\n",
    "                y.append(1)\n",
    "\n",
    "            cand_neg_prop = list(set.difference(set(type2prop[row['entityType']]), set(cand_pos_prop)))\n",
    "#             cand_neg_prop = list(set.difference(set(df.property.unique()), set(cand_pos_prop)))\n",
    "            for neg_prop in cand_neg_prop:\n",
    "                x_query.append(qid2MSvec[name] + [qid2query2MSvec[name]])\n",
    "#                 x_query.append([qid2query2MSvec[name]])\n",
    "                x_pos_prop.append(prop2MSvec[neg_prop])\n",
    "                y.append(0)\n",
    "\n",
    "        x_query = pad_sequences(x_query,dtype='float32', maxlen=MAX_SEQUENCE_LENGHT)\n",
    "        x_pos_prop = np.array(x_pos_prop)\n",
    "        return [x_query, x_pos_prop], np.array(y)\n",
    "\n",
    "df_train = df[df.query_id.isin(trainIds)]\n",
    "bpr = CLF()\n",
    "x_train, y_train = bpr.generate_train_data(df_train)\n",
    "for i in range(10):\n",
    "    test_num = 0\n",
    "    history = bpr.model.fit(x_train, y_train, verbose=0)\n",
    "    res = {\"%s@%d\" %( i,j): [] for i in [\"p\", \"r\", \"ndcg\"] for j in [5, 10 ,15]}\n",
    "    for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "        if row['query_id'] not in testIds:\n",
    "            continue\n",
    "        if row['entity'] not in entityInWiki:\n",
    "                continue\n",
    "        test_num += 1\n",
    "        qrels = qrel[str(row['query_id'])]\n",
    "        cand_properties = type2prop[row['entityType']]\n",
    "#         cand_properties = list(prop2MSvec.keys())\n",
    "#         cand_properties = list(set(type2prop[row['entityType']].tolist() + list(qrels.keys())))\n",
    "#         cand_properties = list(set(type2prop[row['entityType']].tolist() + list(qrels.keys())))\n",
    "\n",
    "        rank = {}\n",
    "        for p in cand_properties:\n",
    "            score = bpr.model.predict([pad_sequences([qid2MSvec[row['query_id']] + [qid2query2MSvec[row['query_id']]]], dtype='float32', maxlen=MAX_SEQUENCE_LENGHT), np.array([prop2MSvec[p]])])[0][0]\n",
    "#             score = bpr.model.predict([pad_sequences([[qid2query2MSvec[row['query_id']]]], dtype='float32', maxlen=MAX_SEQUENCE_LENGHT), np.array([prop2MSvec[p]])])[0][0]\n",
    "            rank[p] = score\n",
    "#         rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "        rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "        our = evaluate(qrels, rank)\n",
    "        for key in res:\n",
    "            res[key].append(our[key])\n",
    "    for key in ['ndcg@15']:\n",
    "        print(key, np.mean(res[key]))\n",
    "# runDocEval(qid2MSvec, prop2MSvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runEval(qid2vec, prop2vec):\n",
    "    class CLF():\n",
    "        def __init__(self):\n",
    "\n",
    "            self.queryInput = Input(shape=(100,))\n",
    "            self.propPosInput = Input(shape=(100,))\n",
    "\n",
    "            queryEmbeddingLayer = Dense(100, name=\"uEmb\")\n",
    "            propEmbeddingLayer = Dense(100, name=\"iEmb\")\n",
    "\n",
    "            qEmb = queryEmbeddingLayer(self.queryInput)\n",
    "            pEmb = propEmbeddingLayer(self.propPosInput)\n",
    "\n",
    "            dense = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "            pred = Multiply()([qEmb, pEmb])\n",
    "            self.pred = dense(pred)\n",
    "\n",
    "            self.model = Model(inputs=[self.queryInput, self.propPosInput], outputs=self.pred)\n",
    "            self.model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "    #         self.predictor = Model([self.queryInput, self.propPosInput], [pDot])\n",
    "    #         self.predictor = Model([self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput], [pDot])\n",
    "        def generate_train_data(self, df):\n",
    "            x_query, x_entity, x_type, x_action, x_pos_prop, x_neg_prop, y = [], [], [], [], [], [], []\n",
    "            for name, group in df.groupby(\"query_id\"):\n",
    "                cand_pos_prop = group.property.tolist()\n",
    "                rele2prop = group[['rele_label', 'property']].groupby(\"rele_label\")[\"property\"].apply(list).to_dict()\n",
    "\n",
    "                for idx, row in group.iterrows():\n",
    "\n",
    "                    x_query.append(qid2vec[name])\n",
    "                    x_pos_prop.append(prop2vec[row['property']])\n",
    "                    y.append(1)\n",
    "\n",
    "                cand_neg_prop = list(set.difference(set(type2prop[row['entityType']]), set(cand_pos_prop)))\n",
    "    #             cand_neg_prop = list(set.difference(set(df.property.unique()), set(cand_pos_prop)))\n",
    "                for neg_prop in cand_neg_prop:\n",
    "                    x_query.append(qid2vec[name])\n",
    "                    x_pos_prop.append(prop2vec[neg_prop])\n",
    "                    y.append(0)\n",
    "\n",
    "            x_query = np.array(x_query)\n",
    "            x_entity = np.array(x_entity)\n",
    "            x_type = np.array(x_type)\n",
    "            x_action = np.array(x_action)\n",
    "            x_pos_prop = np.array(x_pos_prop)\n",
    "            x_neg_prop = np.array(x_neg_prop)\n",
    "    #         return [x_entity, x_type, x_action, x_pos_prop, x_neg_prop], np.ones(len(x_query))\n",
    "            return [x_query, x_pos_prop], np.array(y)\n",
    "\n",
    "    df_train = df[df.query_id.isin(trainIds)]\n",
    "    bpr = CLF()\n",
    "\n",
    "    for i in range(10):\n",
    "        x_train, y_train = bpr.generate_train_data(df_train)\n",
    "        print(x_train[0].shape)\n",
    "        res = {\"%s@%d\" %( i,j): [] for i in [\"p\", \"r\", \"ndcg\"] for j in [5, 10 ,15]}\n",
    "        history = bpr.model.fit(x_train, y_train, verbose=1)\n",
    "        for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "            if row['query_id'] not in testIds:\n",
    "                continue\n",
    "            if row['entity'] not in entityInWiki:\n",
    "                continue\n",
    "            qrels = qrel[str(row['query_id'])]\n",
    "            cand_properties = type2prop[row['entityType']]\n",
    "#             cand_properties = list(set(type2prop[row['entityType']].tolist() + list(qrels.keys())))\n",
    "\n",
    "            rank = {}\n",
    "            for p in cand_properties:\n",
    "                score = bpr.model.predict([[qid2vec[row['query_id']]], [prop2vec[p]]])[0][0]\n",
    "                rank[p] = score\n",
    "            rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "            our = evaluate(qrels, rank)\n",
    "#             res.append(our)\n",
    "            \n",
    "            for key in res:\n",
    "                res[key].append(our[key])\n",
    "        for key in ['ndcg@15']:\n",
    "            print(key, np.mean(res[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3785, 100)\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 1s 384us/step - loss: 0.6791\n",
      "ndcg@15 0.557532011532777\n",
      "(3785, 100)\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 0s 80us/step - loss: 0.6105\n",
      "ndcg@15 0.5729153478538512\n",
      "(3785, 100)\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 0s 77us/step - loss: 0.5699\n",
      "ndcg@15 0.5713174830915543\n",
      "(3785, 100)\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 0s 77us/step - loss: 0.5315\n",
      "ndcg@15 0.5656313224599708\n",
      "(3785, 100)\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 0s 80us/step - loss: 0.4994\n",
      "ndcg@15 0.5702425934497766\n",
      "(3785, 100)\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 0s 80us/step - loss: 0.4669\n",
      "ndcg@15 0.5789620958972643\n",
      "(3785, 100)\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 0s 80us/step - loss: 0.4361\n",
      "ndcg@15 0.5745994579896394\n",
      "(3785, 100)\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 0s 80us/step - loss: 0.4061\n",
      "ndcg@15 0.5675766103516355\n",
      "(3785, 100)\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 0s 80us/step - loss: 0.3789\n",
      "ndcg@15 0.5705560572879054\n",
      "(3785, 100)\n",
      "Epoch 1/1\n",
      "3785/3785 [==============================] - 0s 80us/step - loss: 0.3526\n",
      "ndcg@15 0.5723372308067951\n"
     ]
    }
   ],
   "source": [
    "runEval(qid2query2MSvec, prop2MSvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40586,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_13 to have shape (100,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-188-4e75a124af94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'entity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'action'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'entityType'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_13 to have shape (100,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "def bpr_triplet_loss(X):\n",
    "    positive_item_latent, negative_item_latent = X\n",
    "\n",
    "    loss = 1 - K.log(K.sigmoid(positive_item_latent - negative_item_latent))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred - 0 * y_true)\n",
    "\n",
    "\n",
    "class BPR():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.queryInput = Input(shape=(100,))\n",
    "#         self.entityTypeInput = Input(shape=(100,))\n",
    "#         self.actionInput = Input(shape=(100,))\n",
    "        \n",
    "        self.propPosInput = Input(shape=(100,))\n",
    "        self.propNegInput = Input(shape=(100,))\n",
    "\n",
    "        queryEmbeddingLayer = Dense(100, name=\"uEmb\")\n",
    "        propEmbeddingLayer = Dense(100, name=\"iEmb\")\n",
    "\n",
    "        self.qEmb = queryEmbeddingLayer(self.queryInput)\n",
    "        self.pEmb = propEmbeddingLayer(self.propPosInput)\n",
    "        self.nEmb = propEmbeddingLayer(self.propNegInput)\n",
    "\n",
    "        pDot = Dot(axes=-1)([self.qEmb, self.pEmb])\n",
    "        nDot = Dot(axes=-1)([self.qEmb, self.nEmb])\n",
    "\n",
    "#         peDot = Dot(axes=-1)([self.dentityInput, self.propPosInput])\n",
    "#         ptDot = Dot(axes=-1)([self.entityTypeInput, self.propPosInput])\n",
    "#         paDot = Dot(axes=-1)([self.actionInput, self.propPosInput])\n",
    "\n",
    "#         neDot = Dot(axes=-1)([self.entityInput, self.propNegInput])\n",
    "#         ntDot = Dot(axes=-1)([self.entityTypeInput, self.propNegInput])\n",
    "#         naDot = Dot(axes=-1)([self.actionInput, self.propNegInput])\n",
    "        \n",
    "#         pDot = Concatenate()([self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput])\n",
    "#         nDot = Concatenate()([self.entityInput, self.entityTypeInput, self.actionInput, self.propNegInput])\n",
    "        \n",
    "        dense = Dense(1, activation=\"sigmoid\")\n",
    "        \n",
    "#         pDot = Multiply()([self.queryInput, self.propPosInput])\n",
    "#         nDot = Multiply()([self.queryInput, self.propNegInput])\n",
    "        \n",
    "#         pDot = Dot(axes=-1)([self.queryInput, self.propPosInput])\n",
    "#         nDot = Dot(axes=-1)([self.queryInput, self.propNegInput])\n",
    "        \n",
    "        pDot = dense(pDot)\n",
    "        nDot = dense(nDot)\n",
    "#         pred = Multiply()([q_emb, t_emb])\n",
    "        #\n",
    "        # diff = Subtract()([pDot, nDot])\n",
    "        #\n",
    "        lammbda_output = Lambda(bpr_triplet_loss, output_shape=(1,))\n",
    "        self.pred = lammbda_output([pDot, nDot])\n",
    "\n",
    "        self.model = Model(inputs=[self.queryInput, self.propPosInput, self.propNegInput], outputs=self.pred)\n",
    "#         self.model = Model(inputs=[self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput, self.propNegInput], outputs=self.pred)\n",
    "#         self.model = Model(inputs=[self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput, self.propNegInput], outputs=self.pred)\n",
    "\n",
    "        self.model.compile(optimizer=\"adam\", loss=identity_loss)\n",
    "        self.predictor = Model([self.queryInput, self.propPosInput], [pDot])\n",
    "#         self.predictor = Model([self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput], [pDot])\n",
    "    def generate_train_data(self, df):\n",
    "        x_query, x_entity, x_type, x_action, x_pos_prop, x_neg_prop, y = [], [], [], [], [], [], []\n",
    "        for name, group in df.groupby(\"query_id\"):\n",
    "            cand_pos_prop = group.property.tolist()\n",
    "            rele2prop = group[['rele_label', 'property']].groupby(\"rele_label\")[\"property\"].apply(list).to_dict()\n",
    "\n",
    "            \n",
    "            for idx, row in group.iterrows():\n",
    "                \n",
    "                propLabel = row['rele_label']\n",
    "                for rele in rele2prop:\n",
    "                    if propLabel > rele:\n",
    "                        for neg_prop in rele2prop[rele]:\n",
    "                            x_query.append(qid2MSvec[name])\n",
    "                            x_pos_prop.append(prop2MSvec[row['property']])\n",
    "                            x_neg_prop.append(prop2MSvec[neg_prop])\n",
    "                \n",
    "                cand_neg_prop = list(set.difference(set(type2prop[row['entityType']]), set(cand_pos_prop)))\n",
    "                for neg_prop in cand_neg_prop:\n",
    "                    x_query.append(qid2MSvec[name])\n",
    "                    x_pos_prop.append(prop2MSvec[row['property']])\n",
    "                    x_neg_prop.append(prop2MSvec[neg_prop])\n",
    "                \n",
    "#                 cand_neg_prop = list(set.difference(set(df.property.unique()), set(cand_pos_prop)))\n",
    "#                 for neg_prop in cand_neg_prop:\n",
    "#                     x_query.append(qid2MSvec[name])\n",
    "                    x_pos_prop.append(prop2MSvec[row['property']])\n",
    "                    x_neg_prop.append(prop2MSvec[neg_prop])\n",
    "                            \n",
    "#                 cand_neg_prop = type2prop[row['entityType']]\n",
    "#                 cand_neg_prop = df.property.unique().tolist()\n",
    "\n",
    "#                 for n in range(int(row['rele_label'])):\n",
    "#                 for n in range(1):\n",
    "#                     if int(row['rele_label']) < 3:\n",
    "#                         break\n",
    "#                     x_entity.append(entity2MSvec[row['entity']])\n",
    "#                     x_type.append(type2MSvec[row['entityType']])\n",
    "#                     x_action.append(action2MSvec[row['action'].replace(\"\\'\", \"\")])\n",
    "#                     x_query.append(qid2MSvec[name])\n",
    "#                     x_pos_prop.append(prop2MSvec[row['property']])\n",
    "#                     neg_prop = random.choice(cand_neg_prop)\n",
    "#                     while neg_prop in cand_pos_prop:\n",
    "#                         neg_prop = random.choice(cand_neg_prop)\n",
    "#                     x_neg_prop.append(prop2MSvec[neg_prop])\n",
    "        x_query = np.array(x_query)\n",
    "        x_entity = np.array(x_entity)\n",
    "        x_type = np.array(x_type)\n",
    "        x_action = np.array(x_action)\n",
    "        x_pos_prop = np.array(x_pos_prop)\n",
    "        x_neg_prop = np.array(x_neg_prop)\n",
    "#         return [x_entity, x_type, x_action, x_pos_prop, x_neg_prop], np.ones(len(x_query))\n",
    "        return [x_query, x_pos_prop, x_neg_prop], np.ones(len(x_query))\n",
    "        \n",
    "        \n",
    "# print(x_query)\n",
    "df_train = df[df.query_id.isin(trainIds)]\n",
    "bpr = BPR()\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    x_train, y_train = bpr.generate_train_data(df_train)\n",
    "    print(x_train[0].shape)\n",
    "    history = bpr.model.fit(x_train, y_train, verbose=1)\n",
    "    res = []\n",
    "    for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "        if row['query_id'] not in testIds:\n",
    "            continue\n",
    "        qrels = qrel[str(row['query_id'])]\n",
    "#         cand_properties = type2prop[row['entityType']]\n",
    "        cand_properties = type2prop[row['entityType']]\n",
    "        \n",
    "\n",
    "        rank = {}\n",
    "        for p in cand_properties:\n",
    "#             score = bpr.predictor.predict([[type2MSvec[row['entityType']]], [prop2MSvec[p]]])[0][0]\n",
    "            score = bpr.predictor.predict([[qid2MSvec[row['query_id']]], [prop2MSvec[p]]])[0][0]\n",
    "#             score = bpr.predictor.predict([[x_query], [x_property]])[0][0]\n",
    "            rank[p] = score\n",
    "        rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "        our = evaluate(qrels, rank)\n",
    "\n",
    "        res.append(our)\n",
    "    print(history.history[\"loss\"][0], np.mean(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p@5 0.6377358490566039\n",
      "p@10 0.6121593291404612\n",
      "p@15 0.6090146750524109\n",
      "r@5 0.21082034063385194\n",
      "r@10 0.40143147962953435\n",
      "r@15 0.5702095162757292\n",
      "ndcg@5 0.4251717300658441\n",
      "ndcg@10 0.487037580220878\n",
      "ndcg@15 0.5452583811427126\n",
      "p@5 0.671698113207547\n",
      "p@10 0.6405660377358491\n",
      "p@15 0.6594339622641509\n",
      "r@5 0.21764968417489006\n",
      "r@10 0.4063436619523424\n",
      "r@15 0.6031397662478625\n",
      "ndcg@5 0.47091777226069426\n",
      "ndcg@10 0.5126074197810446\n",
      "ndcg@15 0.5772479732667619\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "res2 = []\n",
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "    if row['query_id'] not in testIds:\n",
    "        continue\n",
    "    if row['entity'] not in entityInWiki:\n",
    "        continue\n",
    "    qrels = qrel[str(row['query_id'])]\n",
    "    cand_properties = type2prop[row['entityType']]\n",
    "    \n",
    "    rank = {}\n",
    "    for p in cand_properties:\n",
    "#         score = bpr.predictor.predict([[type2MSvec[row['entityType']]], [prop2MSvec[p]]])[0][0]\n",
    "#         rank[p] = score\n",
    "#         score = cosine_similarity([type2MSvec[row['entityType']]], [prop2MSvec[p]])[0][0]\n",
    "#         rank[p] = score\n",
    "#         if (row['entityType'], p) not in type2prop2popularity:\n",
    "#             rank[p] = -99999\n",
    "#         else:\n",
    "#         rank[p] = type2prop2popularity[(row['entityType'], p)]\n",
    "#         rank[p] = prop2popularity[p]\n",
    "        rank[p] = 1\n",
    "    rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "    our = evaluate(qrels, rank)\n",
    "#     our = evaluate(qrels, cand_properties)\n",
    "    base = evaluate(qrels, list(run[\"1\"][str(row[\"query_id\"])].keys()))\n",
    "\n",
    "    res.append(our)\n",
    "    res2.append(base)\n",
    "# print(np.mean(res))\n",
    "# print(np.mean(res2))\n",
    "# for i in res2:\n",
    "#     print(key, np.mean(res2[key]))\n",
    "\n",
    "keys = {\"%s@%d\" %( i,j): [] for i in [\"p\", \"r\", \"ndcg\"] for j in [5, 10 ,15]}\n",
    "for r in [res, res2]:\n",
    "    for k in keys:\n",
    "        print(k, np.mean([i[k] for i in r]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.467248908296945"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(qrel[i]) for i in qrel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p@5 0.732075471698113\n",
    "p@10 0.6970649895178196\n",
    "p@15 0.6392033542976939\n",
    "r@5 0.23423827318257484\n",
    "r@10 0.44957616343065254\n",
    "r@15 0.5939206468091041\n",
    "ndcg@5 0.5158856370667873\n",
    "ndcg@10 0.5625110757973671\n",
    "ndcg@15 0.5977080891368343"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

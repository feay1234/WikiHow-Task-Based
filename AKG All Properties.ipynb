{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.autonotebook import tqdm\n",
    "import re\n",
    "import nltk\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "from nltk.corpus import stopwords\n",
    "from pyNTCIREVAL import Labeler\n",
    "from pyNTCIREVAL.metrics import MSnDCG, nERR, nDCG\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectors(queries):\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'application/json',\n",
    "        'Ocp-Apim-Subscription-Key': '924c1505854b4da4a6144a1cce92937f',\n",
    "    }\n",
    "    \n",
    "    queries = [str(i).replace(\"\\'\", \"\") for i in queries]\n",
    "\n",
    "    params = urllib.parse.urlencode({})\n",
    "    \n",
    "    try:\n",
    "        conn = http.client.HTTPSConnection('api.msturing.org')\n",
    "#         conn.request(\"POST\", \"/gen/encode?%s\" % params, '{\"queries\": [\"how to make gingerbread people (in grams)\", \"test AI\"]}', headers)\n",
    "        conn.request(\"POST\", \"/gen/encode?%s\" % params, str({\"queries\": queries}).replace(\"\\'\", \"\\\"\"), headers)\n",
    "        response = conn.getresponse()\n",
    "        data = response.read()\n",
    "        data = json.loads(data)\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "#         print(data)\n",
    "        print(e)\n",
    "#         print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "    \n",
    "    return {data[i]['query']:data[i]['vector'] for i in range(len(data))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "regex = re.compile('[^a-zA-Z0-9]')\n",
    "#First parameter is the replacement, second parameter is your input string\n",
    "def preprocessingText(doc):\n",
    "    doc = regex.sub(' ', doc)\n",
    "    doc = \" \".join([w for w in doc.split() if not w in stop_words])\n",
    "    return doc.lower()\n",
    "\n",
    "def evaluate(qrels, ranked_list):\n",
    "    res = []\n",
    "    grades = [1,2,3,4] # a grade for relevance levels 1 and 2 (Note that level 0 is excluded)\n",
    "    labeler = Labeler(qrels)\n",
    "    labeled_ranked_list = labeler.label(ranked_list)\n",
    "    rel_level_num = 5\n",
    "    xrelnum = labeler.compute_per_level_doc_num(rel_level_num)\n",
    "    metric = MSnDCG(xrelnum, grades, cutoff=10)\n",
    "    result = metric.compute(labeled_ranked_list)\n",
    "    return result\n",
    "\n",
    "trainIds, testIds = [], []\n",
    "for name, group in df.groupby(\"entityType\"):\n",
    "    if group.query_id.nunique() > 1:\n",
    "        ids = list(group.query_id.unique())\n",
    "        mid = int(group.query_id.nunique() / 2)\n",
    "        trainIds.extend(ids[:mid])\n",
    "        testIds.extend(ids[mid:])\n",
    "    else:\n",
    "        ids = list(group.query_id.unique())\n",
    "        trainIds.extend(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/AKG/Test Collection/AKGG/akg_standard_akgg_property_rele.csv\")\n",
    "df_action = pd.read_csv(\"data/AKG/Test Collection/AM/akg_standard_am_verb_object_rele.csv\")\n",
    "\n",
    "with open(\"data/AKG/Formal Run Topics/AKGG_Formal_Run_Topic.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    qid, query, entity, entityType, action = [], [], [], [], []\n",
    "    for p in data['queries']:\n",
    "        qid.append(p['queryId'])\n",
    "        query.append(p['query'])   \n",
    "        entity.append(p['entity'])\n",
    "        entityType.append(' '.join(p['entityTypes']))    \n",
    "        action.append(p['action'])\n",
    "topic = pd.DataFrame({\"query_id\": qid, \"query\": query, \"entity\": entity, \"entityType\": entityType, \"action\":action})\n",
    "for c in [\"query\", \"entityType\", \"action\", \"entity\"]:\n",
    "    topic[c] = topic[c].str.lower().replace(\"\\'\", \"\")\n",
    "    \n",
    "df = df.merge(topic, how=\"inner\", on=\"query_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/AKG/Formal Run Topics/AM_Formal_Run_Topic.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    qid, entityurl, entity, entityType = [], [], [], []\n",
    "    for p in data['queries'][0]:\n",
    "        qid.append(p['queryId'])\n",
    "        entity.append(p['entity'])\n",
    "        entityType.append(' '.join(p['entityTypes']))    \n",
    "        entityurl.append(p['entityurl'])\n",
    "am_topic = pd.DataFrame({\"query_id\": qid, \"url\": entityurl, \"entity\": entity, \"entityType\": entityType})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = pd.read_csv(\"data/wikihowSep.csv\")\n",
    "df_wiki['headline'] = df_wiki['headline'].str.replace(\"\\n\", \"\")\n",
    "df_wiki['title'] = df_wiki['title'].str.replace(\"How to\", \"\")\n",
    "\n",
    "df_wiki['overview'] = [preprocessingText(str(i)) for i in df_wiki['overview']]\n",
    "df_wiki['headline'] = [preprocessingText(str(i)) for i in df_wiki['headline']]\n",
    "df_wiki['text'] = [preprocessingText(str(i)) for i in df_wiki['text']]\n",
    "df_wiki['sectionLabel'] = [preprocessingText(str(i)) for i in df_wiki['sectionLabel']]\n",
    "df_wiki['title'] = [preprocessingText(str(i)) for i in df_wiki['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoVivification(dict):\n",
    "    \"\"\"Implementation of perl's autovivification feature.\"\"\"\n",
    "    def __getitem__(self, item):\n",
    "        try:\n",
    "            return dict.__getitem__(self, item)\n",
    "        except KeyError:\n",
    "            value = self[item] = type(self)()\n",
    "            return value\n",
    "with open(\"data/AKG/Participants Runs/AKGG/akgg-formalrun-cuis.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    run = AutoVivification()\n",
    "    for p in data['runs']:\n",
    "        for res in p['results']:\n",
    "            for prop in res['properties']:\n",
    "                run[p['runid']][str(res['queryid'])][str(prop['property'])] = prop['rank']\n",
    "\n",
    "qids = []\n",
    "props = []\n",
    "for qid in run['1']:\n",
    "    tmp = list(run['1'][str(qid)].keys())\n",
    "    qids.extend([int(qid)] * len(tmp))\n",
    "    props.extend(tmp)\n",
    "df_run = pd.DataFrame({\"query_id\": qids, \"property\": props})\n",
    "df_run = df_run.merge(topic, how=\"left\", on=\"query_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "qrel = collections.defaultdict(dict)\n",
    "for qid, prop, label in df[['query_id', 'property', 'rele_label']].values:\n",
    "    qrel[str(qid)][str(prop)] = int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = df[[\"query_id\", \"entityType\", \"property\"]].append(df_run[[\"query_id\", \"entityType\", \"property\"]])\n",
    "dfp = dfp[dfp.query_id.isin(trainIds)]\n",
    "type2prop = dfp.groupby(\"entityType\")['property'].unique().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(287, 175)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp.property.nunique(), df_run[df_run.query_id.isin(trainIds)].property.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop2popularity = dfp.groupby(\"property\").size().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "type2prop2popularity = dfp.groupby([\"entityType\", \"property\"]).size().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = []\n",
    "for c in [ 'entity', 'entityType', 'action']:\n",
    "    for i in df[c].unique().tolist():\n",
    "        terms.append(i.replace(\"\\'\", \"\"))\n",
    "terms.extend([camel_case_split(i) for i in df.property.unique()])\n",
    "term2MSvec = getTermMSvec(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(term2MSvec.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 1000\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(list(term2MSvec.keys()))\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "def get_pretrain_embeddings(path, word_index, EMBEDDING_DIM=300):\n",
    "    MAX_NUM_WORDS = len(word_index)\n",
    "    BASE_DIR = path + 'data/'\n",
    "    GLOVE_DIR = os.path.join(BASE_DIR, 'w2v')\n",
    "    print('Indexing word vectors.')\n",
    "\n",
    "    embeddings_index = {}\n",
    "    with open(os.path.join(GLOVE_DIR, 'glove.42B.300d.txt'), encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    print('Preparing embedding matrix.')\n",
    "\n",
    "    # prepare embedding matrix\n",
    "    num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    found = 0\n",
    "    for word, i in word_index.items():\n",
    "        if i > MAX_NUM_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if embedding_vector.shape[0] == 0:\n",
    "                continue\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            found += 1\n",
    "\n",
    "    print(\"Token num: %d, Found Tokens: %d\" % (len(word_index), found))\n",
    "\n",
    "    # load pre-trained word embeddings into an Embedding layer\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                embeddings_initializer=Constant(embedding_matrix))\n",
    "\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTermMSvec(all_properties):\n",
    "    tmp = {}\n",
    "    for i in range(0, len(all_properties), 20):\n",
    "        data = getVectors(all_properties[i:i+20])\n",
    "        for i in data:\n",
    "            tmp[i] = data[i]\n",
    "    return tmp\n",
    "\n",
    "# qid2MSvec = {}\n",
    "# for i, j, k, l in df[[\"query_id\", \"entity\", \"action\", \"entityType\"]].drop_duplicates().values:\n",
    "#     q = str(k +\" \" +j + \" \" + l).replace(\"\\'\", \"\")\n",
    "#     try:\n",
    "#         data = getVectors([q])\n",
    "#         qid2MSvec[i] = data[q]\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         print(q)\n",
    "# prop2MSvec = getTermMSvec(dfp.property.unique().tolist())\n",
    "# type2MSvec = getTermMSvec(dfp.entityType.unique().tolist())\n",
    "# action2MSvec = getTermMSvec(df.action.unique().tolist())\n",
    "# entity2MSvec = getTermMSvec(df.entity.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return \" \".join([m.group(0) for m in matches]).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Concatenate, Embedding, Multiply, Dot, Dense, Subtract, Activation, SimpleRNN, Flatten, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "def bpr_triplet_loss(X):\n",
    "    positive_item_latent, negative_item_latent = X\n",
    "\n",
    "    loss = 1 - K.log(K.sigmoid(positive_item_latent - negative_item_latent))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred - 0 * y_true)\n",
    "\n",
    "\n",
    "class BPR():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.entityInput = Input(shape=(100,))\n",
    "        self.entityTypeInput = Input(shape=(100,))\n",
    "        self.actionInput = Input(shape=(100,))\n",
    "        \n",
    "        self.propPosInput = Input(shape=(100,))\n",
    "        self.propNegInput = Input(shape=(100,))\n",
    "\n",
    "        queryEmbeddingLayer = Dense(10, name=\"uEmb\")\n",
    "        propEmbeddingLayer = Dense(10, name=\"iEmb\")\n",
    "\n",
    "#         self.qEmb = queryEmbeddingLayer(self.queryInput)\n",
    "#         self.pEmb = propEmbeddingLayer(self.propPosInput)\n",
    "#         self.nEmb = propEmbeddingLayer(self.propNegInput)\n",
    "\n",
    "#         pDot = Dot(axes=-1)([self.qEmb, self.pEmb])\n",
    "#         nDot = Dot(axes=-1)([self.qEmb, self.nEmb])\n",
    "\n",
    "        peDot = Dot(axes=-1)([self.entityInput, self.propPosInput])\n",
    "        ptDot = Dot(axes=-1)([self.entityTypeInput, self.propPosInput])\n",
    "        paDot = Dot(axes=-1)([self.actionInput, self.propPosInput])\n",
    "\n",
    "        neDot = Dot(axes=-1)([self.entityInput, self.propNegInput])\n",
    "        ntDot = Dot(axes=-1)([self.entityTypeInput, self.propNegInput])\n",
    "        naDot = Dot(axes=-1)([self.actionInput, self.propNegInput])\n",
    "        \n",
    "        pDot = Concatenate()([self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput])\n",
    "        nDot = Concatenate()([self.entityInput, self.entityTypeInput, self.actionInput, self.propNegInput])\n",
    "        \n",
    "        dense = Dense(1, activation=\"linear\")\n",
    "        \n",
    "#         pDot = Multiply()([self.queryInput, self.propPosInput])\n",
    "#         nDot = Multiply()([self.queryInput, self.propNegInput])\n",
    "        \n",
    "#         pDot = Dot(axes=-1)([self.queryInput, self.propPosInput])\n",
    "#         nDot = Dot(axes=-1)([self.queryInput, self.propNegInput])\n",
    "        \n",
    "        pDot = dense(pDot)\n",
    "        nDot = dense(nDot)\n",
    "#         pred = Multiply()([q_emb, t_emb])\n",
    "        #\n",
    "        # diff = Subtract()([pDot, nDot])\n",
    "        #\n",
    "        lammbda_output = Lambda(bpr_triplet_loss, output_shape=(1,))\n",
    "        self.pred = lammbda_output([pDot, nDot])\n",
    "\n",
    "#         self.model = Model(inputs=[self.queryInput, self.propPosInput, self.propNegInput], outputs=self.pred)\n",
    "        self.model = Model(inputs=[self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput, self.propNegInput], outputs=self.pred)\n",
    "\n",
    "        self.model.compile(optimizer=\"adam\", loss=identity_loss)\n",
    "#         self.predictor = Model([self.queryInput, self.propPosInput], [pDot])\n",
    "        self.predictor = Model([self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput], [pDot])\n",
    "    def generate_train_data(self, df):\n",
    "        x_entity, x_type, x_action, x_pos_prop, x_neg_prop, y = [], [], [], [], [], []\n",
    "        for name, group in df.groupby(\"query_id\"):\n",
    "            cand_pos_prop = group.property.tolist()\n",
    "            for idx, row in group.iterrows():\n",
    "#                 cand_neg_prop = type2prop[row['entityType']]\n",
    "                cand_neg_prop = df.property.unique().tolist()\n",
    "\n",
    "                for n in range(int(row['rele_label'])):\n",
    "#                 for n in range(1):\n",
    "#                     if int(row['rele_label']) < 3:\n",
    "#                         break\n",
    "                    x_entity.append(entity2MSvec[row['entity']])\n",
    "                    x_type.append(type2MSvec[row['entityType']])\n",
    "                    x_action.append(action2MSvec[row['action'].replace(\"\\'\", \"\")])\n",
    "                    x_pos_prop.append(prop2MSvec[row['property']])\n",
    "                    neg_prop = random.choice(cand_neg_prop)\n",
    "                    while neg_prop in cand_pos_prop:\n",
    "                        neg_prop = random.choice(cand_neg_prop)\n",
    "                    x_neg_prop.append(prop2MSvec[neg_prop])\n",
    "        x_entity = np.array(x_entity)\n",
    "        x_type = np.array(x_type)\n",
    "        x_action = np.array(x_action)\n",
    "        x_pos_prop = np.array(x_pos_prop)\n",
    "        x_neg_prop = np.array(x_neg_prop)\n",
    "        return [x_entity, x_type, x_action, x_pos_prop, x_neg_prop], np.ones(len(x_query))\n",
    "        \n",
    "        \n",
    "# print(x_query)\n",
    "df_train = df[df.query_id.isin(trainIds)]\n",
    "bpr = BPR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5450881636370655 0.4314965225135292\n",
      "1.5526838118153288 0.4334187952565266\n",
      "1.5433020704107425 0.42797148087480374\n",
      "1.5432055357457692 0.43175021703697974\n",
      "1.5394703051735423 0.4315754185314166\n",
      "1.5428768547973193 0.435186726529109\n",
      "1.5386353054275614 0.43729759427748344\n",
      "1.5342632471693523 0.4369458369801347\n",
      "1.5409849028737876 0.4326755318364465\n",
      "1.5374256969823366 0.42595449066245117\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    x_train, y_train = bpr.generate_train_data(df_train)\n",
    "    history = bpr.model.fit(x_train, y_train, verbose=0)\n",
    "    res = []\n",
    "    for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "        if row['query_id'] not in testIds:\n",
    "            continue\n",
    "        qrels = qrel[str(row['query_id'])]\n",
    "        cand_properties = type2prop[row['entityType']]\n",
    "\n",
    "        rank = {}\n",
    "        for p in cand_properties:\n",
    "#             score = bpr.predictor.predict([[type2MSvec[row['entityType']]], [prop2MSvec[p]]])[0][0]\n",
    "            score = bpr.predictor.predict([[entity2MSvec[row['entity']]], [type2MSvec[row['entityType']]], [action2MSvec[row['action']]], [prop2MSvec[p]]])[0][0]\n",
    "            rank[p] = score\n",
    "        rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "        our = evaluate(qrels, rank)\n",
    "\n",
    "        res.append(our)\n",
    "    print(history.history[\"loss\"][0], np.mean(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4439482474153289\n",
      "0.5324041751716548\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "res2 = []\n",
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "#     if row['query_id'] not in testIds:\n",
    "#         continue\n",
    "    qrels = qrel[str(row['query_id'])]\n",
    "    cand_properties = type2prop[row['entityType']]\n",
    "    \n",
    "    rank = {}\n",
    "    for p in cand_properties:\n",
    "#         score = bpr.predictor.predict([[type2MSvec[row['entityType']]], [prop2MSvec[p]]])[0][0]\n",
    "#         rank[p] = score\n",
    "        score = cosine_similarity([type2MSvec[row['entityType']]], [prop2MSvec[p]])[0][0]\n",
    "        rank[p] = score\n",
    "#         if (row['entityType'], p) not in type2prop2popularity:\n",
    "#             rank[p] = -99999\n",
    "#         else:\n",
    "#             rank[p] = type2prop2popularity[(row['entityType'], p)]\n",
    "#         rank[p] = prop2popularity[p]\n",
    "    rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "    our = evaluate(qrels, rank)\n",
    "#     our = evaluate(qrels, cand_properties)\n",
    "    base = evaluate(qrels, list(run[\"1\"][str(row[\"query_id\"])].keys()))\n",
    "\n",
    "    res.append(our)\n",
    "    res2.append(base)\n",
    "print(np.mean(res))\n",
    "print(np.mean(res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

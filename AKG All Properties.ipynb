{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jarana/anaconda3/envs/keras/lib/python3.7/site-packages/sklearn/feature_extraction/dict_vectorizer.py:6: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## import pandas as pd\n",
    "import numpy as np\n",
    "# from tqdm.autonotebook import tqdm\n",
    "import re, nltk, random, os, json\n",
    "import pandas as pd\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "from nltk.corpus import stopwords\n",
    "from pyNTCIREVAL import Labeler\n",
    "from pyNTCIREVAL.metrics import MSnDCG, nERR, nDCG, AP, RR\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from rank_bm25 import BM25Okapi\n",
    "import time, os, pickle\n",
    "# keras\n",
    "from keras.layers import Input, LSTM, GlobalMaxPooling1D, GlobalAveragePooling1D, Concatenate, Embedding, Multiply, Dot, Dense, Subtract, Activation, SimpleRNN, Flatten, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from modeling import BertRanker\n",
    "from Data import _pad_crop, _mask\n",
    "from modeling_util import SimmatModule\n",
    "\n",
    "br = BertRanker()\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from run_scraper import MultiThreadScraper\n",
    "# wikihows = pd.read_csv(\"data/cedr/wikihow.tsv\", sep=\"\\t\", names=[\"q\", \"qid\", \"text\"]).text.unique().tolist()\n",
    "# title_to_crawl = [i.split(\".\")[0] for i in wikihows]\n",
    "# s = MultiThreadScraper(title_to_crawl, \"data/questions/wikihow/\", \"fail\")\n",
    "# s.run_scraper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikihow = pd.read_csv(\"data/cedr/wikihow.tsv\", sep=\"\\t\", names=[\"q\", \"qid\", \"text\"])\n",
    "for col in [\"question-qw\"]:\n",
    "    for idx, row in wikihow.iterrows():\n",
    "    #     with open(\"data/cedr/query.tsv\", 'a') as fd:\n",
    "    #         fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+row[\"entity\"]+\" \"+row[\"action\"]+\"\\n\")\n",
    "        with open(\"data/cedr/%s.tsv\" % col, 'a') as fd:\n",
    "            query = row['text']\n",
    "            try:\n",
    "                lines = open(\"data/questions/wikihow/%s.csv\" % title[0].replace(\" \", \"_\")).read().splitlines()[1:]\n",
    "            except:\n",
    "#                 fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\" \"+title[0] +\"\\n\")\n",
    "#                 fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\"\\n\")\n",
    "                fd.write(\"question\\t\"+str(row['qid'])+\"\\t\"+query+\"\\n\")\n",
    "                continue\n",
    "            questions = []\n",
    "            for i in lines[1:]:\n",
    "                questions.extend(i.split(\";\"))\n",
    "            questions = \" \".join(list(set(questions)))\n",
    "            if questions == \"\":\n",
    "                fd.write(\"question\\t\"+str(row['qid'])+\"\\t\"+query+\"\\n\")\n",
    "            else:\n",
    "                fd.write(\"question\\t\"+str(row['qid'])+\"\\t\"+ questions +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectors(queries):\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'application/json',\n",
    "        'Ocp-Apim-Subscription-Key': '924c1505854b4da4a6144a1cce92937f',\n",
    "    }\n",
    "    \n",
    "    queries = [str(i).replace(\"\\'\", \"\") for i in queries]\n",
    "\n",
    "    params = urllib.parse.urlencode({})\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            conn = http.client.HTTPSConnection('api.msturing.org')\n",
    "    #         conn.request(\"POST\", \"/gen/encode?%s\" % params, '{\"queries\": [\"how to make gingerbread people (in grams)\", \"test AI\"]}', headers)\n",
    "            conn.request(\"POST\", \"/gen/encode?%s\" % params, str({\"queries\": queries}).replace(\"\\'\", \"\\\"\"), headers)\n",
    "            response = conn.getresponse()\n",
    "            data = response.read()\n",
    "            data = json.loads(data)\n",
    "            conn.close()\n",
    "            break\n",
    "        except Exception as e:\n",
    "    #         print(data)\n",
    "            time.sleep(5)\n",
    "#         print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "    \n",
    "    return {data[i]['query']:data[i]['vector'] for i in range(len(data))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterText(docs, maxlen):\n",
    "    return [\" \".join(i.split(\" \")[:maxlen]) for i in docs]\n",
    "for maxlen in [128]:\n",
    "    queries = [i if i[-1] != \" \" else i[:-1] for i in pd.read_csv(\"data/cedr/query.tsv\", sep=\"\\t\", names=[\"q\", \"qid\", \"text\"]).text.unique().tolist()]\n",
    "    properties = pd.read_csv(\"data/cedr/doc.tsv\", sep=\"\\t\", names=[\"q\", \"qid\", \"text\"]).text.unique().tolist()\n",
    "    wikipedias = pd.read_csv(\"data/cedr/wikipedia.tsv\", sep=\"\\t\", names=[\"q\", \"qid\", \"text\"]).text.unique().tolist()\n",
    "    questions = pd.read_csv(\"data/cedr/question-qq.tsv\", sep=\"\\t\", names=[\"q\", \"qid\", \"text\"]).text.unique().tolist()\n",
    "    wikihows = pd.read_csv(\"data/cedr/wikihow.tsv\", sep=\"\\t\", names=[\"q\", \"qid\", \"text\"]).text.unique().tolist()\n",
    "    question_wikihows = pd.read_csv(\"data/cedr/question-qw.tsv\", sep=\"\\t\", names=[\"q\", \"qid\", \"text\"]).text.unique().tolist()\n",
    "    \n",
    "    wikipedias = filterText(wikipedias, maxlen)\n",
    "    questions = filterText(questions, maxlen)\n",
    "    wikihows = filterText(wikihows, maxlen)\n",
    "    question_wikihows = filterText(question_wikihows, maxlen)\n",
    "    \n",
    "    data = getTermMSvec(queries + properties + wikipedias + questions + wikihows + question_wikihows)\n",
    "    saveDict(data, \"data/cedr/ms%d\" % maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataAll = {}\n",
    "for q in questions:\n",
    "    dataAll[q] = list(getVectors(q.split(\"?\")).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in dataAll:\n",
    "    data[k] = dataAll[k].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "regex = re.compile('[^a-zA-Z0-9.,]')\n",
    "#First parameter is the replacement, second parameter is your input string\n",
    "def preprocessingText(doc,enableStopword=False):\n",
    "    doc = regex.sub(' ', doc)\n",
    "    if enableStopword:\n",
    "        doc = \" \".join([w for w in doc.split() if not w in stop_words])\n",
    "    return doc.lower()\n",
    "\n",
    "def evaluate(qrels, ranked_list):\n",
    "    res = []\n",
    "    grades = [1,2,3,4] # a grade for relevance levels 1 and 2 (Note that level 0 is excluded)\n",
    "    labeler = Labeler(qrels)\n",
    "    labeled_ranked_list = labeler.label(ranked_list)\n",
    "    rel_level_num = 5\n",
    "    xrelnum = labeler.compute_per_level_doc_num(rel_level_num)\n",
    "    result = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in [5, 10 ,15, 20]:\n",
    "        metric = MSnDCG(xrelnum, grades, cutoff=i)\n",
    "        result[\"ndcg@%d\" % i] = metric.compute(labeled_ranked_list)\n",
    "        \n",
    "        nerr = nERR(xrelnum, grades, cutoff=i)\n",
    "        result[\"nerr@%d\" % i] = nerr.compute(labeled_ranked_list)\n",
    "        \n",
    "        \n",
    "        _ranked_list = ranked_list[:i]\n",
    "        result[\"p@%d\" % i] = len(set.intersection(set(qrels.keys()), set(_ranked_list))) / len(_ranked_list)\n",
    "        result[\"r@%d\" % i] = len(set.intersection(set(qrels.keys()), set(_ranked_list))) / len(qrels)\n",
    "    result[\"rp\"] = len(set.intersection(set(qrels.keys()), set(ranked_list[:len(qrels)]))) / len(qrels)\n",
    "    map = AP(xrelnum, grades)\n",
    "    result[\"map\"] = map.compute(labeled_ranked_list)\n",
    "    mrr = RR()\n",
    "    result[\"mrr\"] = mrr.compute(labeled_ranked_list)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return \" \".join([m.group(0) for m in matches]).lower()\n",
    "\n",
    "def getTermMSvec(all_properties):\n",
    "    tmp = {}\n",
    "    for i in range(0, len(all_properties), 20):\n",
    "        data = getVectors(all_properties[i:i+20])\n",
    "        for i in data:\n",
    "            tmp[i] = data[i]\n",
    "    return tmp\n",
    "\n",
    "def saveDict(d, name):\n",
    "    f = open(name,\"wb\")\n",
    "    pickle.dump(d,f)\n",
    "    f.close()\n",
    "def loadDict(name):\n",
    "    return pickle.load( open(name, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/AKG/Test Collection/AKGG/akg_standard_akgg_property_rele.csv\")\n",
    "df_action = pd.read_csv(\"data/AKG/Test Collection/AM/akg_standard_am_verb_object_rele.csv\")\n",
    "\n",
    "with open(\"data/AKG/Formal Run Topics/AKGG_Formal_Run_Topic.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    qid, query, entity, entityType, action = [], [], [], [], []\n",
    "    for p in data['queries']:\n",
    "        qid.append(p['queryId'])\n",
    "        query.append(p['query'])   \n",
    "        entity.append(p['entity'])\n",
    "        entityType.append(' '.join(p['entityTypes']))    \n",
    "        action.append(p['action'])\n",
    "topic = pd.DataFrame({\"query_id\": qid, \"query\": query, \"entity\": entity, \"entityType\": entityType, \"action\":action})\n",
    "for c in [\"query\", \"entityType\", \"action\", \"entity\"]:\n",
    "    topic[c] = topic[c].str.lower().replace(\"\\'\", \"\")\n",
    "    \n",
    "df = df.merge(topic, how=\"inner\", on=\"query_id\")\n",
    "# df['query'] = df[['action', 'entity', 'entityType']].astype(str).apply(' '.join, axis=1)\n",
    "\n",
    "# with open(\"data/AKG/Formal Run Topics/AM_Formal_Run_Topic.json\") as json_file:\n",
    "#     data = json.load(json_file)\n",
    "#     qid, entity, url = [], [], []\n",
    "#     for p in data['queries'][0]:\n",
    "#         qid.append(p['queryId'])\n",
    "#         entity.append(p['entity'].lower())\n",
    "#         url.append(p['entityurl'].split(\"/\")[-1])\n",
    "# topic2 = pd.DataFrame({\"url\":url, \"entity\": entity})\n",
    "# df = df.merge(topic2, how=\"left\", on=\"entity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates()\n",
    "tmp = [i[0]+\" \"+i[1] for i in t[['entity', 'action']].values]\n",
    "t['text']  = [i if i[-1] != \" \" else i[:-1] for i in tmp]\n",
    "t['type'] = [\"query\"] * len(t)\n",
    "t['entityType'] = [i.split()[-1]for i in t.entityType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[['type', 'query_id', 'text', 'entityType']].to_csv(\"/Users/jarana/workspace/WikiHow-Task-Based/data/cedr/akgg-query.tsv\", header=False, index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = df.merge(t[['query_id', 'entityType']], on=\"query_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: pid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-22f8bdea87b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"entityType\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Column not found: {key}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Column not found: pid'"
     ]
    }
   ],
   "source": [
    "cat.groupby(\"entityType\")[\"pid\"].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIds, testIds = [], []\n",
    "for name, group in df.groupby(\"entityType\"):\n",
    "    if group.query_id.nunique() > 1:\n",
    "        ids = list(group.query_id.unique())\n",
    "        mid = int(group.query_id.nunique() / 2)\n",
    "        trainIds.extend(ids[:mid])\n",
    "        testIds.extend(ids[mid:])\n",
    "    else:\n",
    "        ids = list(group.query_id.unique())\n",
    "        trainIds.extend(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = pd.read_csv(\"data/wikihowSep.csv\")\n",
    "df_wiki['headline'] = df_wiki['headline'].str.replace(\"\\n\", \"\")\n",
    "df_wiki['title'] = df_wiki['title'].str.replace(\"How to \", \"\")\n",
    "for col in ['overview', 'headline', 'text', 'sectionLabel', 'title']:\n",
    "    df_wiki[col] = [preprocessingText(str(i), False) for i in df_wiki[col]]\n",
    "    \n",
    "df_wiki['title'] = [i if not i[-1].isdigit() else i[:-1] for i in df_wiki['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoVivification(dict):\n",
    "    \"\"\"Implementation of perl's autovivification feature.\"\"\"\n",
    "    def __getitem__(self, item):\n",
    "        try:\n",
    "            return dict.__getitem__(self, item)\n",
    "        except KeyError:\n",
    "            value = self[item] = type(self)()\n",
    "            return value\n",
    "with open(\"data/AKG/Participants Runs/AKGG/akgg-formalrun-cuis.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    run = AutoVivification()\n",
    "    for p in data['runs']:\n",
    "        for res in p['results']:\n",
    "            for prop in res['properties']:\n",
    "                run[p['runid']][str(res['queryid'])][str(prop['property'])] = prop['rank']\n",
    "\n",
    "qids = []\n",
    "props = []\n",
    "for qid in run['1']:\n",
    "    tmp = list(run['1'][str(qid)].keys())\n",
    "    qids.extend([int(qid)] * len(tmp))\n",
    "    props.extend(tmp)\n",
    "df_run = pd.DataFrame({\"query_id\": qids, \"property\": props})\n",
    "df_run = df_run.merge(topic, how=\"left\", on=\"query_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "qrel = collections.defaultdict(dict)\n",
    "for qid, prop, label in df[['query_id', 'property', 'rele_label']].values:\n",
    "    qrel[str(qid)][str(prop)] = int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = df[[\"query_id\", \"entityType\", \"property\"]].append(df_run[[\"query_id\", \"entityType\", \"property\"]])\n",
    "# _dfp = dfp[dfp.query_id.isin(trainIds)]\n",
    "# type2prop = _dfp.groupby(\"entityType\")['property'].unique().to_dict()\n",
    "# prop2popularity = _dfp.groupby(\"property\").size().to_dict()\n",
    "# type2prop2popularity = _dfp.groupby([\"entityType\", \"property\"]).size().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = []\n",
    "for c in [ 'entity', 'entityType', 'action']:\n",
    "    for i in df[c].unique().tolist():\n",
    "        terms.append(i.replace(\"\\'\", \"\"))\n",
    "terms.extend([camel_case_split(i) for i in df.property.unique()])\n",
    "term2MSvec = getTermMSvec(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfw = df_wiki[df_wiki.title.str.contains(\"|\".join(df.entity.unique().tolist()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bbf1b3f0c01d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mentityInWiki\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mentityInWiki\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfw' is not defined"
     ]
    }
   ],
   "source": [
    "entityInWiki = []\n",
    "for e in df.entity.unique():\n",
    "    tmp = dfw[dfw.title.str.contains(e)]\n",
    "    if len(tmp) > 0:\n",
    "        entityInWiki.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qidInWiki = []\n",
    "for e in entityInWiki:\n",
    "    qidInWiki.append(df[df.entity == e].query_id.tolist()[0])\n",
    "saveDict(qidInWiki, \"qidInWiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "doc2title = {}\n",
    "for name, group in df_wiki.groupby(\"title\"):\n",
    "    doc = []\n",
    "    for col in [\"title\", \"overview\", \"sectionLabel\", \"headline\", \"text\"]:\n",
    "        doc.append(\" \".join(group[col].unique().tolist()))\n",
    "        \n",
    "    doc = \"\\t\".join(doc)\n",
    "    corpus.append(doc)\n",
    "    doc2title[doc] = name\n",
    "    \n",
    "bm25 = BM25Okapi([tokenizer.tokenize(doc) for doc in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prop2MSvec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-6da2354da7ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcand_properties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#         score = cosine_similarity([qid2query2MSvec[row['query_id']]], [prop2MSvec[p]])[0][0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqid2MSvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprop2MSvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mrank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prop2MSvec' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "    if row['query_id'] not in testIds:\n",
    "        continue\n",
    "    if row['entity'] not in entityInWiki:\n",
    "        continue\n",
    "    qrels = qrel[str(row['query_id'])]\n",
    "    cand_properties = type2prop[row['entityType']]\n",
    "\n",
    "    rank = {}\n",
    "    for p in cand_properties:\n",
    "#         score = cosine_similarity([qid2query2MSvec[row['query_id']]], [prop2MSvec[p]])[0][0]\n",
    "        score = cosine_similarity(qid2MSvec[row['query_id']], [prop2MSvec[p]])[0][0]\n",
    "        rank[p] = score\n",
    "    rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "    our = evaluate(qrels, rank)\n",
    "#             res.append(our)\n",
    "\n",
    "    for key in res:\n",
    "        res[key].append(our[key])\n",
    "\n",
    "for key in res:\n",
    "    print(np.mean(res[key]), end=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5048034934497816\t0.5127729257641921\t0.5065138282387192\t0.509533643381508\t0.17278992015155056\t0.35237921409218415\t0.4889972312960076\t0.6305173507019703\t0.3546019188013135\t0.41664410274076863\t0.45913637989681005\t0.5165660856835568\t0.4795895151756526\t0.5089491635212453\t0.5136090981961885\t0.5149396868811922\t0.5054568094950895\t0.6926802818724216\t0.5580711863023917\t\n",
      "0.6829694323144107\t0.6463973799126637\t0.6449417758369723\t0.62648647369773\t0.23146223399890578\t0.4215051531082564\t0.6015380597456212\t0.7495331935641597\t0.48089413935153635\t0.5324041751716548\t0.5825566020969992\t0.6366985862333004\t0.6036407643807893\t0.6226485591459656\t0.6246191060551456\t0.6248695358880084\t0.6149675244438919\t0.8278436265335829\t0.5342167479659979\t\n",
      "nan\tnan\tnan\tnan\tnan\tnan\tnan\tnan\tnan\tnan\tnan\tnan\tnan\tnan\tnan\tnan\tnan\tnan\tnan\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jarana/anaconda3/envs/keras/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/jarana/anaconda3/envs/keras/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "res, res2, res3 = [], [], []\n",
    "for fold in range(5):\n",
    "    _df = pd.read_csv(\"data/cedr/akgg-r-test%d.tsv\" %fold, sep=\"\\t\", usecols=[0], names=[\"qid\"])\n",
    "    testIds = _df.qid.unique()\n",
    "    prop2popularity = df.groupby(\"property\").size().to_dict()\n",
    "\n",
    "    \n",
    "    for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "        if row['query_id'] not in testIds:\n",
    "            continue\n",
    "#         if row['entity'] not in entityInWiki:\n",
    "#             continue\n",
    "        qrels = qrel[str(row['query_id'])]\n",
    "        cand_properties = type2allprop[row['entityType']]\n",
    "        random.shuffle(cand_properties)\n",
    "#         rank = {}\n",
    "#         for p in cand_properties:\n",
    "#             rank[p] = 1\n",
    "#         rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "#         our = evaluate(qrels, rank)\n",
    "        \n",
    "        our = evaluate(qrels, cand_properties)\n",
    "        base = evaluate(qrels, list(run[\"1\"][str(row[\"query_id\"])].keys()))\n",
    "        res.append(our)\n",
    "        res2.append(base)\n",
    "    #     print(key, np.mean(res2[key]))\n",
    "    \n",
    "#         rank = {}\n",
    "#         for p in cand_properties:\n",
    "#             rank[p] = prop2popularity[p] if p in prop2popularity else 0\n",
    "#         rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "#         our = evaluate(qrels, rank)\n",
    "#         res3.append(our)\n",
    "\n",
    "keys = {\"%s@%d\" %( i,j): [] for i in [\"p\", \"r\", \"ndcg\", \"nerr\"] for j in [5, 10 ,15, 20]}\n",
    "keys[\"rp\"] = []\n",
    "keys[\"mrr\"] = []\n",
    "keys[\"map\"] = []\n",
    "for r in [res, res2, res3]:\n",
    "    for k in keys:\n",
    "        print(np.mean([i[k] for i in r]), end=\"\\t\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['additionalType', 'error', 'location', 'potentialAction', 'object',\n",
       "       'startTime', 'actionStatus', 'url', 'image', 'target', 'endTime',\n",
       "       'result', 'instrument', 'description', 'disambiguatingDescription',\n",
       "       'participant', 'alternateName', 'name', 'agent', 'identifier'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type2allprop[row['entityType']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create property index\n",
    "prop2pid = {p:i for i, p in enumerate(dfp.property.unique())}\n",
    "df[\"pid\"] = [prop2pid[i] for i in df['property']]\n",
    "type2allprop = dfp.groupby(\"entityType\")['property'].unique().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.property.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.drop_duplicates(\"query_id\").iterrows():\n",
    "    with open(\"data/cedr/entity.tsv\", 'a') as fd:\n",
    "        fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+ row['entity'] +\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm25 = pd.read_csv(\"data/cedr/query-title-bm25.tsv\", names=[\"q\", \"qid\", \"text\"], sep=\"\\t\")\n",
    "# for col in [\"overview\", \"sectionLabel\", \"headline\", \"text\"]:\n",
    "for col in [\"question-qw\"]:\n",
    "    for idx, row in df.drop_duplicates(\"query_id\").iterrows():\n",
    "    #     with open(\"data/cedr/query.tsv\", 'a') as fd:\n",
    "    #         fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+row[\"entity\"]+\" \"+row[\"action\"]+\"\\n\")\n",
    "        with open(\"data/cedr/%s.tsv\" % col, 'a') as fd:\n",
    "            query = row['action'] + \" \" + row['entity']\n",
    "            query = query.replace(\"\\'\", \"\")\n",
    "#             tokenized_query = query.split(\" \")\n",
    "    #         doc = bm25.get_top_n(tokenized_query, corpus, n=10)\n",
    "    #         titles = [doc2title[i] for i in doc]\n",
    "    #         overview = doc[0].split(\" [SEP] \")[4]\n",
    "    #         text = \" \".join(df_wiki[df_wiki.title == titles[0]].text.unique().tolist())\n",
    "    #         print(overview)\n",
    "    #     break\n",
    "    #         text = \" \".join(df_wiki[df_wiki.title == titles[0]].text.tolist())\n",
    "    #         fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\" \"+titles[0]+ \" \" + text +\"\\n\")\n",
    "    #         fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\" \"+titles[0] +\"\\n\")\n",
    "    \n",
    "#     questions\n",
    "            title = [df_bm25[df_bm25.text.str.contains(query[:10])].text.values[0].replace(query+\" \", \"\")]\n",
    "            try:\n",
    "                lines = open(\"data/questions/wikihow/%s.csv\" % title[0].replace(\" \", \"_\")).read().splitlines()[1:]\n",
    "            except:\n",
    "#                 fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\" \"+title[0] +\"\\n\")\n",
    "#                 fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\"\\n\")\n",
    "                fd.write(\"question\\t\"+str(row['query_id'])+\"\\t\"+query+\"\\n\")\n",
    "                continue\n",
    "            questions = []\n",
    "            for i in lines[1:]:\n",
    "                questions.extend(i.split(\";\"))\n",
    "            questions = \" \".join(list(set(questions)))\n",
    "            if questions == \"\":\n",
    "                fd.write(\"question\\t\"+str(row['query_id'])+\"\\t\"+query+\"\\n\")\n",
    "            else:\n",
    "#             fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\" \"+title[0]+ \" \" + questions +\"\\n\")\n",
    "#             fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\" \" + questions +\"\\n\")\n",
    "                fd.write(\"question\\t\"+str(row['query_id'])+\"\\t\"+ questions +\"\\n\")\n",
    "            \n",
    "#         break\n",
    "            \n",
    "#             text = \" \".join(df_wiki[df_wiki.title == titles[0]][col].unique().tolist())\n",
    "#             fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\" \"+titles[0]+ \" \" + text +\"\\n\")\n",
    "\n",
    "    #         if row['query_id'] not in qid2wiki:\n",
    "    #             fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\"\\n\")\n",
    "    #             doc = bm25.get_top_n(tokenized_query, corpus, n=5)\n",
    "    #             title = [doc2title[i] for i in doc]\n",
    "    #             print(row['action'], \",\", row['entity'], \"\\t\", title)\n",
    "    #             continue\n",
    "    #         BERT\n",
    "    #         titles = dfw[dfw.title.str.contains(row['entity'])].title.unique().tolist()\n",
    "    #         rank = {}\n",
    "    #         for title in titles:\n",
    "    #             rank[title] = bertSim(query, title)\n",
    "    #         best_title = sorted(rank.items(), key=lambda x: x[1])[-1][0]\n",
    "    #         text = \" \".join(df_wiki[df_wiki.title == best_title].text.tolist())\n",
    "    #         fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\" \"+best_title+ \" \"+ text +\"\\n\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm25 = pd.read_csv(\"data/cedr/query-title-bm25.tsv\", names=[\"q\", \"qid\", \"text\"], sep=\"\\t\")\n",
    "for col in [\"overview\", \"headline\", \"sectionLabel\",\"text\"]:\n",
    "    for idx, row in df.drop_duplicates(\"query_id\").iterrows():\n",
    "        with open(\"data/cedr/wiki-%s-bm25.tsv\" % col, 'a') as fd:\n",
    "            query = row['action'] + \" \" + row['entity']\n",
    "            query = query.replace(\"\\'\", \"\")\n",
    "            title = df_bm25[df_bm25.text.str.contains(query[:10])].text.values[0].replace(query+\" \", \"\")\n",
    "            text = \" \".join(df_wiki[df_wiki.title == title][col].unique().tolist())\n",
    "            if text == \"steps\" or text == \"\":\n",
    "                fd.write(\"wiki\\t\"+str(row['query_id'])+\"\\t\"+ query +\"\\n\")                \n",
    "            else:\n",
    "                fd.write(\"wiki\\t\"+str(row['query_id'])+\"\\t\"+ text +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222 spray lacquer ['What does spray lacquer do?', 'Does lacquer make wood waterproof?', 'What does spray lacquer do?', 'Does lacquer seal wood?', 'What is the difference between varnish and lacquer?', 'Is lacquer good for outdoors?', 'What is the difference between varnish and lacquer?', 'Does lacquer turn yellow?', 'What is the difference between varnish and lacquer?', \"What's better lacquer or polyurethane?\", 'How long does it take for spray lacquer to dry?', 'How long does lacquer last?', 'How long does it take for spray lacquer to dry?', 'Should I sand between coats of lacquer?', 'How long does it take for spray lacquer to dry?', 'Does lacquer scratch easily?', 'What is the difference between lacquer and polyurethane finish?', 'What is the best lacquer for wood?', 'What is the difference between lacquer and polyurethane finish?', 'Is lacquer thicker than polyurethane?', 'What is the difference between lacquer and polyurethane finish?', 'Can you brush on lacquer?']       query_id                   property  rele_label    query   entity  \\\n",
      "3655       222                      brand           3  lacquer  lacquer   \n",
      "3656       222            isConsumableFor           2  lacquer  lacquer   \n",
      "3657       222              alternateName           1  lacquer  lacquer   \n",
      "3658       222               manufacturer           2  lacquer  lacquer   \n",
      "3659       222                       logo           2  lacquer  lacquer   \n",
      "3660       222                     offers           2  lacquer  lacquer   \n",
      "3661       222              itemCondition           2  lacquer  lacquer   \n",
      "3662       222                 identifier           1  lacquer  lacquer   \n",
      "3663       222                      model           2  lacquer  lacquer   \n",
      "3664       222            potentialAction           2  lacquer  lacquer   \n",
      "3665       222                     review           4  lacquer  lacquer   \n",
      "3666       222                  productID           1  lacquer  lacquer   \n",
      "3667       222                isSimilarTo           2  lacquer  lacquer   \n",
      "3668       222                      depth           2  lacquer  lacquer   \n",
      "3669       222  isAccessoryOrSparePartFor           2  lacquer  lacquer   \n",
      "3670       222                      award           3  lacquer  lacquer   \n",
      "3671       222                   material           2  lacquer  lacquer   \n",
      "3672       222                      color           3  lacquer  lacquer   \n",
      "\n",
      "         entityType             action  pid  \n",
      "3655  thing product  use acrylic paint  116  \n",
      "3656  thing product  use acrylic paint  270  \n",
      "3657  thing product  use acrylic paint   16  \n",
      "3658  thing product  use acrylic paint  183  \n",
      "3659  thing product  use acrylic paint  113  \n",
      "3660  thing product  use acrylic paint   61  \n",
      "3661  thing product  use acrylic paint  265  \n",
      "3662  thing product  use acrylic paint   18  \n",
      "3663  thing product  use acrylic paint  267  \n",
      "3664  thing product  use acrylic paint   11  \n",
      "3665  thing product  use acrylic paint   63  \n",
      "3666  thing product  use acrylic paint  263  \n",
      "3667  thing product  use acrylic paint  119  \n",
      "3668  thing product  use acrylic paint  272  \n",
      "3669  thing product  use acrylic paint  266  \n",
      "3670  thing product  use acrylic paint   51  \n",
      "3671  thing product  use acrylic paint   30  \n",
      "3672  thing product  use acrylic paint  264  \n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# qid2questions2MSvec = {}\n",
    "for file in glob.glob(\"akgBM25/*\"):\n",
    "    title = file.split(\"/\")[1].replace(\"_\", \" \").replace(\".csv\", \"\")\n",
    "    lines = open(file).read().splitlines()\n",
    "    questions = []\n",
    "    for i in lines[1:]:\n",
    "        questions.extend(i.split(\";\"))\n",
    "    print(bestTitle2qid[title], title, questions, df[df.query_id == bestTitle2qid[title]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    break\n",
    "#     title2questions[title] = list(set(questions))\n",
    "#     data = getVectors([title] + list(set(questions)) )\n",
    "#     qid2questions2MSvec[bestTitle2qid[title]] = list(data.values())\n",
    "# qid2questions2MSvec = loadDict(\"qid2questions2MSvec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmarking have thing action\n",
      "overclock graphics card\n",
      "\n",
      "bodybuilding take thing action\n",
      "begin bodybuilding\n",
      "\n",
      "brainstorming brainstorm thing action\n",
      "develop alternative perspectives decision making\n",
      "\n",
      "chain smoking start thing action\n",
      "quit smoking using allen carr book\n",
      "\n",
      "funding get funding thing action\n",
      "get venture capital investment\n",
      "\n",
      "hair coloring bleach hair back to brown thing action\n",
      "ombre hair\n",
      "\n",
      "hit and run stop at the scene of an accident thing action\n",
      "know whether call police after car accident\n",
      "\n",
      "language acquisition suggest a foreign language thats easy to learn thing action\n",
      "teach adults foreign language\n",
      "\n",
      "mediation help you come up with possible solutions thing action\n",
      "prepare child custody mediation\n",
      "\n",
      "pull up do any aerobic physical activity thing action\n",
      "improve your study routine exercise\n",
      "\n",
      "spoofing attack know who do the spoof  thing action\n",
      "make funny movie spoof\n",
      "\n",
      "spray painting use on fabric thing action\n",
      "spray paint your sofa\n",
      "\n",
      "translation know japanese recipe ingredient translation thing action\n",
      "learn japanese words\n",
      "\n",
      "all about my mother forget thing creativework movie\n",
      "do mastering sims 2 machima\n",
      "\n",
      "back to the future remember thing creativework movie\n",
      "document your child educational milestones\n",
      "\n",
      "braveheart love that movie thing creativework movie\n",
      "hardcore dance\n",
      "\n",
      "give me five give fives thing creativework musicrecording\n",
      "make viral video\n",
      "\n",
      "on the road follow thing creativework book\n",
      "write book title\n",
      "\n",
      "our daughter leave thing creativework movie\n",
      "improve your mother daughter relationship\n",
      "\n",
      "run fast happen thing creativework musicrecording\n",
      "meet john cena\n",
      "\n",
      "shakespeares plays prepare for a shakespeare play thing creativework\n",
      "read shakespeare beginners\n",
      "\n",
      "showtime choose thing creativework movie\n",
      "search movie showtimes using spotlight search iphone\n",
      "\n",
      "so young abandon thing creativework movie\n",
      "create large scale home movies\n",
      "\n",
      "striptease obsess thing creativework movie\n",
      "perform striptease\n",
      "\n",
      "take on me forget thing creativework musicrecording\n",
      "meet john cena\n",
      "\n",
      "terms of endearment come thing creativework movie\n",
      "name main character\n",
      "\n",
      "the devil wears prada watch the devil wears prada thing creativework movie\n",
      "be christcore\n",
      "\n",
      "the wizard of oz watch full episode of robot chicken thing creativework movie\n",
      "decide what you be halloween\n",
      "\n",
      "the incredible hulk watch movie with english subtitle thing creativework movie\n",
      "add subtitles movie\n",
      "\n",
      "the paper make love with a girl thing creativework movie\n",
      "come up movie idea\n",
      "\n",
      "the pyramid reclaim thing creativework movie\n",
      "rent camel cairo\n",
      "\n",
      "the sixth sense see the spirits of people who have died thing creativework movie\n",
      "decorate bedroom like haunted house\n",
      "\n",
      "the tall man meet a good woman thing creativework movie\n",
      "shag dance\n",
      "\n",
      "today lose thing creativework tvseries\n",
      "yolo\n",
      "\n",
      "trust me make trust thing creativework movie\n",
      "deal with going different school than your boyfriend girlfriend\n",
      "\n",
      "porco rosso watch porco rosso ( 1992 ) thing creativework movie\n",
      "write source card\n",
      "\n",
      "prom night love the colors of prom thing creativework movie\n",
      "act like carrie white\n",
      "\n",
      "knowing find the name of a song  thing creativework movie\n",
      "get sync rights\n",
      "\n",
      "krrish see thing creativework movie\n",
      "explode team fortress \n",
      "\n",
      "law & order: special victims unit win emmy thing creativework tvseries\n",
      "take action against lease violations\n",
      "\n",
      "lost season 3 release on dvd thing creativework tvseries\n",
      "watch walking dead season 3\n",
      "\n",
      "mary poppins watch this video thing creativework movie\n",
      "be leading lady\n",
      "\n",
      "baby shower give thing event\n",
      "decorate baby shower\n",
      "\n",
      "chernobyl disaster happen thing event\n",
      "help children cope disaster\n",
      "\n",
      "formula one become a f1 driver thing event sportsevent\n",
      "become f1 driver\n",
      "\n",
      "graduation attend ceremony thing event educationevent\n",
      "prepare for graduation\n",
      "\n",
      "honeymoon recommend thing event\n",
      "find honeymoon venue greek islands\n",
      "\n",
      "immigration apply for citizenship thing event\n",
      "have dual citizenship us canada\n",
      "\n",
      "islamic terrorism issue indictment thing event\n",
      "prove multiplicity\n",
      "\n",
      "live aid raise dollars thing event\n",
      "fundraise your pta\n",
      "\n",
      "the exodus believe thing event\n",
      "become seventh day adventist\n",
      "\n",
      "world war ii win thing event\n",
      "learn about world war ii\n",
      "\n",
      "flood pevent thing event\n",
      "make flood shelter kids pre teens\n",
      "\n",
      "anti-gravity create an anti gravity propulsion system thing intangible\n",
      "amortize assets\n",
      "\n",
      "html embed a sound in a html page thing intangible computerlanguage\n",
      "embed bing map address\n",
      "\n",
      "php debug thing intangible computerlanguage\n",
      "amortize assets\n",
      "\n",
      "english grammar teach thing intangible\n",
      "amortize assets\n",
      "\n",
      "free education apply thing intangible\n",
      "amortize assets\n",
      "\n",
      "job satisfaction get thing intangible\n",
      "do cost analysis\n",
      "\n",
      "belief say thing intangible\n",
      "amortize assets\n",
      "\n",
      "tagalog language teach thing intangible language\n",
      "speak tagalog\n",
      "\n",
      "environmentalism support environmentalism and climate change awareness thing intangible\n",
      "escape materialism find happiness\n",
      "\n",
      "bibliography write the bibliography thing intangible\n",
      "write bibliography\n",
      "\n",
      "email box send thing intangible\n",
      "amortize assets\n",
      "\n",
      "google maps prepare thing intangible service\n",
      "calculate amortization patents\n",
      "\n",
      "property insurance receive thing intangible service financialproduct\n",
      "sell business assets\n",
      "\n",
      "voip phone use thing intangible service\n",
      "amortize assets\n",
      "\n",
      "separation of church and state allow thing intangible\n",
      "amortize assets\n",
      "\n",
      "indian institute of technology joint entrance examination pass thing intangible\n",
      "amortize assets\n",
      "\n",
      "history of television watch thing intangible\n",
      "amortize assets\n",
      "\n",
      "acupuncture work thing medicalentity medicalprocedure\n",
      "find licensed acupuncturist\n",
      "\n",
      "apgar score improve thing medicalentity medicalintangible\n",
      "read apgar score\n",
      "\n",
      "asthma relieve thing medicalentity medicalcondition\n",
      "countercondition cat\n",
      "\n",
      "back pain treat thing medicalentity medicalcondition\n",
      "countercondition cat\n",
      "\n",
      "brain develop thing medicalentity anatomicalsystem\n",
      "enable windows subsystem linux\n",
      "\n",
      "carpal tunnel syndrome suffer thing medicalentity medicalcondition\n",
      "diagnose carpal tunnel syndrome\n",
      "\n",
      "dna identify thing medicalentity anatomicalsystem\n",
      "get dna test\n",
      "\n",
      "hepatitis c diagnose thing medicalentity medicalcondition\n",
      "diagnose viral hepatitis\n",
      "\n",
      "insomnia cause thing medicalentity medicalcondition\n",
      "stop insomnia\n",
      "\n",
      "kidney donate thing medicalentity anatomicalsystem\n",
      "be kidney donor\n",
      "\n",
      "naproxen increase thing medicalentity substance drug\n",
      "choose counter pain medication\n",
      "\n",
      "pubis break thing medicalentity anatomicalsystem\n",
      "enable windows subsystem linux\n",
      "\n",
      "respiratory system evaluate thing medicalentity anatomicalsystem\n",
      "enable windows subsystem linux\n",
      "\n",
      "self-esteem boost thing medicalentity medicalintangible\n",
      "develop personal growth\n",
      "\n",
      "shock cause thing medicalentity medicalcondition\n",
      "countercondition cat\n",
      "\n",
      "skin care thing medicalentity anatomicalsystem\n",
      "enable windows subsystem linux\n",
      "\n",
      "speech disorder overcome thing medicalentity medicalcondition\n",
      "countercondition cat\n",
      "\n",
      "spinal muscular atrophy reduce thing medicalentity medicalcondition\n",
      "decide spinal decompression therapy is you\n",
      "\n",
      "testosterone increase thing medicalentity substance\n",
      "increase chest hair\n",
      "\n",
      "thyroid hormone produce thing medicalentity substance\n",
      "study medical concept hormone\n",
      "\n",
      "trichinosis stop thing medicalentity parasiticdisease\n",
      "treat droopy eyelids\n",
      "\n",
      "urinary incontinence prevent thing medicalentity medicalcondition\n",
      "improve incontinence\n",
      "\n",
      "blood type donate thing medicalentity medicalintangible\n",
      "get paid for donating plasma\n",
      "\n",
      "adidas wear thing organization corporation\n",
      "be nammer\n",
      "\n",
      "aol use thing organization corporation\n",
      "connect aol\n",
      "\n",
      "bacardi drink thing organization corporation\n",
      "make rum runner\n",
      "\n",
      "big oil sell thing organization corporation\n",
      "write nonprofit governing board statement\n",
      "\n",
      "bp own thing organization corporation\n",
      "start small business california\n",
      "\n",
      "carolina panthers play thing organization sportsorganization\n",
      "form llc south carolina\n",
      "\n",
      "chicago white sox win thing organization sportsorganization\n",
      "be chicago white sox fan\n",
      "\n",
      "espn watch thing organization broadcastchannel\n",
      "watch fifa world cup online\n",
      "\n",
      "fedex call thing organization corporation\n",
      "get job fedex\n",
      "\n",
      "fifa watch thing organization sportsorganization\n",
      "follow fifa world cup\n",
      "\n",
      "ford motor company sell thing organization corporation\n",
      "become car designer\n",
      "\n",
      "ibm join thing organization corporation\n",
      "install ibm java ubuntu linux\n",
      "\n",
      "isuzu motors build thing organization corporation\n",
      "program vex robotics clawbot\n",
      "\n",
      "liberal party of canada support thing organization governmentorganization\n",
      "find your political standpoint\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little league baseball join thing organization sportsorganization\n",
      "become baseball writer\n",
      "\n",
      "los angeles lakers win thing organization sportsorganization\n",
      "be good orlando magic fan\n",
      "\n",
      "major league baseball announce retirement thing organization sportsorganization\n",
      "become baseball writer\n",
      "\n",
      "maroon 5 perform ballad thing organization performinggroup musicgroup\n",
      "access shared folders windows 7\n",
      "\n",
      "maybelline buy makeup thing organization company\n",
      "buy beauty products budget\n",
      "\n",
      "miami heat win the championship thing organization sportsorganization\n",
      "be good orlando magic fan\n",
      "\n",
      "motorola unlock motorola c975 thing organization corporation\n",
      "remove motorola subsidy password message\n",
      "\n",
      "nvidia take complaint thing organization company\n",
      "update nvidia drivers\n",
      "\n",
      "people for the ethical treatment of animals treat animals thing organization ngo\n",
      "help stop cruelty towards animals\n",
      "\n",
      "pga tour win tournament thing organization sportsorganization\n",
      "win your ncaa basketball tournament pool\n",
      "\n",
      "singapore airlines operate route thing organization airline\n",
      "find cheap flights online\n",
      "\n",
      "tiscali get insurance thing organization corporation\n",
      "start title company\n",
      "\n",
      "visa inc. extend visa thing organization corporation\n",
      "avoid violating your b1 business visa\n",
      "\n",
      "walmart apply for a job thing organization corporation\n",
      "get job walmart\n",
      "\n",
      "university of houston apply for a job  thing organization educationalorganization collegeoruniversity\n",
      "become scholarship consultant\n",
      "\n",
      "abraham lincoln work lawyer thing person\n",
      "make abraham lincoln costume\n",
      "\n",
      "alan turing pass a turing test thing person\n",
      "plant turing sunflowers\n",
      "\n",
      "aristotle discover secrets thing person\n",
      "use whisper\n",
      "\n",
      "barbie hsu study relationship thing person\n",
      "install android htc hd\n",
      "\n",
      "batman play villain thing person\n",
      "play lego batman the videogame\n",
      "\n",
      "british royal family idolise family thing person\n",
      "formally address british royalty aristocracy person\n",
      "\n",
      "diana ross sings jazz thing person\n",
      "act like diana meade secret circle triology\n",
      "\n",
      "jason kidd provide scoring thing person\n",
      "be point guard\n",
      "\n",
      "jean-claude van damme splits  thing person\n",
      "entertain yourself\n",
      "\n",
      "jesus died on a tree thing person\n",
      "get closer with god our holy father\n",
      "\n",
      "johann sebastian bach listen  thing person\n",
      "listen bach\n",
      "\n",
      "kathy griffin watch premiere thing person\n",
      "keep with hipster\n",
      "\n",
      "luther vandross left money thing person\n",
      "model molecule using avogadro software\n",
      "\n",
      "pope benedict xvi visit pope thing person\n",
      "address pope\n",
      "\n",
      "prince charming become man thing person\n",
      "stop waiting prince charming\n",
      "\n",
      "roger federer win season thing person\n",
      "write sports article\n",
      "\n",
      "samuel beckett find hidden talent thing person\n",
      "discover your talents\n",
      "\n",
      "serena williams beat niculescu thing person\n",
      "care premature baby\n",
      "\n",
      "tink sing music thing person\n",
      "make region choir\n",
      "\n",
      "tony robbins live with passion thing person\n",
      "discover what you really want new career\n",
      "\n",
      "vin diesel become warming thing person\n",
      "become diesel mechanic\n",
      "\n",
      "acapulco hold mesino thing place\n",
      "make phone call mexico\n",
      "\n",
      "angola take crudes thing place\n",
      "form roda\n",
      "\n",
      "british columbia establish welfare thing place\n",
      "understand canadian slang\n",
      "\n",
      "chennai fire naphtha thing place\n",
      "remove stickers safely guitar\n",
      "\n",
      "cincinnati buy a beginner drum set thing place\n",
      "buy bongo drum\n",
      "\n",
      "dormitory provide meals thing place accommodation\n",
      "get around venice cheap\n",
      "\n",
      "egypt visit thing place\n",
      "time your trip cairo\n",
      "\n",
      "egyptian pyramids build thing place touristattraction\n",
      "enjoy attractions cairo\n",
      "\n",
      "finland go thing place\n",
      "write position paper model un\n",
      "\n",
      "glasgow look for a job  thing place\n",
      "become licensed taxi driver glasgow\n",
      "\n",
      "greece live in greece thing place\n",
      "get italy greece\n",
      "\n",
      "hawaii use wireless thing place\n",
      "get free cell phone\n",
      "\n",
      "himalayas go on a trek thing place\n",
      "go trekking himalayas\n",
      "\n",
      "iran discuss iran threat thing place\n",
      "visit iran\n",
      "\n",
      "kansas city locate thing place\n",
      "make science fair title\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-294-ad797db9fc45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mproperties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mentityType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm25\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_top_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#     print(tokenizer.tokenize(query), group.entityType.unique(), properties)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/rank_bm25.py\u001b[0m in \u001b[0;36mget_top_n\u001b[0;34m(self, query, documents, n)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The documents given don't match the index corpus!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mtop_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/rank_bm25.py\u001b[0m in \u001b[0;36mget_scores\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mdoc_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mq_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_freqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n\u001b[1;32m    114\u001b[0m                                                (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/rank_bm25.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mdoc_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mq_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_freqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n\u001b[1;32m    114\u001b[0m                                                (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "for name, group in df.groupby(\"query_id\"):\n",
    "    entity = group.entity.unique()[0]\n",
    "    action = group.action.unique()[0]\n",
    "    entityType = group.entityType.unique()[0]\n",
    "    properties = \" \".join(group.property.unique())\n",
    "    query = entity + \" \" + action + \" \" + entityType\n",
    "    doc = bm25.get_top_n(tokenizer.tokenize(query), corpus, n=10)\n",
    "#     print(tokenizer.tokenize(query), group.entityType.unique(), properties)\n",
    "    print(query)\n",
    "    print(doc[0].split(\"\\t\")[0])\n",
    "    print()\n",
    "#     for d in doc:\n",
    "#         print(d.split(\"\\t\")[0])\n",
    "    \n",
    "    \n",
    "#     data = getVectors(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"entity\", \"action\", \"entityType\"]].drop_duplicates().to_csv(\"tmp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in df.groupby(\"query_id\"):\n",
    "    query = group.entity.iloc[0] + \" \" + group.action.iloc[0] + \" \" + group.entityType.iloc[0] # .replace(\"thing \",\"\").replace(\"thing\",\"\")\n",
    "    query = group.entityType.iloc[0] + \" \" + group.action.iloc[0]# .replace(\"thing \",\"\").replace(\"thing\",\"\")\n",
    "    with open(\"data/cedr/type-action.tsv\", 'a') as fd:\n",
    "        fd.write(\"query\\t\"+str(name)+\"\\t\"+query +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entityType\n",
       "thing                                                               5\n",
       "thing action                                                      139\n",
       "thing creativework                                                 22\n",
       "thing creativework book                                            12\n",
       "thing creativework movie                                          379\n",
       "thing creativework musicrecording                                  42\n",
       "thing creativework tvseries                                        61\n",
       "thing event                                                       157\n",
       "thing event educationevent                                         22\n",
       "thing event sportsevent                                            19\n",
       "thing intangible                                                   59\n",
       "thing intangible computerlanguage                                  11\n",
       "thing intangible language                                           4\n",
       "thing intangible service                                           38\n",
       "thing intangible service financialproduct                          21\n",
       "thing medicalentity anatomicalsystem                               82\n",
       "thing medicalentity medicalcondition                              200\n",
       "thing medicalentity medicalintangible                              23\n",
       "thing medicalentity medicalprocedure                               23\n",
       "thing medicalentity parasiticdisease                                9\n",
       "thing medicalentity substance                                      21\n",
       "thing medicalentity substance drug                                 22\n",
       "thing organization airline                                         25\n",
       "thing organization broadcastchannel                                23\n",
       "thing organization company                                         39\n",
       "thing organization corporation                                    256\n",
       "thing organization educationalorganization collegeoruniversity     19\n",
       "thing organization governmentorganization                          23\n",
       "thing organization ngo                                             20\n",
       "thing organization performinggroup musicgroup                      11\n",
       "thing organization sportsorganization                             150\n",
       "thing person                                                      330\n",
       "thing place                                                       434\n",
       "thing place accommodation                                          22\n",
       "thing place landform bodyofwater                                   20\n",
       "thing place landform mountain                                      33\n",
       "thing place localbusiness entertainmentbusiness                    16\n",
       "thing place touristattraction                                      16\n",
       "thing product                                                     822\n",
       "thing product vehicle                                              53\n",
       "thing product vehicle car                                         113\n",
       "dtype: int64"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"entityType\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in prop2pid:\n",
    "    with open(\"data/cedr/doc.tsv\", 'a') as fd:\n",
    "        fd.write(\"doc\\t\"+str(prop2pid[key])+\"\\t\"+key+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entityType'] = [i.split()[-1]for i in df.entityType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    with open(\"data/cedr/akgg-qrel.tsv\", 'a') as fd:\n",
    "        fd.write(str(row[\"query_id\"]) + \"\\t0\\t\"+str(row[\"pid\"])+\"\\t\"+str(row[\"rele_label\"])+\"\\t\"+str(row[\"entityType\"])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5fold data generation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "foldNum = 5\n",
    "trainId5fold = {i:[] for i in range(foldNum)}\n",
    "validateId5fold = {i:[] for i in range(foldNum)}\n",
    "testId5fold = {i:[] for i in range(foldNum)}\n",
    "for name, group in df.groupby(\"entityType\"):\n",
    "    if group.query_id.nunique() >= foldNum:\n",
    "        ids = np.array(group.query_id.unique())\n",
    "        \n",
    "        kf = KFold(n_splits=foldNum)\n",
    "        kf.get_n_splits(ids)\n",
    "        for idx, ele in enumerate(kf.split(ids)):\n",
    "            train, test = ids[ele[0]], ids[ele[1]]\n",
    "            train, val, _, _ = train_test_split(train, train, test_size=0.2)\n",
    "            trainId5fold[idx].extend(train)\n",
    "            validateId5fold[idx].extend(val)\n",
    "            testId5fold[idx].extend(test)\n",
    "    else:\n",
    "        ids = list(group.query_id.unique())\n",
    "        for i in range(foldNum):\n",
    "            trainId5fold[i].extend(ids)\n",
    "\n",
    "def writeRunFile(filename, _df):\n",
    "    for name, group in _df.groupby(\"query_id\"):\n",
    "        for idx, row in enumerate(group.iterrows()):\n",
    "            with open(filename, 'a') as fd:\n",
    "                fd.write(str(row[1]['query_id']) +\"\\tQ0\\t\" + str(row[1]['pid']) + \"\\t\" + str(idx+1) + \"\\t\" + str(row[1]['rele_label']) + \"\\trun\\n\")\n",
    "                \n",
    "for fold in range(foldNum):\n",
    "    for idx, group in df[df.query_id.isin(trainId5fold[fold])].groupby(\"query_id\"):\n",
    "        pos = group['property'].tolist()\n",
    "        neg = list(set.difference(set(type2allprop[group.iloc[0]['entityType']]), set(pos)))\n",
    "\n",
    "        for idx, row in group.iterrows():\n",
    "            with open(\"data/cedr/train%d.tsv\" % fold, 'a') as fd:\n",
    "                fd.write(str(row[\"query_id\"]) + \"\\t\"+str(row[\"pid\"])+\"\\n\")\n",
    "        for n in neg:\n",
    "            with open(\"data/cedr/train%d.tsv\" % fold, 'a') as fd:\n",
    "                fd.write(str(row[\"query_id\"]) + \"\\t\"+str(prop2pid[n])+\"\\n\")\n",
    "    \n",
    "    for idx, group in df[df.query_id.isin(validateId5fold[fold])].groupby(\"query_id\"):\n",
    "        pos = group['property'].tolist()\n",
    "        neg = list(set.difference(set(type2allprop[group.iloc[0]['entityType']]), set(pos)))\n",
    "\n",
    "        lines = {}\n",
    "        for idx, row in group.iterrows():\n",
    "            lines[str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[row['property']])+\"\\t0\\t\" +str(row['rele_label'])+\"\\trun\\n\"] = row['rele_label']\n",
    "\n",
    "        for n in neg:\n",
    "            lines[str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[n])+\"\\t0\\t0\\trun\\n\"] = 0\n",
    "        lines = [i[0] for i in sorted(lines.items(), key=lambda x: x[1], reverse=True)]\n",
    "        for l in lines:\n",
    "            with open(\"data/cedr/valid%d.tsv\" % fold, 'a') as fd:\n",
    "                fd.write(l)\n",
    "                \n",
    "    for idx, group in df[df.query_id.isin(testId5fold[fold])].groupby(\"query_id\"):\n",
    "        pos = group['property'].tolist()\n",
    "        neg = list(set.difference(set(type2allprop[group.iloc[0]['entityType']]), set(pos)))\n",
    "\n",
    "        lines = {}\n",
    "        for idx, row in group.iterrows():\n",
    "            lines[str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[row['property']])+\"\\t0\\t\" +str(row['rele_label'])+\"\\trun\\n\"] = row['rele_label']\n",
    "\n",
    "        for n in neg:\n",
    "            lines[str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[n])+\"\\t0\\t0\\trun\\n\"] = 0\n",
    "        lines = [i[0] for i in sorted(lines.items(), key=lambda x: x[1], reverse=True)]\n",
    "        for l in lines:\n",
    "            with open(\"data/cedr/test%d.tsv\" % fold, 'a') as fd:\n",
    "                fd.write(l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5fold data generation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "type2allprop = df.groupby(\"entityType\")['property'].unique().to_dict()\n",
    "\n",
    "foldNum = 5\n",
    "trainId5fold = {i:[] for i in range(foldNum)}\n",
    "validateId5fold = {i:[] for i in range(foldNum)}\n",
    "testId5fold = {i:[] for i in range(foldNum)}\n",
    "\n",
    "ids = df.query_id.unique().tolist()\n",
    "kf = KFold(n_splits=foldNum)\n",
    "kf.get_n_splits(ids)\n",
    "\n",
    "# akgg-r > all proprerty from runs.\n",
    "# akgg-r2 > only property from qrel and random neg\n",
    "\n",
    "for idx, ele in enumerate(kf.split(ids)):\n",
    "    train, test = [ids[i] for i in ele[0]], [ids[i] for i in ele[1]]\n",
    "    train, val, _, _ = train_test_split(train, train, test_size=0.2)\n",
    "    trainId5fold[idx].extend(train)\n",
    "    validateId5fold[idx].extend(val)\n",
    "    testId5fold[idx].extend(test)\n",
    "# for name, group in df.groupby(\"entityType\"):\n",
    "#     if group.query_id.nunique() >= foldNum:\n",
    "#         ids = np.array(group.query_id.unique())\n",
    "        \n",
    "#         kf = KFold(n_splits=foldNum)\n",
    "#         kf.get_n_splits(ids)\n",
    "#         for idx, ele in enumerate(kf.split(ids)):\n",
    "#             train, test = ids[ele[0]], ids[ele[1]]\n",
    "#             train, val, _, _ = train_test_split(train, train, test_size=0.2)\n",
    "#             trainId5fold[idx].extend(train)\n",
    "#             validateId5fold[idx].extend(val)\n",
    "#             testId5fold[idx].extend(test)\n",
    "#     else:\n",
    "#         ids = list(group.query_id.unique())\n",
    "#         for i in range(foldNum):\n",
    "#             trainId5fold[i].extend(ids)\n",
    "            \n",
    "def writeRunFile(filename, _df):\n",
    "    for name, group in _df.groupby(\"query_id\"):\n",
    "        for idx, row in enumerate(group.iterrows()):\n",
    "            with open(filename, 'a') as fd:\n",
    "                fd.write(str(row[1]['query_id']) +\"\\tQ0\\t\" + str(row[1]['pid']) + \"\\t\" + str(idx+1) + \"\\t\" + str(row[1]['rele_label']) + \"\\trun\\n\")\n",
    "\n",
    "cand_props = df.property.unique()            \n",
    "\n",
    "for fold in range(foldNum):\n",
    "    for idx, group in df[df.query_id.isin(trainId5fold[fold])].groupby(\"query_id\"):\n",
    "        pos = group['property'].tolist()\n",
    "        neg = list(set.difference(set(type2allprop[group.iloc[0]['entityType']]), set(pos)))\n",
    "        if len(neg) == 0:\n",
    "            neg = list(set.difference(set(cand_props), set(pos)))\n",
    "#         neg = list(set.difference(set(cand_props), set(pos)))\n",
    "\n",
    "        for idx, row in group.iterrows():\n",
    "            with open(\"data/cedr/akgg-r2-train%d.tsv\" % fold, 'a') as fd:\n",
    "                fd.write(str(row[\"query_id\"]) + \"\\t\"+str(row[\"pid\"])+\"\\n\")\n",
    "        for n in neg:\n",
    "            with open(\"data/cedr/akgg-r2-train%d.tsv\" % fold, 'a') as fd:\n",
    "                fd.write(str(row[\"query_id\"]) + \"\\t\"+str(prop2pid[n])+\"\\n\")\n",
    "    \n",
    "    for idx, group in df[df.query_id.isin(validateId5fold[fold])].groupby(\"query_id\"):\n",
    "        pos = group['property'].tolist()\n",
    "        neg = list(set.difference(set(type2allprop[group.iloc[0]['entityType']]), set(pos)))\n",
    "        if len(neg) == 0:\n",
    "            neg = list(set.difference(set(cand_props), set(pos)))\n",
    "#         neg = list(set.difference(set(cand_props), set(pos)))\n",
    "\n",
    "        lines = {}\n",
    "        for idx, row in group.iterrows():\n",
    "            lines[str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[row['property']])+\"\\t0\\t\" +str(row['rele_label'])+\"\\trun\\n\"] = row['rele_label']\n",
    "\n",
    "        for n in neg:\n",
    "            lines[str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[n])+\"\\t0\\t0\\trun\\n\"] = 0\n",
    "        lines = [i[0] for i in sorted(lines.items(), key=lambda x: x[1], reverse=True)]\n",
    "        for l in lines:\n",
    "            with open(\"data/cedr/akgg-r2-valid%d.tsv\" % fold, 'a') as fd:\n",
    "                fd.write(l)\n",
    "                \n",
    "    for idx, group in df[df.query_id.isin(testId5fold[fold])].groupby(\"query_id\"):\n",
    "        pos = group['property'].tolist()\n",
    "        neg = list(set.difference(set(type2allprop[group.iloc[0]['entityType']]), set(pos)))\n",
    "        if len(neg) == 0:\n",
    "            neg = list(set.difference(set(cand_props), set(pos)))\n",
    "\n",
    "        lines = {}\n",
    "        for idx, row in group.iterrows():\n",
    "            lines[str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[row['property']])+\"\\t0\\t\" +str(row['rele_label'])+\"\\trun\\n\"] = row['rele_label']\n",
    "\n",
    "        for n in neg:\n",
    "            lines[str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[n])+\"\\t0\\t0\\trun\\n\"] = 0\n",
    "        lines = [i[0] for i in sorted(lines.items(), key=lambda x: x[1], reverse=True)]\n",
    "        for l in lines:\n",
    "            with open(\"data/cedr/akgg-r2-test%d.tsv\" % fold, 'a') as fd:\n",
    "                fd.write(l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['thing action', 'thing creativework movie',\n",
       "       'thing creativework musicrecording', 'thing creativework book',\n",
       "       'thing creativework', 'thing creativework tvseries', 'thing event',\n",
       "       'thing event sportsevent', 'thing event educationevent',\n",
       "       'thing intangible', 'thing intangible computerlanguage',\n",
       "       'thing intangible language', 'thing intangible service',\n",
       "       'thing intangible service financialproduct',\n",
       "       'thing medicalentity medicalprocedure',\n",
       "       'thing medicalentity medicalintangible',\n",
       "       'thing medicalentity medicalcondition',\n",
       "       'thing medicalentity anatomicalsystem',\n",
       "       'thing medicalentity substance drug',\n",
       "       'thing medicalentity substance',\n",
       "       'thing medicalentity parasiticdisease',\n",
       "       'thing organization corporation',\n",
       "       'thing organization sportsorganization',\n",
       "       'thing organization broadcastchannel',\n",
       "       'thing organization governmentorganization',\n",
       "       'thing organization performinggroup musicgroup',\n",
       "       'thing organization company', 'thing organization ngo',\n",
       "       'thing organization airline',\n",
       "       'thing organization educationalorganization collegeoruniversity',\n",
       "       'thing person', 'thing place', 'thing place accommodation',\n",
       "       'thing place touristattoraction',\n",
       "       'thing place localbusiness entertainmentbusiness',\n",
       "       'thing place landform bodyofwater',\n",
       "       'thing place landform mountain', 'thing product',\n",
       "       'thing product vehicle car', 'thing product vehicle', 'thing'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.entityType.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action 14\n",
      "creativework 29\n",
      "intangible 20\n",
      "medicalentity 23\n",
      "organization 29\n",
      "person 21\n",
      "place 33\n"
     ]
    }
   ],
   "source": [
    "categories = [\"action\", \"creativework\", \"intangible\", \"medicalentity\", \"organization\", \"person\", \"place\"]\n",
    "for i in categories:\n",
    "    print(i, df[df.entityType.str.contains(i)].query_id.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test\n",
    "for idx, group in df[df.query_id.isin(trainIds)].groupby(\"query_id\"):\n",
    "    pos = group['property'].tolist()\n",
    "    neg = list(set.difference(set(type2allprop[group.iloc[0]['entityType']]), set(pos)))\n",
    "\n",
    "    for idx, row in group.iterrows():\n",
    "        with open(\"data/cedr/train.tsv\", 'a') as fd:\n",
    "            fd.write(str(row[\"query_id\"]) + \"\\t\"+str(row[\"pid\"])+\"\\n\")\n",
    "    for n in neg:\n",
    "        with open(\"data/cedr/train.tsv\", 'a') as fd:\n",
    "            fd.write(str(row[\"query_id\"]) + \"\\t\"+str(prop2pid[n])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, group in df[df.query_id.isin(testIds)].groupby(\"query_id\"):\n",
    "    pos = group['property'].tolist()\n",
    "    neg = list(set.difference(set(type2allprop[group.iloc[0]['entityType']]), set(pos)))\n",
    "\n",
    "    lines = {}\n",
    "    for idx, row in group.iterrows():\n",
    "#         with open(\"data/cedr/test.run\", 'a') as fd:\n",
    "#             fd.write(str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[n])+\"\\t0\\t\" +str(row['rele_label'])+\"\\trun\\n\")\n",
    "        lines[str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[row['property']])+\"\\t0\\t\" +str(row['rele_label'])+\"\\trun\\n\"] = row['rele_label']\n",
    "            \n",
    "    for n in neg:\n",
    "#         with open(\"data/cedr/test.run\", 'a') as fd:\n",
    "        lines[str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[n])+\"\\t0\\t0\\trun\\n\"] = 0\n",
    "    lines = [i[0] for i in sorted(lines.items(), key=lambda x: x[1], reverse=True)]\n",
    "    for l in lines:\n",
    "        with open(\"data/cedr/test.tsv\", 'a') as fd:\n",
    "            fd.write(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 27)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.query_id == 71]), len(type2allprop2[df[df.query_id == 71].iloc[0].entityType])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229 20 0\n"
     ]
    }
   ],
   "source": [
    "c, c1, c2 = 0, 0, 0\n",
    "for idx, group in df.groupby(\"query_id\"):\n",
    "    pos = group.property.tolist()\n",
    "    neg1 = list(set.difference(set(type2allprop[group.iloc[0]['entityType']]), set(pos)))\n",
    "    neg2 = list(set.difference(set(type2allprop2[group.iloc[0]['entityType']]), set(pos)))\n",
    "    if len(neg1) == 0:\n",
    "        c1 += 1\n",
    "    if len(neg2) == 0:\n",
    "        c2 += 1\n",
    "    c += 1\n",
    "print(c, c1, c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get(query, doc):\n",
    "    _query_token = br.tokenize(query)\n",
    "    _doc_token = br.tokenize(doc)\n",
    "    query_token = _pad_crop([_query_token], 5)\n",
    "    doc_token = _pad_crop([_doc_token], 5)\n",
    "    query_mask = _mask([_query_token], 5)\n",
    "    doc_mask = _mask([_doc_token], 5)\n",
    "    return br.encode_bert(query_token, query_mask, doc_token, doc_mask)\n",
    "\n",
    "get(\"have benchmarking\", \"startTime\")[1][12].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def bertSim(query, doc):\n",
    "    QLEN = DLEN = 20\n",
    "    query = br.tokenize(query)\n",
    "    doc = br.tokenize(doc)\n",
    "    query_tok = _pad_crop([query], QLEN)\n",
    "    doc_tok = _pad_crop([doc], DLEN)\n",
    "    query_mask = _mask([query], QLEN)\n",
    "    doc_mask =_mask([doc], DLEN)\n",
    "    c, q, d = br.encode_bert(query_tok, query_mask, doc_tok, doc_mask)\n",
    "    return 1 - cosine(torch.mean(q[-1][0], dim=0).detach().numpy(), torch.mean(d[-1][0], dim=0).detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    print(row['query_id'])\n",
    "    bertSim(row['entity'], row['property']), row['rele_label']\n",
    "    print(1 - cosine()\n",
    ")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5])\n",
      "torch.Size([4, 5])\n",
      "torch.Size([12, 4, 5, 768])\n",
      "torch.Size([4, 5, 12, 768])\n",
      "torch.Size([4, 12, 768])\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "def getBertVec(text):\n",
    "    \n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    \n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]*4)\n",
    "    segments_tensors = torch.tensor([segments_ids]*4)\n",
    "    print(tokens_tensor.shape)\n",
    "    print(segments_tensors.shape)\n",
    "    # Predict hidden states features for each layer\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "        \n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        print(token_embeddings.shape)\n",
    "        token_embeddings = token_embeddings.permute(1,2,0,3)\n",
    "        print(token_embeddings.shape)\n",
    "        \n",
    "#       second to last\n",
    "        token_vecs = token_embeddings[:,0]\n",
    "        print(token_vecs.shape)\n",
    "        print(\"here\")\n",
    "    \n",
    "#         token_vecs = torch.squeeze(torch.sum(torch.stack(emb), dim=0), dim=0)\n",
    "\n",
    "        sentence_embedding = torch.mean(token_vecs, dim=1)\n",
    "\n",
    "        return encoded_layers\n",
    "a = getBertVec(\"test how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 768])"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5887700534759358\t0.5579916815210934\t0.5498811645870469\t0.5370915032679738\t0.2023824341397186\t0.37383273408613293\t0.5205524147829946\t0.6563343661279278\t0.41748150210732166\t0.46722664435583194\t0.5077280078477923\t0.5583809990552392\t0.5483985055151065\t0.5730014618779197\t0.57668781331345\t0.5775744590750268\t0.5401313764702022\t0.5732620320855615\t0.5561200237670827\t0.5415032679738562\t0.5312091503267974\t0.19533384738903942\t0.373239288499416\t0.5152811387331357\t0.6512255028340639\t0.40852621914075843\t0.46013109212750586\t0.4984495450441254\t0.549658459665293\t0.5485529136943489\t0.5740434585853085\t0.578324970616467\t0.5791509041302285\t0.5332822412411633\t"
     ]
    }
   ],
   "source": [
    "def getBertVec(text):\n",
    "    \n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    \n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    # Predict hidden states features for each layer\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "        \n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "        \n",
    "#       second to last\n",
    "        token_vecs = encoded_layers[0][0]\n",
    "#         token_vecs = torch.squeeze(torch.sum(torch.stack(emb), dim=0), dim=0)\n",
    "\n",
    "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "        return sentence_embedding.detach().numpy()\n",
    "def bertSim2(query, doc):\n",
    "    return 1 - cosine(getBertVec(query), getBertVec(doc))\n",
    "\n",
    "# for idx, row in df.iterrows():\n",
    "#     print(row['query_id'])\n",
    "#     print(bertSim2(row['action'] +\" \" +row['entity'], row['property']), row['rele_label'])\n",
    "\n",
    "#     for col in [\"title\", \"overview\", \"sectionLabel\", \"headline\", \"text\"]:\n",
    "for col in [3,4]:\n",
    "    res, res2, res3 = [], [], []\n",
    "    for fold in range(5):\n",
    "        _df = pd.read_csv(\"data/cedr/test%d.tsv\" %fold, sep=\"\\t\", usecols=[0], names=[\"qid\"])\n",
    "        testIds = _df.qid.unique()\n",
    "\n",
    "        for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "            if row['query_id'] not in testIds:\n",
    "                continue\n",
    "#             print(fold, row['query_id'])\n",
    "            qrels = qrel[str(row['query_id'])]\n",
    "            cand_properties = type2prop[row['entityType']]\n",
    "\n",
    "            rank = {}\n",
    "            doc = bm25.get_top_n(tokenizer.tokenize(row['action'] +\" \" +row['entity']), corpus, n=1)[0].split(\"\\t\")[col][:500]\n",
    "\n",
    "            for p in cand_properties:\n",
    "    #             rank[p] = bertSim2(row['action'] +\" \" +row['entity'], p)\n",
    "                rank[p] = bertSim2(doc, p)\n",
    "    #             print(rank[p])\n",
    "            rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "            our = evaluate(qrels, rank)\n",
    "\n",
    "            res.append(our)\n",
    "\n",
    "    keys = {\"%s@%d\" %( i,j): [] for i in [\"p\", \"r\", \"ndcg\", \"nerr\"] for j in [5, 10 ,15, 20]}\n",
    "    keys[\"rp\"] = []\n",
    "    keys[\"mrr\"] = []\n",
    "    keys[\"map\"] = []\n",
    "    for r in [res]:\n",
    "        for k in keys:\n",
    "            print(np.mean([i[k] for i in r]), end=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1307, -0.2163, -0.2279,  ...,  0.0138,  0.1159,  0.0976],\n",
      "         [ 2.2972, -0.5218, -0.3770,  ...,  0.7855,  0.5923, -4.9415],\n",
      "         [-0.7612,  0.3292,  0.2475,  ...,  0.3704,  1.8085,  0.0186]]])\n"
     ]
    }
   ],
   "source": [
    "def getBertVec(text):\n",
    "    \n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    \n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    # Predict hidden states features for each layer\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "        \n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "        print(encoded_layers[0] + encoded_layers[1])\n",
    "        \n",
    "#       sum\n",
    "#         token_vecs = torch.sum(encoded_layers, dim=0)[0]\n",
    "#         token_vecs = encoded_layers[-1][0]\n",
    "        token_vecs = torch.sum(torch.stack(emb), dim=0)\n",
    "    \n",
    "#         print(token_vecs.size())\n",
    "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "        return sentence_embedding.detach().numpy(), encoded_layers\n",
    "# _, emb = getBertVec(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.read_csv(\"data/cedr/question-qq.tsv\", names=[\"q\", \"qid\", \"text\"], sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = []\n",
    "for idx, row in _df.iterrows():\n",
    "    new_texts.append(preprocessingText(row.text, True))\n",
    "_df[\"text\"] = new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = []\n",
    "for idx, row in _df.iterrows():\n",
    "    new_texts.append(\"?\".join(row.text.split(\"?\")[:1]))\n",
    "_df[\"text\"] = new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _df.to_csv(\"data/cedr/question-qq3-nostopword.tsv\", header=False, index=False, sep=\"\\t\")\n",
    "_df.to_csv(\"data/cedr/question-qq1.tsv\", header=False, index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_crawl = [\" \".join(i) for i in df[['action', 'entity']].drop_duplicates().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "notfound = []\n",
    "bugs = {\"Lockheed_C-5_Galaxy\":\"Lockheed C-5 Galaxy\", \"IMac\":\"IMac computer\", \"Diet_Coke\": \"Diet_Coke drink\", \"Copper\": \"Copper science\", \"Prison_Break_(season_2)\": \"Prison Break (season 2)\", \"Showtime_%28film%29\":\"Showtime film\", \"The_Wizard_of_Oz_%281939_film%29\": \"The Wizard of Oz film\", \"The_Pyramid_%28film%29\":\"The Pyramid film\", \"Prom_Night_%281980_film%29\":\"Prom Night film\", \"Law_%26_Order%3A_Special_Victims_Unit\": \"Law Order Special film\", \"Mary_Poppins_%28film%29\": \"Mary Poppins film\", \"Shock_%28circulatory%29\": \"Shock circulatory\", \"DNA\": \"DNA science\", \"Skin\": \"Skin science\", \"BP\":\"BP company\", \"Jason_Kidd\": \"Jason Kidd person\", \"Jean-Claude_Van_Damme\": \"Jean Claude Van Damme\", \"Alberta\": \"Alberta place\", \"Himalayas\": \"Himalayas place\", \"Kings_Island\":\"KingsIsland\", \"Mount_Everest\": \"MountEverest\", \"Predator_%28film%29\": \"Predator_%28film%29\", \"Braveheart\": \"Braveheart film\", \"Brainstorming\": \"Brain storming\", \"Give_Me_Five!\": \"Give_Me_Five! music\"}\n",
    "for idx, row in df[['query_id', 'url', 'entityType']].drop_duplicates().iterrows():\n",
    "    try:\n",
    "        if row['url'] in bugs:\n",
    "            with open(\"data/cedr/wikipedia.tsv\", 'a') as fd:\n",
    "                fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+preprocessingText(wikipedia.summary(urllib.parse.unquote(bugs[row['url']])))+\"\\n\")\n",
    "        else:\n",
    "            with open(\"data/cedr/wikipedia.tsv\", 'a') as fd:\n",
    "                fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+preprocessingText(wikipedia.summary(urllib.parse.unquote(row['url'])))+\"\\n\")\n",
    "    except:\n",
    "        print(row[\"url\"])\n",
    "        notfound.append((row['query_id'], row['url'], row['entityType']))\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jarana/anaconda3/envs/keras/lib/python3.7/site-packages/sklearn/feature_extraction/dict_vectorizer.py:6: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## import pandas as pd\n",
    "import numpy as np\n",
    "# from tqdm.autonotebook import tqdm\n",
    "import re, nltk, random, os, json\n",
    "import pandas as pd\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "from nltk.corpus import stopwords\n",
    "from pyNTCIREVAL import Labeler\n",
    "from pyNTCIREVAL.metrics import MSnDCG, nERR, nDCG\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from rank_bm25 import BM25Okapi\n",
    "import time, os, pickle\n",
    "# keras\n",
    "from keras.layers import Input, LSTM, GlobalMaxPooling1D, GlobalAveragePooling1D, Concatenate, Embedding, Multiply, Dot, Dense, Subtract, Activation, SimpleRNN, Flatten, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectors(queries):\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'application/json',\n",
    "        'Ocp-Apim-Subscription-Key': '924c1505854b4da4a6144a1cce92937f',\n",
    "    }\n",
    "    \n",
    "    queries = [str(i).replace(\"\\'\", \"\") for i in queries]\n",
    "\n",
    "    params = urllib.parse.urlencode({})\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            conn = http.client.HTTPSConnection('api.msturing.org')\n",
    "    #         conn.request(\"POST\", \"/gen/encode?%s\" % params, '{\"queries\": [\"how to make gingerbread people (in grams)\", \"test AI\"]}', headers)\n",
    "            conn.request(\"POST\", \"/gen/encode?%s\" % params, str({\"queries\": queries}).replace(\"\\'\", \"\\\"\"), headers)\n",
    "            response = conn.getresponse()\n",
    "            data = response.read()\n",
    "            data = json.loads(data)\n",
    "            conn.close()\n",
    "            break\n",
    "        except Exception as e:\n",
    "    #         print(data)\n",
    "            time.sleep(5)\n",
    "#         print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "    \n",
    "    return {data[i]['query']:data[i]['vector'] for i in range(len(data))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [i if i[-1] != \" \" else i[:-1] for i in pd.read_csv(\"data/cedr/query.tsv\", sep=\"\\t\", names=[\"q\", \"qid\", \"text\"]).text.unique().tolist()]\n",
    "properties = pd.read_csv(\"data/cedr/doc.tsv\", sep=\"\\t\", names=[\"q\", \"qid\", \"text\"]).text.unique().tolist()\n",
    "wikis = pd.read_csv(\"data/cedr/wikipedia1.tsv\", sep=\"\\t\", names=[\"q\", \"qid\", \"text\"]).text.unique().tolist()\n",
    "questions = pd.read_csv(\"data/cedr/question-qq1.tsv\", sep=\"\\t\", names=[\"q\", \"qid\", \"text\"]).text.unique().tolist()\n",
    "data = getTermMSvec(queries + properties + wikis + questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDict(data, \"data/cedr/query-doc-wiki-question1.pkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "regex = re.compile('[^a-zA-Z0-9.,]')\n",
    "#First parameter is the replacement, second parameter is your input string\n",
    "def preprocessingText(doc,enableStopword=False):\n",
    "    doc = regex.sub(' ', doc)\n",
    "    if enableStopword:\n",
    "        doc = \" \".join([w for w in doc.split() if not w in stop_words])\n",
    "    return doc.lower()\n",
    "\n",
    "def evaluate(qrels, ranked_list):\n",
    "    res = []\n",
    "    grades = [1,2,3,4] # a grade for relevance levels 1 and 2 (Note that level 0 is excluded)\n",
    "    labeler = Labeler(qrels)\n",
    "    labeled_ranked_list = labeler.label(ranked_list)\n",
    "    rel_level_num = 5\n",
    "    xrelnum = labeler.compute_per_level_doc_num(rel_level_num)\n",
    "    result = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in [5, 10 ,15, 20]:\n",
    "        metric = MSnDCG(xrelnum, grades, cutoff=i)\n",
    "        result[\"ndcg@%d\" % i] = metric.compute(labeled_ranked_list)\n",
    "        \n",
    "        nerr = nERR(xrelnum, grades, cutoff=i)\n",
    "        result[\"nerr@%d\" % i] = nerr.compute(labeled_ranked_list)\n",
    "        \n",
    "        \n",
    "        _ranked_list = ranked_list[:i]\n",
    "        result[\"p@%d\" % i] = len(set.intersection(set(qrels.keys()), set(_ranked_list))) / len(_ranked_list)\n",
    "        result[\"r@%d\" % i] = len(set.intersection(set(qrels.keys()), set(_ranked_list))) / len(qrels)\n",
    "    result[\"rp\"] = len(set.intersection(set(qrels.keys()), set(ranked_list[:len(qrels)]))) / len(qrels)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return \" \".join([m.group(0) for m in matches]).lower()\n",
    "\n",
    "def getTermMSvec(all_properties):\n",
    "    tmp = {}\n",
    "    for i in range(0, len(all_properties), 20):\n",
    "        data = getVectors(all_properties[i:i+20])\n",
    "        for i in data:\n",
    "            tmp[i] = data[i]\n",
    "    return tmp\n",
    "\n",
    "def saveDict(d, name):\n",
    "    f = open(name,\"wb\")\n",
    "    pickle.dump(d,f)\n",
    "    f.close()\n",
    "def loadDict(name):\n",
    "    return pickle.load( open(name, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/AKG/Test Collection/AKGG/akg_standard_akgg_property_rele.csv\")\n",
    "df_action = pd.read_csv(\"data/AKG/Test Collection/AM/akg_standard_am_verb_object_rele.csv\")\n",
    "\n",
    "with open(\"data/AKG/Formal Run Topics/AKGG_Formal_Run_Topic.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    qid, query, entity, entityType, action = [], [], [], [], []\n",
    "    for p in data['queries']:\n",
    "        qid.append(p['queryId'])\n",
    "        query.append(p['query'])   \n",
    "        entity.append(p['entity'])\n",
    "        entityType.append(' '.join(p['entityTypes']))    \n",
    "        action.append(p['action'])\n",
    "topic = pd.DataFrame({\"query_id\": qid, \"query\": query, \"entity\": entity, \"entityType\": entityType, \"action\":action})\n",
    "for c in [\"query\", \"entityType\", \"action\", \"entity\"]:\n",
    "    topic[c] = topic[c].str.lower().replace(\"\\'\", \"\")\n",
    "    \n",
    "df = df.merge(topic, how=\"inner\", on=\"query_id\")\n",
    "# df['query'] = df[['action', 'entity', 'entityType']].astype(str).apply(' '.join, axis=1)\n",
    "\n",
    "with open(\"data/AKG/Formal Run Topics/AM_Formal_Run_Topic.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    qid, entity, url = [], [], []\n",
    "    for p in data['queries'][0]:\n",
    "        qid.append(p['queryId'])\n",
    "        entity.append(p['entity'].lower())\n",
    "        url.append(p['entityurl'].split(\"/\")[-1])\n",
    "topic2 = pd.DataFrame({\"url\":url, \"entity\": entity})\n",
    "df = df.merge(topic2, how=\"inner\", on=\"entity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIds, testIds = [], []\n",
    "for name, group in df.groupby(\"entityType\"):\n",
    "    if group.query_id.nunique() > 1:\n",
    "        ids = list(group.query_id.unique())\n",
    "        mid = int(group.query_id.nunique() / 2)\n",
    "        trainIds.extend(ids[:mid])\n",
    "        testIds.extend(ids[mid:])\n",
    "    else:\n",
    "        ids = list(group.query_id.unique())\n",
    "        trainIds.extend(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = pd.read_csv(\"data/wikihowSep.csv\")\n",
    "df_wiki['headline'] = df_wiki['headline'].str.replace(\"\\n\", \"\")\n",
    "df_wiki['title'] = df_wiki['title'].str.replace(\"How to \", \"\")\n",
    "for col in ['overview', 'headline', 'text', 'sectionLabel', 'title']:\n",
    "    df_wiki[col] = [preprocessingText(str(i), True) for i in df_wiki[col]]\n",
    "    \n",
    "df_wiki['title'] = [i if not i[-1].isdigit() else i[:-1] for i in df_wiki['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoVivification(dict):\n",
    "    \"\"\"Implementation of perl's autovivification feature.\"\"\"\n",
    "    def __getitem__(self, item):\n",
    "        try:\n",
    "            return dict.__getitem__(self, item)\n",
    "        except KeyError:\n",
    "            value = self[item] = type(self)()\n",
    "            return value\n",
    "with open(\"data/AKG/Participants Runs/AKGG/akgg-formalrun-cuis.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    run = AutoVivification()\n",
    "    for p in data['runs']:\n",
    "        for res in p['results']:\n",
    "            for prop in res['properties']:\n",
    "                run[p['runid']][str(res['queryid'])][str(prop['property'])] = prop['rank']\n",
    "\n",
    "qids = []\n",
    "props = []\n",
    "for qid in run['1']:\n",
    "    tmp = list(run['1'][str(qid)].keys())\n",
    "    qids.extend([int(qid)] * len(tmp))\n",
    "    props.extend(tmp)\n",
    "df_run = pd.DataFrame({\"query_id\": qids, \"property\": props})\n",
    "df_run = df_run.merge(topic, how=\"left\", on=\"query_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "qrel = collections.defaultdict(dict)\n",
    "for qid, prop, label in df[['query_id', 'property', 'rele_label']].values:\n",
    "    qrel[str(qid)][str(prop)] = int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = df[[\"query_id\", \"entityType\", \"property\"]].append(df_run[[\"query_id\", \"entityType\", \"property\"]])\n",
    "_dfp = dfp[dfp.query_id.isin(trainIds)]\n",
    "type2prop = _dfp.groupby(\"entityType\")['property'].unique().to_dict()\n",
    "prop2popularity = _dfp.groupby(\"property\").size().to_dict()\n",
    "type2prop2popularity = _dfp.groupby([\"entityType\", \"property\"]).size().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = []\n",
    "for c in [ 'entity', 'entityType', 'action']:\n",
    "    for i in df[c].unique().tolist():\n",
    "        terms.append(i.replace(\"\\'\", \"\"))\n",
    "terms.extend([camel_case_split(i) for i in df.property.unique()])\n",
    "term2MSvec = getTermMSvec(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfw = df_wiki[df_wiki.title.str.contains(\"|\".join(df.entity.unique().tolist()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bbf1b3f0c01d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mentityInWiki\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mentityInWiki\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfw' is not defined"
     ]
    }
   ],
   "source": [
    "entityInWiki = []\n",
    "for e in df.entity.unique():\n",
    "    tmp = dfw[dfw.title.str.contains(e)]\n",
    "    if len(tmp) > 0:\n",
    "        entityInWiki.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qidInWiki = []\n",
    "for e in entityInWiki:\n",
    "    qidInWiki.append(df[df.entity == e].query_id.tolist()[0])\n",
    "saveDict(qidInWiki, \"qidInWiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "doc2title = {}\n",
    "for name, group in df_wiki.groupby(\"title\"):\n",
    "    doc = []\n",
    "    for col in [\"title\", \"overview\", \"sectionLabel\", \"headline\", \"text\"]:\n",
    "        doc.append(\" \".join(group[col].unique().tolist()))\n",
    "        \n",
    "    doc = \"\\t\".join(doc)\n",
    "    corpus.append(doc)\n",
    "    doc2title[doc] = name\n",
    "    \n",
    "bm25 = BM25Okapi([tokenizer.tokenize(doc) for doc in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'180 scooter\\tdoing 180 means rotating scooter 180 degrees. this basic fundamental trick scooter. it tends one first tricks learned basic ollie jump essential learning tricks. with practice expect add trick repertoire move scooter tricks time.\\tlanding 180 landing 180 barrier performing faux 180\\tscout locations. practice safety. gain momentum. compress knees. do ollie. initiate 180. stick landing. find barrier mid shin level. practice ollieing barrier. perform ollie. land 180. understand faux 180. practice stationary. hop spin. land trick.\\tfind open area free hazards uneven surfaces sharp objects pedestrians . skate parks great places practice tricks. also large open parking lots used tricks. make sure rules skating scooters if shy stick driveway garage till feel comfortable. some might poke fun wear knee elbow pads joints worth always wear helmet. you prone injury first practice complex tricks like 180. don increase chances needing knee surgery push quick bursts reach fast speed. this help build enough momentum get air . gauge fast reach scooter aim notches speed. this means bending knees getting low possible remaining balanced. center weight torso. if feels uncomfortable first practice riding around crouched position. push ground extending jumping motion holding onto handle bars. once air pull scooter towards yourself. make sure wheels least half foot ground otherwise one wheels might catch pavement perform 180. shift top half body beginning head direction want 180. you either go left backside right frontside . rotate full 180 facing opposite direction. when turn head upper body rest body scooter follow. straighten legs reduce spinning land scooter. if stick landing first try keep trying skate parks barriers sizes get skate park use small cardboard box. avoid using anything sharp edges could cause serious injury. use soft materials cushions cardboard boxes etc. practicing over. make sure clear barrier ollie. if probably need move smaller barrier practice ollie. this means bending knees getting low possible remaining balanced. center weight torso. push ground jumping motion holding onto handle bars. once air pull scooter towards body. make sure wheels least half foot ground otherwise one wheels might catch barrier perform 180.make sure center gravity directly torso try keep eyes focused right barrier. shift top half body beginning head direction want 180. you either go left backside right frontside . rotate full 180 facing opposite direction. when turn head upper body rest body scooter follow. begin right ollie. you probably need practice times stick landing. have confidence learn mistakes. if notice wheel clipping barrier time need adjust initiate ollie. if little scared landing traditional 180 safer version. during faux 180 feet leave scooter momentarily scooter performs 180. this inspired old school skateboarding tricks incorporate one foot pavement. you want get comfortable whipping scooter 180. while scooter spin handle bars around scooter 180. once get feel movement required hands try incorporating body. once scooter completes 180 hop feet right scooter hits surface. once practiced garage times time bring streets. you need go fast pull one off. just gain casual speed. try hop one foot spinning handlebar 180 motion. the best look one foot ground one foot air scooter operating 180 spin. for starting hop feet. once scooter completes 180 try hop back onto scooter still air. this way scooter land 180 time.'"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load : qid2MSvec.pkl\n",
    "import pickle\n",
    "\n",
    "qid2MSvec = pickle.load( open( \"qid2MSvec.pkl\", \"rb\" ) )\n",
    "# qid2MSvec = {}\n",
    "# for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "#     query = row['action'] + \" \" + row['entity']\n",
    "#     if row['query_id'] in qid2MSvec:\n",
    "#         continue\n",
    "#     if row['entity'] not in entityInWiki:\n",
    "#         data = getVectors([query])\n",
    "#         qid2MSvec[row['query_id']] = list(data.values())\n",
    "#         continue        \n",
    "# #     titles = [str(i).replace(\"\\'\", \"\") for i in dfw[dfw.title.str.contains(row['entity'])].text.unique().tolist()]\n",
    "#     titles = dfw[dfw.title.str.contains(row['entity'])].title.unique().tolist()\n",
    "#     data = getTermMSvec([query] + titles)\n",
    "#     rank = {}\n",
    "#     for i in data:\n",
    "#         if i == query:\n",
    "#             continue\n",
    "#         rank[i] = cosine_similarity([data[query]], [data[i]])[0][0]\n",
    "#     best_title = sorted(rank.items(), key=lambda x: x[1])[-1][0]\n",
    "#     sentences = []\n",
    "#     tokens = \" \".join(dfw[dfw.title == best_title].text.tolist()).split()\n",
    "#     for i in range(0, len(tokens), 10):\n",
    "#         sentences.append(\" \".join(tokens[i:i+10]))\n",
    "#     data = getTermMSvec([query] + sentences)\n",
    "#     qid2MSvec[row['query_id']] = list(data.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get questions\n",
    "import glob\n",
    "from run_scraper import crawl\n",
    "# from tqdm.autonotebook import tqdm\n",
    "\n",
    "# finish = []\n",
    "# for file in glob.glob(\"akg/*\"):\n",
    "#     finish.append(file.split(\"/\")[1].replace(\"_\", \" \").replace(\".csv\", \"\"))\n",
    "bestTitle2qid = loadDict(\"bestTitle2qid.pkl\")\n",
    "# bestTitle2qid = {}\n",
    "# for idx, row in tqdm(df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows(), total=229):\n",
    "#     query = row['action'] + \" \" + row['entity']\n",
    "#     if row['entity'] not in entityInWiki:\n",
    "#         continue        \n",
    "#     titles = dfw[dfw.title.str.contains(row['entity'])].title.unique().tolist()\n",
    "#     data = getTermMSvec([query] + titles)\n",
    "#     rank = {}\n",
    "#     for i in data:\n",
    "#         if i == query:\n",
    "#             continue\n",
    "#         rank[i] = cosine_similarity([data[query]], [data[i]])[0][0]\n",
    "#     best_title = sorted(rank.items(), key=lambda x: x[1])[-1][0]\n",
    "#     bestTitle2qid[best_title] = row['query_id']\n",
    "#     try:\n",
    "#         crawl(best_title, \"akg/\", \"no_question\")\n",
    "#     except:\n",
    "#         print(best_title)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what mean benchmarking what benchmarking used what four types benchmarking',\n",
       " 'which supplements i take bodybuilding does creatine cause hair loss when i take bodybuilding supplements',\n",
       " 'what 4 rules brainstorming what another word brainstorming how brainstorm',\n",
       " 'what considered chain smoking is ok smoke 2 cigarettes day how many cigarettes chain smoker smoke day',\n",
       " 'how get funding charity how i apply charitable grant what source funding',\n",
       " 'can i dye hair brown bleaching does dying hair brown damage how dye blonde hair back brown',\n",
       " 'stop scene accident hit run',\n",
       " 'why difficult learn foreign language is english hard foreigners learn what best age learning foreign language',\n",
       " 'how i successful mediation what success rate mediation what 5 steps mediation',\n",
       " 'how aerobic exercise affect muscles is running aerobic anaerobic what decreases aerobic exercise',\n",
       " 'what spoofing attack what mean spoofing how spoofing attack occur',\n",
       " 'how spray paint fabric can put fabric paint spray bottle what spray paint best fabric',\n",
       " 'what washoku what oil japanese cook what traditional japanese foods',\n",
       " 'why mother important life what makes mother mother why mother important life',\n",
       " 'remember back future',\n",
       " 'who wife braveheart did robert bruce really betray wallace what movie song love princess',\n",
       " 'give fives give five',\n",
       " 'what rules crossing road what rules followed crossing road what purpose rules road',\n",
       " 'why parents abandon child how many fathers leave families can i leave 9 year old home alone',\n",
       " 'what happens run fast does running burn fat do long runs make faster',\n",
       " 'why shakespeare use blank verse what example blank verse should i use accent audition',\n",
       " 'can get showtime is showtime free amazon prime what channels showtime',\n",
       " 'why birds abandon young do birds abandon eggs touched can legally abandon child',\n",
       " 'obsess striptease',\n",
       " 'forget take',\n",
       " 'come terms endearment',\n",
       " 'watch devil wears prada devil wears prada',\n",
       " 'watch full episode robot chicken wizard oz',\n",
       " 'watch movie english subtitle incredible hulk',\n",
       " 'make love girl paper',\n",
       " 'are pyramid schemes illegal why pyramid scheme illegal is arbonne pyramid scheme',\n",
       " 'what movie says i see dead people what 6th sense called what story the sixth sense',\n",
       " 'meet good woman tall man',\n",
       " 'is healthy eat one meal day does fasting slow metabolism can lose weight eating day',\n",
       " 'make trust trust',\n",
       " 'watch porco rosso 1992 porco rosso',\n",
       " 'what best color wear prom how i find right prom dress body type what common prom dress color',\n",
       " 'how i find song name can google identify songs how i identify song',\n",
       " 'who hero krrish 3 what story krrish 3 where krrish shot india',\n",
       " 'win emmy law order special victims unit',\n",
       " 'what happens lost series is lost series netflix how many hours lost',\n",
       " 'watch video mary poppins',\n",
       " 'who plans baby shower do open gifts baby shower what happens baby shower',\n",
       " 'is chernobyl reactor 4 still burning how long survive chernobyl how bad chernobyl disaster',\n",
       " 'how become f1 driver do f3 drivers get paid can watch formula 1 amazon prime',\n",
       " 'do attend graduation ceremony do get keep cap gown does open university graduation ceremony',\n",
       " 'which country best honeymoon what cheapest honeymoon destination what top 10 honeymoon destinations',\n",
       " 'how i apply british citizenship what documents i need apply citizenship how long indefinite leave remain i apply citizenship',\n",
       " 'issue indictment islamic terrorism',\n",
       " 'how much money live aid raise us dollars did michael jackson live aid how much live aid raise uk',\n",
       " 'believe exodus',\n",
       " 'which country world war two how many people died wwii did allies win world war 2',\n",
       " 'pevent flood',\n",
       " 'can make anti gravity does zero gravity exist how anti gravity work',\n",
       " 'embed sound html page html',\n",
       " 'how i debug php what php functions how i debug using xdebug',\n",
       " 'how i start teaching english grammar beginners how many words basic english how i improve english grammar vocabulary',\n",
       " 'what free school mean are public schools free is nursery free uk',\n",
       " 'what satisfied job how i happier work what happiest jobs',\n",
       " 'what concept belief what attitudes what belief example',\n",
       " 'teach tagalog language',\n",
       " 'what example environmentalism what environmentalists goals what difference conservation environmentalism',\n",
       " 'how i write bibliography how write bibliography school project what bibliography example',\n",
       " 'can email files box how send email attachment how i create send email',\n",
       " 'how i create google map pins what difference google maps google my maps how i use google maps',\n",
       " 'what covered building insurance can claim roof repairs insurance is best get buildings contents insurance together',\n",
       " 'can i use regular phone voip can use normal phone voip how voip phone work',\n",
       " 'what constitution say separation church state how many times god mentioned constitution where separation church state come',\n",
       " 'how i get admission iit entrance exam can cheat jee mains what entrance exams iit',\n",
       " 'watch history television',\n",
       " 'does acupuncture really work does acupuncture reduce inflammation what acupuncture used',\n",
       " 'what apgar scores predict what apgar score birth what 5 apgar scores',\n",
       " 'what helps asthma naturally what drink good asthma what best treatment asthma',\n",
       " 'what best treatment lower back pain how i stop lower back hurting how i get rid back pain',\n",
       " 'how brain develop what part brain controls memory what stage brain develop',\n",
       " 'what happen carpal tunnel syndrome treated is carpal tunnel disability what causes carpal tunnel flare',\n",
       " 'how dna identify person is dna admissible court what considered dna',\n",
       " 'what blood test shows hep c who tested hcv can hep c found routine blood tests',\n",
       " 'what main cause insomnia is insomnia mental illness how i stop insomnia',\n",
       " 'what side effects donating kidney is donating kidney painful how much cost donate kidney',\n",
       " 'is naproxen strong painkiller can i drink alcohol naproxen is taking naproxen dangerous',\n",
       " 'can break pubis is bone groin can walk around broken pelvis',\n",
       " 'how respiratory system measured what 5 main functions respiratory system what included respiratory assessment',\n",
       " 'boost self esteem',\n",
       " 'what main causes shock what common signs shock what mean go shock',\n",
       " 'how i take care skin what food causes pimples what good skin care routine',\n",
       " 'what neurological disorders cause speech problems what three basic types speech impairments what causes speech disorders',\n",
       " 'is spinal muscular atrophy curable what two types atrophy what life expectancy someone spinal muscular atrophy',\n",
       " 'which foods increase testosterone can testosterone increase size which foods kill testosterone',\n",
       " 'what hormone thyroid gland produce what main cause thyroid problems what needed thyroid hormone production',\n",
       " 'can get rid trichinosis how tell trichinosis does freezing pork kill trichinosis',\n",
       " 'prevent urinary incontinence',\n",
       " 'which blood type high demand who cannot donate blood is o a universal donor',\n",
       " 'wear adidas',\n",
       " 'does anybody still use aol is aol still good email provider what aol com used',\n",
       " 'drink bacardi',\n",
       " 'sell big oil',\n",
       " 'who bp owned where bp headquarters is bp owned british',\n",
       " 'what carolina panther owner who owns bank america stadium who bought carolina panthers',\n",
       " 'win chicago white sox',\n",
       " 'can stream espn free can watch espn amazon prime can watch espn espn app',\n",
       " 'how i call fedex how i speak human fedex can i call fedex package',\n",
       " 'watch fifa',\n",
       " 'who owns ford motor company is mazda owned ford what companies owned ford',\n",
       " 'what ibm partner what ibm cloud called how i get ibm id',\n",
       " 'build isuzu motors',\n",
       " 'what liberal party stand canada what conservatives stand what liberal party slogan',\n",
       " 'how much cost play little league baseball who lowest paid major league baseball player how start little league program',\n",
       " 'how many wins lakers how many times kobe miss playoffs why la lakers called lakers',\n",
       " 'announce retirement major league baseball',\n",
       " 'perform ballad maroon 5',\n",
       " 'where maybelline located is maybelline lipstick made china how i contact maybelline uk',\n",
       " 'has miami heat championship who nba rings when miami heat win championship',\n",
       " 'unlock motorola c975 motorola',\n",
       " 'where headquarters nvidia does asus nvidia what nvidia computer',\n",
       " 'what peta stop animal cruelty do animals rights does peta approve pets',\n",
       " 'who pga tournaments is tiger woods golf hall fame how many wins tiger woods pga tour',\n",
       " 'operate route singapore airlines',\n",
       " 'get insurance tiscali',\n",
       " 'extend visa visa inc.',\n",
       " 'how i apply job walmart how long take get hired walmart can i apply job walmart person',\n",
       " 'apply job university houston',\n",
       " 'work lawyer abraham lincoln',\n",
       " 'has robot passed turing test how many computers passed turing test what mean pass turing test',\n",
       " 'discover secrets aristotle',\n",
       " 'study relationship barbie hsu',\n",
       " 'who villain batman who first super villain who gonna play new batman',\n",
       " 'why england royal family does royal family get paid what hrh royal family',\n",
       " 'sings jazz diana ross',\n",
       " 'what position jason kidd did jason kidd get fired is jason kidd hall famer',\n",
       " 'how much jean claude van damme worth what tom cruise worth',\n",
       " 'is jesus tree life what tree life bible version how many trees mentioned bible',\n",
       " 'what bach famous works what bach favorite instrument why bach famous',\n",
       " 'watch premiere kathy griffin',\n",
       " 'left money luther vandross',\n",
       " 'why pope benedict xvi resign what pope benedict when pope visit glasgow',\n",
       " 'become man prince charming',\n",
       " 'what grand slams federer win who greatest tennis player time what years federer win wimbledon',\n",
       " 'find hidden talent samuel beckett',\n",
       " 'beat niculescu serena williams',\n",
       " 'what tinks net worth what tink mean twitter who tink father',\n",
       " 'live passion tony robbins',\n",
       " 'what happened vin diesel stunt double how much stunt doubles make why vin diesel name daughter pauline',\n",
       " 'hold mesino acapulco',\n",
       " 'does angola oil refineries is angola rich poor is angola member opec',\n",
       " 'how much person get welfare bc how much money get welfare canada how much single mother get welfare bc',\n",
       " 'fire naphtha chennai',\n",
       " 'how much beginner drum set are drums hard learn what best drum set beginner',\n",
       " 'what i cook dorm room can toaster dorm what eat college dorms',\n",
       " 'is safe visit egypt 2019 do i need visa egypt is egypt safe travel',\n",
       " 'how egyptians build pyramids what pyramids made who actually built pyramids',\n",
       " 'go finland',\n",
       " 'look job glasgow',\n",
       " 'is safe live greece can i buy property greece how much money need retire greece',\n",
       " 'is hawaii covered verizon wireless does cost extra call hawaii do cell phones work hawaii',\n",
       " 'how i get himalayas how high himalayas when hike himalayas',\n",
       " 'what countries allies iran are iraq iran allies does iran nuclear weapons',\n",
       " 'where exactly kansas city why 2 kansas cities what kansas city situated',\n",
       " 'get bar kelowna kelowna',\n",
       " 'how much kings island tickets kroger how i save money kings island how much cost get kings island',\n",
       " 'reach lake lake ozarks',\n",
       " 'how i move melbourne is melbourne good place live can emigrate australia 50',\n",
       " 'is kenya better south africa which intelligent country africa what nicest city africa',\n",
       " 'how much cost climb mount everest how much sherpas make how long take climb mount everest',\n",
       " 'what customs north korea what main religion north korea are allowed visit north korea',\n",
       " 'stay carlton hotel prague prague',\n",
       " 'is punta cana safe how i avoid getting sick punta cana does booking com all inclusive include flights',\n",
       " 'where rocky mountains located where rocky mountains start how far rocky mountains reach',\n",
       " 'which terminal international terminal sfo is terminal 1 domestic international how i get san francisco airport',\n",
       " 'can foreigners drive saudi arabia what country youngest driving age what side road drive saudi arabia',\n",
       " 'is south india developed which richest city south india is north india developed south india',\n",
       " 'get greencard sudan',\n",
       " 'what beautiful city iran can tourists drink alcohol iran what beautiful city iran',\n",
       " 'who performing times square new year eve why kiss midnight where i watch ball drop times square',\n",
       " 'does nigerian need visa trinidad tobago how much nigerian visa how many hours nigeria trinidad tobago',\n",
       " 'take deliveries west indies',\n",
       " 'how lager beer made is stella artois lager how long take make lager',\n",
       " 'are pretzels healthy snack are pretzels healthier chips are pretzels religious',\n",
       " 'cook beetroot',\n",
       " 'what boric acid made is boric acid good skin is boric acid safe humans',\n",
       " 'make pancake buttermilk',\n",
       " 'can play chess online can king take queen what best online chess game',\n",
       " 'fix chrysler neon',\n",
       " 'will clenbuterol burn belly fat do steroids make lose weight is clen illegal',\n",
       " 'how remove oxidation copper does brasso clean copper how clean copper bottom pans',\n",
       " 'can buy raw milk does raw milk taste different can buy raw milk uk',\n",
       " 'how bad diet coke does coke zero make gain weight what happens drink diet coke',\n",
       " 'can i drink diet soda i diabetes can diabetics drink coke zero what drinks ok diabetics',\n",
       " 'switch ethernet hub',\n",
       " 'buy ford aerostar',\n",
       " 'how french horn played who best french horn player is french horn difficult play',\n",
       " 'drive manual transmission gear stick',\n",
       " 'is demand goat meat why goat meat expensive how long keep raw mutton fridge',\n",
       " 'what best guitar amp home use do tube amps sound better what best small guitar amplifier',\n",
       " 'is good use hair dryer is air drying hair bad why use hair dryer',\n",
       " 'where i sell used handbags can finance louis vuitton where i buy authentic designer bags online',\n",
       " 'how i connect desktop hard drive laptop can use desktop external hard drive laptop how connect hard drive computer',\n",
       " 'hear baby heartbeat heart rate monitor',\n",
       " 'download hollywood video',\n",
       " 'what makes car hybrid how long batteries last hybrid car what full hybrid vehicle',\n",
       " 'how i get mac recognize printer how i enable airprint how i connect pc mac',\n",
       " 'rent jet ski jet ski',\n",
       " 'what best detergent washing clothes how i get laundry smell really good can put detergent directly washer',\n",
       " 'are lincoln town cars good cars what year stop making lincoln town car how many miles lincoln town cars last',\n",
       " 'how i upgrade mac latest version how i update old macbook can upgrade ssd macbook pro',\n",
       " 'why marmite banned can eat much marmite is marmite sold us',\n",
       " 'make mentos react coke mentos',\n",
       " 'know metal gear solid',\n",
       " 'make dessert nutella',\n",
       " 'how much gold pocket watch worth what 17 jewels watches how i tell pocket watch gold',\n",
       " 'make faster porsche 911',\n",
       " 'do install replacement windows inside outside how i install operating system what average cost replace windows home',\n",
       " 'cut stained glass',\n",
       " 'can recharge car battery will car battery recharge let sit how long take recharge car battery',\n",
       " 'bake banana bread',\n",
       " 'serve barbecue sauce',\n",
       " 'buy dvd camcorder camcorder',\n",
       " 'how much cost replace heating system when i replace heating system do i need replace radiators new boiler',\n",
       " 'what popular chardonnay what nice chardonnay what characteristics chardonnay',\n",
       " 'remove mascara eye shadow',\n",
       " 'can put fake eyelashes eyelash extensions are eyelash extensions worth money do fake lashes ruin real lashes',\n",
       " 'what difference acrylic lacquer paint does lacquer paint need clear coat what acrylic lacquer paint',\n",
       " 'make lockheed c 5 galaxy',\n",
       " 'hold tea mason jar',\n",
       " 'why everything freezing refrigerator why fridge cold freezer how i stop fridge freezing',\n",
       " 'look recipe submarine sandwich',\n",
       " 'remove door panel volkswagen passat',\n",
       " 'can i drink apple cider vinegar everyday how drink apple cider what benefits drinking apple cider',\n",
       " 'how keep labrador dog how often wash dog are labradors high maintenance']"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# qid2questions2MSvec = {}\n",
    "# for file in glob.glob(\"akg/*\"):\n",
    "#     title = file.split(\"/\")[1].replace(\"_\", \" \").replace(\".csv\", \"\")\n",
    "#     lines = open(file).read().splitlines()\n",
    "#     questions = []\n",
    "#     for i in lines[1:]:\n",
    "#         questions.extend(i.split(\";\"))\n",
    "#     title2questions[title] = list(set(questions))\n",
    "#     data = getVectors([title] + list(set(questions)) )\n",
    "#     qid2questions2MSvec[bestTitle2qid[title]] = list(data.values())\n",
    "qid2questions2MSvec = loadDict(\"qid2questions2MSvec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qid2query2MSvec = {}\n",
    "# queries = []\n",
    "# for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "#     query = preprocessingText(row['action'] + \" \" + row['entity'] + \" \" + row['entityType'])\n",
    "#     qid2query2MSvec[row['query_id']] = getVectors([query])[query]\n",
    "qid2query2MSvec = pickle.load( open( \"qid2query2MSvec.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# properties = []\n",
    "# for i in run[\"1\"]:\n",
    "#     properties.extend(run[\"1\"][i].keys())\n",
    "# properties = list(set(properties + df.property.unique().tolist()))\n",
    "# prop2MSvec = getTermMSvec(properties)\n",
    "prop2MSvec = pickle.load( open( \"prop2MSvec.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLF with Question MS vectors, Two DSSM\n",
    "MAX_QUESTION_LENGHT = np.max([len(qid2questions2MSvec[i]) for i in qid2questions2MSvec])\n",
    "MAX_SEQUENCE_LENGHT = int(np.max([len(qid2MSvec[i]) for i in qid2MSvec])) + 1\n",
    "# def runDocEval(qid2MSvec, prop2MSvec):\n",
    "class CLF():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.queryInput = Input(shape=(MAX_SEQUENCE_LENGHT, 100,))\n",
    "        self.questionInput = Input(shape=(MAX_QUESTION_LENGHT, 100,))\n",
    "        \n",
    "        self.propPosInput = Input(shape=(100,))\n",
    "        lstm = LSTM(100)\n",
    "\n",
    "        queryEmbeddingLayer = Dense(100, name=\"uEmb\")\n",
    "        questionEmbeddingLayer = Dense(100, name=\"qEmb\")\n",
    "        propEmbeddingLayer = Dense(100, name=\"iEmb\")\n",
    "\n",
    "#         qEmb = queryEmbeddingLayer(lstm(self.queryInput))\n",
    "        qEmb = queryEmbeddingLayer(self.queryInput)\n",
    "        qtEmb = questionEmbeddingLayer(self.questionInput)\n",
    "        pEmb = propEmbeddingLayer(self.propPosInput)\n",
    "        \n",
    "        qEmb = Concatenate(axis=1)([qEmb, qtEmb])\n",
    "\n",
    "        dense = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "        pred = Multiply()([qEmb, pEmb])\n",
    "        \n",
    "        self.pred = dense(GlobalMaxPooling1D()(pred))\n",
    "#         self.pred = dense(pred)\n",
    "\n",
    "        self.model = Model(inputs=[self.queryInput, self.questionInput, self.propPosInput], outputs=self.pred)\n",
    "        self.model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "    def generate_train_data(self, df):\n",
    "        x_query, x_question, x_pos_prop, y = [], [], [], []\n",
    "        for name, group in df.groupby(\"query_id\"):\n",
    "            cand_pos_prop = group.property.tolist()\n",
    "            rele2prop = group[['rele_label', 'property']].groupby(\"rele_label\")[\"property\"].apply(list).to_dict()\n",
    "            for idx, row in group.iterrows():\n",
    "                if name in qid2questions2MSvec:\n",
    "                    x_query.append(qid2MSvec[name] + [qid2query2MSvec[name]])\n",
    "                    x_question.append(qid2questions2MSvec[name])\n",
    "                else:\n",
    "                    x_query.append(qid2MSvec[name] + [qid2query2MSvec[name]])\n",
    "                    x_question.append([])\n",
    "#                 x_query.append([qid2query2MSvec[name]])\n",
    "                x_pos_prop.append(prop2MSvec[row['property']])\n",
    "                y.append(1)\n",
    "        \n",
    "            cand_neg_prop = list(set.difference(set(type2prop[row['entityType']]), set(cand_pos_prop)))\n",
    "#             cand_neg_prop = list(set.difference(set(df.property.unique()), set(cand_pos_prop)))\n",
    "            for neg_prop in cand_neg_prop:\n",
    "                if name in qid2questions2MSvec:\n",
    "                    x_query.append(qid2MSvec[name] + [qid2query2MSvec[name]])\n",
    "                    x_question.append(qid2questions2MSvec[name])\n",
    "                    \n",
    "                else:\n",
    "                    x_query.append(qid2MSvec[name] + [qid2query2MSvec[name]])\n",
    "                    x_question.append([])\n",
    "#                 x_query.append([qid2query2MSvec[name]])\n",
    "                x_pos_prop.append(prop2MSvec[neg_prop])\n",
    "                y.append(0)\n",
    "\n",
    "        x_query = pad_sequences(x_query,dtype='float32', maxlen=MAX_SEQUENCE_LENGHT)\n",
    "        x_question = pad_sequences(x_question,dtype='float32', maxlen=MAX_QUESTION_LENGHT)\n",
    "        x_pos_prop = np.array(x_pos_prop)\n",
    "        return [x_query, x_question, x_pos_prop], np.array(y)\n",
    "\n",
    "df_train = df[df.query_id.isin(trainIds)]\n",
    "bpr = CLF()\n",
    "x_train, y_train = bpr.generate_train_data(df_train)\n",
    "bestNDCG = 0\n",
    "bestRes = {}\n",
    "for i in range(10):\n",
    "    test_num = 0\n",
    "    history = bpr.model.fit(x_train, y_train, verbose=0)\n",
    "    res = {\"%s@%d\" %( i,j): [] for i in [\"p\", \"r\", \"ndcg\"] for j in [5, 10 ,15]}\n",
    "    for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "        if row['query_id'] not in testIds:\n",
    "            continue\n",
    "        if row['entity'] not in entityInWiki:\n",
    "                continue\n",
    "        test_num += 1\n",
    "        qrels = qrel[str(row['query_id'])]\n",
    "        cand_properties = type2prop[row['entityType']]\n",
    "#         cand_properties = list(prop2MSvec.keys())\n",
    "#         cand_properties = list(set(type2prop[row['entityType']].tolist() + list(qrels.keys())))\n",
    "#         cand_properties = list(set(type2prop[row['entityType']].tolist() + list(qrels.keys())))\n",
    "\n",
    "        rank = {}\n",
    "        for p in cand_properties:\n",
    "            if row['query_id'] in qid2questions2MSvec:\n",
    "                \n",
    "                \n",
    "                score = bpr.model.predict([pad_sequences([qid2MSvec[row['query_id']] + [qid2query2MSvec[row['query_id']]]], dtype='float32', maxlen=MAX_SEQUENCE_LENGHT), pad_sequences(qid2questions2MSvec[row['query_id']], maxlen=MAX_QUESTION_LENGHT), np.array([prop2MSvec[p]])])[0][0]\n",
    "            else:    \n",
    "                score = bpr.model.predict([pad_sequences([qid2MSvec[row['query_id']] + [qid2query2MSvec[row['query_id']]]], dtype='float32', maxlen=MAX_SEQUENCE_LENGHT), pad_sequences([[]], maxlen=MAX_QUESTION_LENGHT), np.array([prop2MSvec[p]])])[0][0]\n",
    "#             score = bpr.model.predict([pad_sequences([[qid2query2MSvec[row['query_id']]]], dtype='float32', maxlen=MAX_SEQUENCE_LENGHT), np.array([prop2MSvec[p]])])[0][0]\n",
    "            rank[p] = score\n",
    "#         rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "        rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "        our = evaluate(qrels, rank)\n",
    "        for key in res:\n",
    "            res[key].append(our[key])\n",
    "            \n",
    "    ndcg = np.mean(res[\"ndcg@15\"])\n",
    "    if ndcg > bestNDCG:\n",
    "        bestRes = res\n",
    "        bestNDCG = ndcg\n",
    "\n",
    "    for key in res:\n",
    "        print(np.mean(res[key]), end=\"\\t\")\n",
    "    print()\n",
    "# runDocEval(qid2MSvec, prop2MSvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03433255, 0.44975244, 0.53311028, 0.35410887, 0.99748882,\n",
       "       0.73896041, 0.05642281, 0.07724881, 0.30524471, 0.29512638,\n",
       "       0.97062959, 0.718337  , 0.902602  , 0.47875612, 0.88266367,\n",
       "       0.81159707, 0.32974246, 0.95067312, 0.00537919, 0.64954441,\n",
       "       0.30603224, 0.72527115, 0.53430609, 0.13414405, 0.53767863,\n",
       "       0.47238097, 0.73724248, 0.46319916, 0.15872898, 0.26946032,\n",
       "       0.00593328, 0.17845321, 0.98740532, 0.91324818, 0.03026555,\n",
       "       0.25765835, 0.04933299, 0.34362212, 0.35729627, 0.1945712 ,\n",
       "       0.66704162, 0.09759326, 0.79909578, 0.38164191, 0.02747136,\n",
       "       0.96399566, 0.645902  , 0.84524526, 0.65987698, 0.44199329,\n",
       "       0.96531829, 0.65426018, 0.32554636, 0.25989493, 0.14494618,\n",
       "       0.43417079, 0.30338033, 0.97656624, 0.70829141, 0.10266963,\n",
       "       0.93817736, 0.83154394, 0.07704345, 0.03940803, 0.31647717,\n",
       "       0.60313842, 0.84947373, 0.35856523, 0.43557343, 0.50307816,\n",
       "       0.73536156, 0.36752177, 0.59516322, 0.7562519 , 0.04427243,\n",
       "       0.34382657, 0.20245657, 0.55511451, 0.21935717, 0.43709961,\n",
       "       0.86162883, 0.60330015, 0.17083659, 0.79592569, 0.82390909,\n",
       "       0.73975104, 0.68529653, 0.70154199, 0.76159782, 0.75789757,\n",
       "       0.15610835, 0.55366869, 0.45372986, 0.63702959, 0.40363732,\n",
       "       0.13976583, 0.53266404, 0.34769354, 0.70660171, 0.96750107])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.671698113207547\t0.6366876310272537\t0.6203354297693919\t0.22388223817060196\t0.4133358055643235\t0.57834138494253\t0.47090433295647216\t0.5183526025857245\t0.5728543191502038\t\n",
      "0.709433962264151\t0.6649895178197064\t0.6341719077568133\t0.2331357750679849\t0.4283894973210938\t0.5894220754484205\t0.49540236929097214\t0.5444787522211142\t0.5870978820484891\t\n",
      "0.7245283018867924\t0.661215932914046\t0.629140461215933\t0.23857754092069766\t0.4283239182298706\t0.5849860094893391\t0.5176873486004275\t0.555101694843192\t0.592600191636385\t\n",
      "0.7169811320754715\t0.6819706498951781\t0.6429769392033543\t0.2317280128695807\t0.43903134497503327\t0.5950146069149806\t0.5024859418527098\t0.5527983263296364\t0.5912588846363841\t\n",
      "0.6830188679245284\t0.6555555555555554\t0.6266247379454928\t0.2258671590431623\t0.42203337826636494\t0.581666006122312\t0.4961853233090351\t0.5453267137717537\t0.5871258145110986\t\n",
      "0.7358490566037734\t0.6649895178197064\t0.6253668763102724\t0.24180761878203316\t0.42761346905134506\t0.58336777066551\t0.5189738054601633\t0.5522064639068422\t0.5842390124752599\t\n",
      "0.6830188679245283\t0.6574423480083856\t0.6316561844863733\t0.22878449882509694\t0.4270995751883946\t0.5899444169975452\t0.4897735021611441\t0.5442825310442067\t0.5875158148827939\t\n",
      "0.6490566037735849\t0.6366876310272536\t0.6039832285115303\t0.21538562942208014\t0.4157125194482809\t0.5657457719698504\t0.46981105084534863\t0.5251316533518977\t0.5646346406969522\t\n",
      "0.6754716981132075\t0.6498951781970651\t0.619077568134172\t0.22168901291995513\t0.4232088121230301\t0.5776878128167632\t0.490499644547348\t0.5389465574224899\t0.5735704301343865\t\n",
      "0.7056603773584904\t0.6517819706498952\t0.6228511530398323\t0.23513351503012114\t0.42521967258488086\t0.5826530451232536\t0.5157772532026943\t0.5504777655545816\t0.5917016322045907\t\n"
     ]
    }
   ],
   "source": [
    "# CLF with Question MS vectors\n",
    "max_questions = np.max([len(qid2questions2MSvec[i]) for i in qid2questions2MSvec])\n",
    "MAX_SEQUENCE_LENGHT = int(np.max([len(qid2MSvec[i]) for i in qid2MSvec])) + max_questions + 1\n",
    "# def runDocEval(qid2MSvec, prop2MSvec):\n",
    "class CLF():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.queryInput = Input(shape=(MAX_SEQUENCE_LENGHT, 100,))\n",
    "        self.propPosInput = Input(shape=(100,))\n",
    "        lstm = LSTM(100)\n",
    "\n",
    "        queryEmbeddingLayer = Dense(100, name=\"uEmb\")\n",
    "        propEmbeddingLayer = Dense(100, name=\"iEmb\")\n",
    "\n",
    "#         qEmb = queryEmbeddingLayer(lstm(self.queryInput))\n",
    "        qEmb = queryEmbeddingLayer(self.queryInput)\n",
    "        pEmb = propEmbeddingLayer(self.propPosInput)\n",
    "\n",
    "        dense = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "        pred = Multiply()([qEmb, pEmb])\n",
    "        \n",
    "        self.pred = dense(GlobalMaxPooling1D()(pred))\n",
    "#         self.pred = dense(pred)\n",
    "\n",
    "        self.model = Model(inputs=[self.queryInput, self.propPosInput], outputs=self.pred)\n",
    "        self.model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "    def generate_train_data(self, df):\n",
    "        x_query, x_pos_prop, y = [], [], []\n",
    "        for name, group in df.groupby(\"query_id\"):\n",
    "            cand_pos_prop = group.property.tolist()\n",
    "            rele2prop = group[['rele_label', 'property']].groupby(\"rele_label\")[\"property\"].apply(list).to_dict()\n",
    "            for idx, row in group.iterrows():\n",
    "                if name in qid2questions2MSvec:\n",
    "                    x_query.append(qid2MSvec[name] + qid2questions2MSvec[name] + [qid2query2MSvec[name]])\n",
    "                else:\n",
    "                    x_query.append(qid2MSvec[name] + [qid2query2MSvec[name]])\n",
    "#                 x_query.append([qid2query2MSvec[name]])\n",
    "                x_pos_prop.append(prop2MSvec[row['property']])\n",
    "                y.append(1)\n",
    "        \n",
    "            cand_neg_prop = list(set.difference(set(type2prop[row['entityType']]), set(cand_pos_prop)))\n",
    "#             cand_neg_prop = list(set.difference(set(df.property.unique()), set(cand_pos_prop)))\n",
    "            \n",
    "            count = 0\n",
    "            while count != len(group):\n",
    "                for neg_prop in cand_neg_prop:\n",
    "                    if name in qid2questions2MSvec:\n",
    "                        x_query.append(qid2MSvec[name] + qid2questions2MSvec[name] + [qid2query2MSvec[name]])\n",
    "                    else:\n",
    "                        x_query.append(qid2MSvec[name] + [qid2query2MSvec[name]])\n",
    "    #                 x_query.append([qid2query2MSvec[name]])\n",
    "                    x_pos_prop.append(prop2MSvec[neg_prop])\n",
    "                    y.append(0)\n",
    "                    count += 1\n",
    "                    if count == len(group):\n",
    "                        break\n",
    "\n",
    "        x_query = pad_sequences(x_query,dtype='float32', maxlen=MAX_SEQUENCE_LENGHT)\n",
    "        x_pos_prop = np.array(x_pos_prop)\n",
    "        return [x_query, x_pos_prop], np.array(y)\n",
    "\n",
    "df_train = df[df.query_id.isin(trainIds)]\n",
    "bpr = CLF()\n",
    "x_train, y_train = bpr.generate_train_data(df_train)\n",
    "bestNDCG = 0\n",
    "bestRes = {}\n",
    "for i in range(10):\n",
    "    test_num = 0\n",
    "    history = bpr.model.fit(x_train, y_train, verbose=0)\n",
    "    res = {\"%s@%d\" %( i,j): [] for i in [\"p\", \"r\", \"ndcg\"] for j in [5, 10 ,15]}\n",
    "    for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "        if row['query_id'] not in testIds:\n",
    "            continue\n",
    "        if row['entity'] not in entityInWiki:\n",
    "                continue\n",
    "        test_num += 1\n",
    "        qrels = qrel[str(row['query_id'])]\n",
    "        cand_properties = type2prop[row['entityType']]\n",
    "#         cand_properties = list(prop2MSvec.keys())\n",
    "#         cand_properties = list(set(type2prop[row['entityType']].tolist() + list(qrels.keys())))\n",
    "#         cand_properties = list(set(type2prop[row['entityType']].tolist() + list(qrels.keys())))\n",
    "\n",
    "        rank = {}\n",
    "        for p in cand_properties:\n",
    "            if row['query_id'] in qid2questions2MSvec:\n",
    "                score = bpr.model.predict([pad_sequences([qid2MSvec[row['query_id']]+ qid2questions2MSvec[row['query_id']] + [qid2query2MSvec[row['query_id']]]], dtype='float32', maxlen=MAX_SEQUENCE_LENGHT), np.array([prop2MSvec[p]])])[0][0]\n",
    "            else:    \n",
    "                score = bpr.model.predict([pad_sequences([qid2MSvec[row['query_id']] + [qid2query2MSvec[row['query_id']]]], dtype='float32', maxlen=MAX_SEQUENCE_LENGHT), np.array([prop2MSvec[p]])])[0][0]\n",
    "#             score = bpr.model.predict([pad_sequences([[qid2query2MSvec[row['query_id']]]], dtype='float32', maxlen=MAX_SEQUENCE_LENGHT), np.array([prop2MSvec[p]])])[0][0]\n",
    "            rank[p] = score\n",
    "#         rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "        rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "        our = evaluate(qrels, rank)\n",
    "        for key in res:\n",
    "            res[key].append(our[key])\n",
    "            \n",
    "    ndcg = np.mean(res[\"ndcg@15\"])\n",
    "    if ndcg > bestNDCG:\n",
    "        bestRes = res\n",
    "        bestNDCG = ndcg\n",
    "\n",
    "    for key in res:\n",
    "        print(np.mean(res[key]), end=\"\\t\")\n",
    "    print()\n",
    "# runDocEval(qid2MSvec, prop2MSvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jarana/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jarana/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jarana/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jarana/anaconda3/envs/keras/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jarana/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jarana/anaconda3/envs/keras/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/jarana/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bestNDCG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-3e705b7da29c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mour\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mndcg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ndcg@15\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mndcg\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbestNDCG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mbestRes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mbestNDCG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndcg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bestNDCG' is not defined"
     ]
    }
   ],
   "source": [
    "# Most recent and best performing model\n",
    "MAX_SEQUENCE_LENGHT = int(np.max([len(qid2MSvec[i]) for i in qid2MSvec])) + 1\n",
    "# def runDocEval(qid2MSvec, prop2MSvec):\n",
    "class CLF():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.queryInput = Input(shape=(MAX_SEQUENCE_LENGHT, 100,))\n",
    "        self.propPosInput = Input(shape=(100,))\n",
    "        lstm = LSTM(100)\n",
    "\n",
    "        queryEmbeddingLayer = Dense(100, name=\"uEmb\")\n",
    "        propEmbeddingLayer = Dense(100, name=\"iEmb\")\n",
    "\n",
    "#         qEmb = queryEmbeddingLayer(lstm(self.queryInput))\n",
    "        qEmb = queryEmbeddingLayer(self.queryInput)\n",
    "        pEmb = propEmbeddingLayer(self.propPosInput)\n",
    "\n",
    "        dense = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "        pred = Multiply()([qEmb, pEmb])\n",
    "        \n",
    "        self.pred = dense(GlobalMaxPooling1D()(pred))\n",
    "#         self.pred = dense(pred)\n",
    "\n",
    "        self.model = Model(inputs=[self.queryInput, self.propPosInput], outputs=self.pred)\n",
    "        self.model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "    def generate_train_data(self, df):\n",
    "        x_query, x_pos_prop, y = [], [], []\n",
    "        for name, group in df.groupby(\"query_id\"):\n",
    "            cand_pos_prop = group.property.tolist()\n",
    "            rele2prop = group[['rele_label', 'property']].groupby(\"rele_label\")[\"property\"].apply(list).to_dict()\n",
    "            for idx, row in group.iterrows():\n",
    "                x_query.append(qid2MSvec[name] + [qid2query2MSvec[name]])\n",
    "#                 x_query.append([qid2query2MSvec[name]])\n",
    "                x_pos_prop.append(prop2MSvec[row['property']])\n",
    "                y.append(1)\n",
    "\n",
    "            cand_neg_prop = list(set.difference(set(type2prop[row['entityType']]), set(cand_pos_prop)))\n",
    "#             cand_neg_prop = list(set.difference(set(df.property.unique()), set(cand_pos_prop)))\n",
    "            for neg_prop in cand_neg_prop:\n",
    "                x_query.append(qid2MSvec[name] + [qid2query2MSvec[name]])\n",
    "#                 x_query.append([qid2query2MSvec[name]])\n",
    "                x_pos_prop.append(prop2MSvec[neg_prop])\n",
    "                y.append(0)\n",
    "\n",
    "        x_query = pad_sequences(x_query,dtype='float32', maxlen=MAX_SEQUENCE_LENGHT)\n",
    "        x_pos_prop = np.array(x_pos_prop)\n",
    "        return [x_query, x_pos_prop], np.array(y)\n",
    "\n",
    "df_train = df[df.query_id.isin(trainIds)]\n",
    "bpr = CLF()\n",
    "x_train, y_train = bpr.generate_train_data(df_train)\n",
    "for i in range(10):\n",
    "    test_num = 0\n",
    "    history = bpr.model.fit(x_train, y_train, verbose=0)\n",
    "    res = {\"%s@%d\" %( i,j): [] for i in [\"p\", \"r\", \"ndcg\"] for j in [5, 10 ,15]}\n",
    "    for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "        if row['query_id'] not in testIds:\n",
    "            continue\n",
    "        if row['entity'] not in entityInWiki:\n",
    "                continue\n",
    "        test_num += 1\n",
    "        qrels = qrel[str(row['query_id'])]\n",
    "        cand_properties = type2prop[row['entityType']]\n",
    "#         cand_properties = list(prop2MSvec.keys())\n",
    "#         cand_properties = list(set(type2prop[row['entityType']].tolist() + list(qrels.keys())))\n",
    "#         cand_properties = list(set(type2prop[row['entityType']].tolist() + list(qrels.keys())))\n",
    "\n",
    "        rank = {}\n",
    "        for p in cand_properties:\n",
    "            score = bpr.model.predict([pad_sequences([qid2MSvec[row['query_id']] + [qid2query2MSvec[row['query_id']]]], dtype='float32', maxlen=MAX_SEQUENCE_LENGHT), np.array([prop2MSvec[p]])])[0][0]\n",
    "#             score = bpr.model.predict([pad_sequences([[qid2query2MSvec[row['query_id']]]], dtype='float32', maxlen=MAX_SEQUENCE_LENGHT), np.array([prop2MSvec[p]])])[0][0]\n",
    "            rank[p] = score\n",
    "#         rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "        rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "        our = evaluate(qrels, rank)\n",
    "        for key in res:\n",
    "            res[key].append(our[key])\n",
    "    ndcg = np.mean(res[\"ndcg@15\"])\n",
    "    if ndcg > bestNDCG:\n",
    "        bestRes = res\n",
    "        bestNDCG = ndcg\n",
    "\n",
    "for key in bestRes:\n",
    "    print(np.mean(bestRes[key]), end=\"\\t\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qid2query2MSvec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-8daab49f637d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbestRes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mrunEval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqid2query2MSvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop2MSvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'qid2query2MSvec' is not defined"
     ]
    }
   ],
   "source": [
    "def runEval(qid2vec, prop2vec):\n",
    "    class CLF():\n",
    "        def __init__(self):\n",
    "\n",
    "            self.queryInput = Input(shape=(100,))\n",
    "            self.propPosInput = Input(shape=(100,))\n",
    "\n",
    "            queryEmbeddingLayer = Dense(100, name=\"uEmb\")\n",
    "            propEmbeddingLayer = Dense(100, name=\"iEmb\")\n",
    "\n",
    "            qEmb = queryEmbeddingLayer(self.queryInput)\n",
    "            pEmb = propEmbeddingLayer(self.propPosInput)\n",
    "\n",
    "            dense = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "            pred = Multiply()([qEmb, pEmb])\n",
    "            self.pred = dense(pred)\n",
    "\n",
    "            self.model = Model(inputs=[self.queryInput, self.propPosInput], outputs=self.pred)\n",
    "            self.model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "    #         self.predictor = Model([self.queryInput, self.propPosInput], [pDot])\n",
    "    #         self.predictor = Model([self.entityInput, self.entityTypeInput, self.actionInput, self.propPosInput], [pDot])\n",
    "        def generate_train_data(self, df):\n",
    "            x_query, x_entity, x_type, x_action, x_pos_prop, x_neg_prop, y = [], [], [], [], [], [], []\n",
    "            for name, group in df.groupby(\"query_id\"):\n",
    "                cand_pos_prop = group.property.tolist()\n",
    "                rele2prop = group[['rele_label', 'property']].groupby(\"rele_label\")[\"property\"].apply(list).to_dict()\n",
    "\n",
    "                for idx, row in group.iterrows():\n",
    "\n",
    "                    x_query.append(qid2query2MSvec[name])\n",
    "                    x_pos_prop.append(prop2vec[row['property']])\n",
    "                    y.append(1)\n",
    "\n",
    "                cand_neg_prop = list(set.difference(set(type2prop[row['entityType']]), set(cand_pos_prop)))\n",
    "    #             cand_neg_prop = list(set.difference(set(df.property.unique()), set(cand_pos_prop)))\n",
    "                for neg_prop in cand_neg_prop:\n",
    "                    x_query.append(qid2query2MSvec[name])\n",
    "                    x_pos_prop.append(prop2vec[neg_prop])\n",
    "                    y.append(0)\n",
    "\n",
    "            x_query = np.array(x_query)\n",
    "            x_entity = np.array(x_entity)\n",
    "            x_type = np.array(x_type)\n",
    "            x_action = np.array(x_action)\n",
    "            x_pos_prop = np.array(x_pos_prop)\n",
    "            x_neg_prop = np.array(x_neg_prop)\n",
    "    #         return [x_entity, x_type, x_action, x_pos_prop, x_neg_prop], np.ones(len(x_query))\n",
    "            return [x_query, x_pos_prop], np.array(y)\n",
    "\n",
    "    df_train = df[df.query_id.isin(trainIds)]\n",
    "    bpr = CLF()\n",
    "    bestNDCG = 0\n",
    "    for i in range(10):\n",
    "        x_train, y_train = bpr.generate_train_data(df_train)\n",
    "        res = {\"%s@%d\" %( i,j): [] for i in [\"p\", \"r\", \"ndcg\"] for j in [5, 10 ,15]}\n",
    "        history = bpr.model.fit(x_train, y_train, verbose=0)\n",
    "        for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "            if row['query_id'] not in testIds:\n",
    "                continue\n",
    "            if row['entity'] not in entityInWiki:\n",
    "                continue\n",
    "            qrels = qrel[str(row['query_id'])]\n",
    "            cand_properties = type2prop[row['entityType']]\n",
    "#             cand_properties = list(set(type2prop[row['entityType']].tolist() + list(qrels.keys())))\n",
    "\n",
    "            rank = {}\n",
    "            for p in cand_properties:\n",
    "                score = bpr.model.predict([[qid2query2MSvec[row['query_id']]], [prop2vec[p]]])[0][0]\n",
    "                rank[p] = score\n",
    "            rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "            our = evaluate(qrels, rank)\n",
    "#             res.append(our)\n",
    "            \n",
    "            for key in res:\n",
    "                res[key].append(our[key])\n",
    "        ndcg = np.mean(res[\"ndcg@15\"])\n",
    "        if ndcg > bestNDCG:\n",
    "            bestRes = res\n",
    "            bestNDCG = ndcg\n",
    "    for key in bestRes:\n",
    "        print(np.mean(bestRes[key]), end=\"\\t\")\n",
    "    print()\n",
    "runEval(qid2query2MSvec, prop2MSvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prop2MSvec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-6da2354da7ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcand_properties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#         score = cosine_similarity([qid2query2MSvec[row['query_id']]], [prop2MSvec[p]])[0][0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqid2MSvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprop2MSvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mrank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prop2MSvec' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "for idx, row in df[['query_id', 'entity', 'action', 'entityType', 'query']].drop_duplicates().iterrows():\n",
    "    if row['query_id'] not in testIds:\n",
    "        continue\n",
    "    if row['entity'] not in entityInWiki:\n",
    "        continue\n",
    "    qrels = qrel[str(row['query_id'])]\n",
    "    cand_properties = type2prop[row['entityType']]\n",
    "\n",
    "    rank = {}\n",
    "    for p in cand_properties:\n",
    "#         score = cosine_similarity([qid2query2MSvec[row['query_id']]], [prop2MSvec[p]])[0][0]\n",
    "        score = cosine_similarity(qid2MSvec[row['query_id']], [prop2MSvec[p]])[0][0]\n",
    "        rank[p] = score\n",
    "    rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "    our = evaluate(qrels, rank)\n",
    "#             res.append(our)\n",
    "\n",
    "    for key in res:\n",
    "        res[key].append(our[key])\n",
    "\n",
    "for key in res:\n",
    "    print(np.mean(res[key]), end=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6101604278074866\t0.5948900772430183\t0.5896316102198454\t0.5574123588829472\t0.20284161831080866\t0.3989336998954003\t0.5614833881840439\t0.6809485818203794\t0.40964763205666543\t0.47519438511873113\t0.5273372667425006\t0.5693692367007145\t0.5358990143229291\t0.5609055996150769\t0.563725013589058\t0.5643480165469257\t0.5830465112235345\t\n",
      "0.6850267379679144\t0.6479946524064172\t0.6486185383244206\t0.6317289172916564\t0.23021496830630514\t0.41787634293956993\t0.5984617287476647\t0.7509116055739267\t0.476448623780293\t0.5278082500975781\t0.5786404600253108\t0.6365679457993964\t0.5914453777924802\t0.6107507241365906\t0.6127116370974913\t0.6129443731413323\t0.6199307191531469\t\n",
      "0.5379679144385027\t0.5117349970291147\t0.543285799168152\t0.5485888294711824\t0.19291116746854142\t0.34842390242658505\t0.5183212550711478\t0.6702678707001852\t0.37389565453132\t0.42318352491460753\t0.48473882856970074\t0.5464452199439942\t0.5017111667854726\t0.5313927487094723\t0.5367587117675158\t0.537517480835144\t0.5496962850794768\t\n"
     ]
    }
   ],
   "source": [
    "res, res2, res3 = [], [], []\n",
    "for fold in range(5):\n",
    "    _df = pd.read_csv(\"data/cedr/test%d.tsv\" %fold, sep=\"\\t\", usecols=[0], names=[\"qid\"])\n",
    "    testIds = _df.qid.unique()\n",
    "    prop2popularity = df.groupby(\"property\").size().to_dict()\n",
    "\n",
    "    \n",
    "    for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "        if row['query_id'] not in testIds:\n",
    "            continue\n",
    "#         if row['entity'] not in entityInWiki:\n",
    "#             continue\n",
    "        qrels = qrel[str(row['query_id'])]\n",
    "        cand_properties = type2prop[row['entityType']]\n",
    "\n",
    "        rank = {}\n",
    "        for p in cand_properties:\n",
    "            rank[p] = 1\n",
    "        rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "        our = evaluate(qrels, rank)\n",
    "        \n",
    "    #     our = evaluate(qrels, cand_properties)\n",
    "        base = evaluate(qrels, list(run[\"1\"][str(row[\"query_id\"])].keys()))\n",
    "        res.append(our)\n",
    "        res2.append(base)\n",
    "    #     print(key, np.mean(res2[key]))\n",
    "    \n",
    "        rank = {}\n",
    "        for p in cand_properties:\n",
    "            rank[p] = prop2popularity[p] if p in prop2popularity else 0\n",
    "        rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "        our = evaluate(qrels, rank)\n",
    "        res3.append(our)\n",
    "\n",
    "keys = {\"%s@%d\" %( i,j): [] for i in [\"p\", \"r\", \"ndcg\", \"nerr\"] for j in [5, 10 ,15, 20]}\n",
    "keys[\"rp\"] = []\n",
    "for r in [res, res2, res3]:\n",
    "    for k in keys:\n",
    "        print(np.mean([i[k] for i in r]), end=\"\\t\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create property index\n",
    "prop2pid = {p:i for i, p in enumerate(dfp.property.unique())}\n",
    "df[\"pid\"] = [prop2pid[i] for i in df['property']]\n",
    "type2allprop = dfp.groupby(\"entityType\")['property'].unique().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \" [SEP] \".join([title, overview, sectionLabel, headline, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.drop_duplicates(\"query_id\").iterrows():\n",
    "    with open(\"data/cedr/entity.tsv\", 'a') as fd:\n",
    "        fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+ row['entity'] +\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm25 = pd.read_csv(\"data/cedr/query-title-bm25.tsv\", names=[\"q\", \"qid\", \"text\"], sep=\"\\t\")\n",
    "# for col in [\"overview\", \"sectionLabel\", \"headline\", \"text\"]:\n",
    "for col in [\"question-qw\"]:\n",
    "    for idx, row in df.drop_duplicates(\"query_id\").iterrows():\n",
    "    #     with open(\"data/cedr/query.tsv\", 'a') as fd:\n",
    "    #         fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+row[\"entity\"]+\" \"+row[\"action\"]+\"\\n\")\n",
    "        with open(\"data/cedr/%s.tsv\" % col, 'a') as fd:\n",
    "            query = row['action'] + \" \" + row['entity']\n",
    "            query = query.replace(\"\\'\", \"\")\n",
    "#             tokenized_query = query.split(\" \")\n",
    "    #         doc = bm25.get_top_n(tokenized_query, corpus, n=10)\n",
    "    #         titles = [doc2title[i] for i in doc]\n",
    "    #         overview = doc[0].split(\" [SEP] \")[4]\n",
    "    #         text = \" \".join(df_wiki[df_wiki.title == titles[0]].text.unique().tolist())\n",
    "    #         print(overview)\n",
    "    #     break\n",
    "    #         text = \" \".join(df_wiki[df_wiki.title == titles[0]].text.tolist())\n",
    "    #         fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\" \"+titles[0]+ \" \" + text +\"\\n\")\n",
    "    #         fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\" \"+titles[0] +\"\\n\")\n",
    "    \n",
    "#     questions\n",
    "            title = [df_bm25[df_bm25.text.str.contains(query[:10])].text.values[0].replace(query+\" \", \"\")]\n",
    "            try:\n",
    "                lines = open(\"data/questions/wiki/%s.csv\" % title[0].replace(\" \", \"_\")).read().splitlines()[1:]\n",
    "            except:\n",
    "#                 fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\" \"+title[0] +\"\\n\")\n",
    "#                 fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\"\\n\")\n",
    "                fd.write(\"question\\t\"+str(row['query_id'])+\"\\t\"+query+\"\\n\")\n",
    "                continue\n",
    "            questions = []\n",
    "            for i in lines[1:]:\n",
    "                questions.extend(i.split(\";\"))\n",
    "            questions = \" \".join(list(set(questions)))\n",
    "            if questions == \"\":\n",
    "                fd.write(\"question\\t\"+str(row['query_id'])+\"\\t\"+query+\"\\n\")\n",
    "            else:\n",
    "#             fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\" \"+title[0]+ \" \" + questions +\"\\n\")\n",
    "#             fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\" \" + questions +\"\\n\")\n",
    "                fd.write(\"question\\t\"+str(row['query_id'])+\"\\t\"+ questions +\"\\n\")\n",
    "            \n",
    "#         break\n",
    "            \n",
    "#             text = \" \".join(df_wiki[df_wiki.title == titles[0]][col].unique().tolist())\n",
    "#             fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\" \"+titles[0]+ \" \" + text +\"\\n\")\n",
    "\n",
    "    #         if row['query_id'] not in qid2wiki:\n",
    "    #             fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\"\\n\")\n",
    "    #             doc = bm25.get_top_n(tokenized_query, corpus, n=5)\n",
    "    #             title = [doc2title[i] for i in doc]\n",
    "    #             print(row['action'], \",\", row['entity'], \"\\t\", title)\n",
    "    #             continue\n",
    "    #         BERT\n",
    "    #         titles = dfw[dfw.title.str.contains(row['entity'])].title.unique().tolist()\n",
    "    #         rank = {}\n",
    "    #         for title in titles:\n",
    "    #             rank[title] = bertSim(query, title)\n",
    "    #         best_title = sorted(rank.items(), key=lambda x: x[1])[-1][0]\n",
    "    #         text = \" \".join(df_wiki[df_wiki.title == best_title].text.tolist())\n",
    "    #         fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+query+\" \"+best_title+ \" \"+ text +\"\\n\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm25 = pd.read_csv(\"data/cedr/query-title-bm25.tsv\", names=[\"q\", \"qid\", \"text\"], sep=\"\\t\")\n",
    "for col in [\"overview\", \"headline\", \"sectionLabel\",\"text\"]:\n",
    "    for idx, row in df.drop_duplicates(\"query_id\").iterrows():\n",
    "        with open(\"data/cedr/wiki-%s-bm25.tsv\" % col, 'a') as fd:\n",
    "            query = row['action'] + \" \" + row['entity']\n",
    "            query = query.replace(\"\\'\", \"\")\n",
    "            title = df_bm25[df_bm25.text.str.contains(query[:10])].text.values[0].replace(query+\" \", \"\")\n",
    "            text = \" \".join(df_wiki[df_wiki.title == title][col].unique().tolist())\n",
    "            if text == \"steps\" or text == \"\":\n",
    "                fd.write(\"wiki\\t\"+str(row['query_id'])+\"\\t\"+ query +\"\\n\")                \n",
    "            else:\n",
    "                fd.write(\"wiki\\t\"+str(row['query_id'])+\"\\t\"+ text +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222 spray lacquer ['What does spray lacquer do?', 'Does lacquer make wood waterproof?', 'What does spray lacquer do?', 'Does lacquer seal wood?', 'What is the difference between varnish and lacquer?', 'Is lacquer good for outdoors?', 'What is the difference between varnish and lacquer?', 'Does lacquer turn yellow?', 'What is the difference between varnish and lacquer?', \"What's better lacquer or polyurethane?\", 'How long does it take for spray lacquer to dry?', 'How long does lacquer last?', 'How long does it take for spray lacquer to dry?', 'Should I sand between coats of lacquer?', 'How long does it take for spray lacquer to dry?', 'Does lacquer scratch easily?', 'What is the difference between lacquer and polyurethane finish?', 'What is the best lacquer for wood?', 'What is the difference between lacquer and polyurethane finish?', 'Is lacquer thicker than polyurethane?', 'What is the difference between lacquer and polyurethane finish?', 'Can you brush on lacquer?']       query_id                   property  rele_label    query   entity  \\\n",
      "3655       222                      brand           3  lacquer  lacquer   \n",
      "3656       222            isConsumableFor           2  lacquer  lacquer   \n",
      "3657       222              alternateName           1  lacquer  lacquer   \n",
      "3658       222               manufacturer           2  lacquer  lacquer   \n",
      "3659       222                       logo           2  lacquer  lacquer   \n",
      "3660       222                     offers           2  lacquer  lacquer   \n",
      "3661       222              itemCondition           2  lacquer  lacquer   \n",
      "3662       222                 identifier           1  lacquer  lacquer   \n",
      "3663       222                      model           2  lacquer  lacquer   \n",
      "3664       222            potentialAction           2  lacquer  lacquer   \n",
      "3665       222                     review           4  lacquer  lacquer   \n",
      "3666       222                  productID           1  lacquer  lacquer   \n",
      "3667       222                isSimilarTo           2  lacquer  lacquer   \n",
      "3668       222                      depth           2  lacquer  lacquer   \n",
      "3669       222  isAccessoryOrSparePartFor           2  lacquer  lacquer   \n",
      "3670       222                      award           3  lacquer  lacquer   \n",
      "3671       222                   material           2  lacquer  lacquer   \n",
      "3672       222                      color           3  lacquer  lacquer   \n",
      "\n",
      "         entityType             action  pid  \n",
      "3655  thing product  use acrylic paint  116  \n",
      "3656  thing product  use acrylic paint  270  \n",
      "3657  thing product  use acrylic paint   16  \n",
      "3658  thing product  use acrylic paint  183  \n",
      "3659  thing product  use acrylic paint  113  \n",
      "3660  thing product  use acrylic paint   61  \n",
      "3661  thing product  use acrylic paint  265  \n",
      "3662  thing product  use acrylic paint   18  \n",
      "3663  thing product  use acrylic paint  267  \n",
      "3664  thing product  use acrylic paint   11  \n",
      "3665  thing product  use acrylic paint   63  \n",
      "3666  thing product  use acrylic paint  263  \n",
      "3667  thing product  use acrylic paint  119  \n",
      "3668  thing product  use acrylic paint  272  \n",
      "3669  thing product  use acrylic paint  266  \n",
      "3670  thing product  use acrylic paint   51  \n",
      "3671  thing product  use acrylic paint   30  \n",
      "3672  thing product  use acrylic paint  264  \n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# qid2questions2MSvec = {}\n",
    "for file in glob.glob(\"akgBM25/*\"):\n",
    "    title = file.split(\"/\")[1].replace(\"_\", \" \").replace(\".csv\", \"\")\n",
    "    lines = open(file).read().splitlines()\n",
    "    questions = []\n",
    "    for i in lines[1:]:\n",
    "        questions.extend(i.split(\";\"))\n",
    "    print(bestTitle2qid[title], title, questions, df[df.query_id == bestTitle2qid[title]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    break\n",
    "#     title2questions[title] = list(set(questions))\n",
    "#     data = getVectors([title] + list(set(questions)) )\n",
    "#     qid2questions2MSvec[bestTitle2qid[title]] = list(data.values())\n",
    "# qid2questions2MSvec = loadDict(\"qid2questions2MSvec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmarking have thing action\n",
      "overclock graphics card\n",
      "\n",
      "bodybuilding take thing action\n",
      "begin bodybuilding\n",
      "\n",
      "brainstorming brainstorm thing action\n",
      "develop alternative perspectives decision making\n",
      "\n",
      "chain smoking start thing action\n",
      "quit smoking using allen carr book\n",
      "\n",
      "funding get funding thing action\n",
      "get venture capital investment\n",
      "\n",
      "hair coloring bleach hair back to brown thing action\n",
      "ombre hair\n",
      "\n",
      "hit and run stop at the scene of an accident thing action\n",
      "know whether call police after car accident\n",
      "\n",
      "language acquisition suggest a foreign language thats easy to learn thing action\n",
      "teach adults foreign language\n",
      "\n",
      "mediation help you come up with possible solutions thing action\n",
      "prepare child custody mediation\n",
      "\n",
      "pull up do any aerobic physical activity thing action\n",
      "improve your study routine exercise\n",
      "\n",
      "spoofing attack know who do the spoof  thing action\n",
      "make funny movie spoof\n",
      "\n",
      "spray painting use on fabric thing action\n",
      "spray paint your sofa\n",
      "\n",
      "translation know japanese recipe ingredient translation thing action\n",
      "learn japanese words\n",
      "\n",
      "all about my mother forget thing creativework movie\n",
      "do mastering sims 2 machima\n",
      "\n",
      "back to the future remember thing creativework movie\n",
      "document your child educational milestones\n",
      "\n",
      "braveheart love that movie thing creativework movie\n",
      "hardcore dance\n",
      "\n",
      "give me five give fives thing creativework musicrecording\n",
      "make viral video\n",
      "\n",
      "on the road follow thing creativework book\n",
      "write book title\n",
      "\n",
      "our daughter leave thing creativework movie\n",
      "improve your mother daughter relationship\n",
      "\n",
      "run fast happen thing creativework musicrecording\n",
      "meet john cena\n",
      "\n",
      "shakespeares plays prepare for a shakespeare play thing creativework\n",
      "read shakespeare beginners\n",
      "\n",
      "showtime choose thing creativework movie\n",
      "search movie showtimes using spotlight search iphone\n",
      "\n",
      "so young abandon thing creativework movie\n",
      "create large scale home movies\n",
      "\n",
      "striptease obsess thing creativework movie\n",
      "perform striptease\n",
      "\n",
      "take on me forget thing creativework musicrecording\n",
      "meet john cena\n",
      "\n",
      "terms of endearment come thing creativework movie\n",
      "name main character\n",
      "\n",
      "the devil wears prada watch the devil wears prada thing creativework movie\n",
      "be christcore\n",
      "\n",
      "the wizard of oz watch full episode of robot chicken thing creativework movie\n",
      "decide what you be halloween\n",
      "\n",
      "the incredible hulk watch movie with english subtitle thing creativework movie\n",
      "add subtitles movie\n",
      "\n",
      "the paper make love with a girl thing creativework movie\n",
      "come up movie idea\n",
      "\n",
      "the pyramid reclaim thing creativework movie\n",
      "rent camel cairo\n",
      "\n",
      "the sixth sense see the spirits of people who have died thing creativework movie\n",
      "decorate bedroom like haunted house\n",
      "\n",
      "the tall man meet a good woman thing creativework movie\n",
      "shag dance\n",
      "\n",
      "today lose thing creativework tvseries\n",
      "yolo\n",
      "\n",
      "trust me make trust thing creativework movie\n",
      "deal with going different school than your boyfriend girlfriend\n",
      "\n",
      "porco rosso watch porco rosso ( 1992 ) thing creativework movie\n",
      "write source card\n",
      "\n",
      "prom night love the colors of prom thing creativework movie\n",
      "act like carrie white\n",
      "\n",
      "knowing find the name of a song  thing creativework movie\n",
      "get sync rights\n",
      "\n",
      "krrish see thing creativework movie\n",
      "explode team fortress \n",
      "\n",
      "law & order: special victims unit win emmy thing creativework tvseries\n",
      "take action against lease violations\n",
      "\n",
      "lost season 3 release on dvd thing creativework tvseries\n",
      "watch walking dead season 3\n",
      "\n",
      "mary poppins watch this video thing creativework movie\n",
      "be leading lady\n",
      "\n",
      "baby shower give thing event\n",
      "decorate baby shower\n",
      "\n",
      "chernobyl disaster happen thing event\n",
      "help children cope disaster\n",
      "\n",
      "formula one become a f1 driver thing event sportsevent\n",
      "become f1 driver\n",
      "\n",
      "graduation attend ceremony thing event educationevent\n",
      "prepare for graduation\n",
      "\n",
      "honeymoon recommend thing event\n",
      "find honeymoon venue greek islands\n",
      "\n",
      "immigration apply for citizenship thing event\n",
      "have dual citizenship us canada\n",
      "\n",
      "islamic terrorism issue indictment thing event\n",
      "prove multiplicity\n",
      "\n",
      "live aid raise dollars thing event\n",
      "fundraise your pta\n",
      "\n",
      "the exodus believe thing event\n",
      "become seventh day adventist\n",
      "\n",
      "world war ii win thing event\n",
      "learn about world war ii\n",
      "\n",
      "flood pevent thing event\n",
      "make flood shelter kids pre teens\n",
      "\n",
      "anti-gravity create an anti gravity propulsion system thing intangible\n",
      "amortize assets\n",
      "\n",
      "html embed a sound in a html page thing intangible computerlanguage\n",
      "embed bing map address\n",
      "\n",
      "php debug thing intangible computerlanguage\n",
      "amortize assets\n",
      "\n",
      "english grammar teach thing intangible\n",
      "amortize assets\n",
      "\n",
      "free education apply thing intangible\n",
      "amortize assets\n",
      "\n",
      "job satisfaction get thing intangible\n",
      "do cost analysis\n",
      "\n",
      "belief say thing intangible\n",
      "amortize assets\n",
      "\n",
      "tagalog language teach thing intangible language\n",
      "speak tagalog\n",
      "\n",
      "environmentalism support environmentalism and climate change awareness thing intangible\n",
      "escape materialism find happiness\n",
      "\n",
      "bibliography write the bibliography thing intangible\n",
      "write bibliography\n",
      "\n",
      "email box send thing intangible\n",
      "amortize assets\n",
      "\n",
      "google maps prepare thing intangible service\n",
      "calculate amortization patents\n",
      "\n",
      "property insurance receive thing intangible service financialproduct\n",
      "sell business assets\n",
      "\n",
      "voip phone use thing intangible service\n",
      "amortize assets\n",
      "\n",
      "separation of church and state allow thing intangible\n",
      "amortize assets\n",
      "\n",
      "indian institute of technology joint entrance examination pass thing intangible\n",
      "amortize assets\n",
      "\n",
      "history of television watch thing intangible\n",
      "amortize assets\n",
      "\n",
      "acupuncture work thing medicalentity medicalprocedure\n",
      "find licensed acupuncturist\n",
      "\n",
      "apgar score improve thing medicalentity medicalintangible\n",
      "read apgar score\n",
      "\n",
      "asthma relieve thing medicalentity medicalcondition\n",
      "countercondition cat\n",
      "\n",
      "back pain treat thing medicalentity medicalcondition\n",
      "countercondition cat\n",
      "\n",
      "brain develop thing medicalentity anatomicalsystem\n",
      "enable windows subsystem linux\n",
      "\n",
      "carpal tunnel syndrome suffer thing medicalentity medicalcondition\n",
      "diagnose carpal tunnel syndrome\n",
      "\n",
      "dna identify thing medicalentity anatomicalsystem\n",
      "get dna test\n",
      "\n",
      "hepatitis c diagnose thing medicalentity medicalcondition\n",
      "diagnose viral hepatitis\n",
      "\n",
      "insomnia cause thing medicalentity medicalcondition\n",
      "stop insomnia\n",
      "\n",
      "kidney donate thing medicalentity anatomicalsystem\n",
      "be kidney donor\n",
      "\n",
      "naproxen increase thing medicalentity substance drug\n",
      "choose counter pain medication\n",
      "\n",
      "pubis break thing medicalentity anatomicalsystem\n",
      "enable windows subsystem linux\n",
      "\n",
      "respiratory system evaluate thing medicalentity anatomicalsystem\n",
      "enable windows subsystem linux\n",
      "\n",
      "self-esteem boost thing medicalentity medicalintangible\n",
      "develop personal growth\n",
      "\n",
      "shock cause thing medicalentity medicalcondition\n",
      "countercondition cat\n",
      "\n",
      "skin care thing medicalentity anatomicalsystem\n",
      "enable windows subsystem linux\n",
      "\n",
      "speech disorder overcome thing medicalentity medicalcondition\n",
      "countercondition cat\n",
      "\n",
      "spinal muscular atrophy reduce thing medicalentity medicalcondition\n",
      "decide spinal decompression therapy is you\n",
      "\n",
      "testosterone increase thing medicalentity substance\n",
      "increase chest hair\n",
      "\n",
      "thyroid hormone produce thing medicalentity substance\n",
      "study medical concept hormone\n",
      "\n",
      "trichinosis stop thing medicalentity parasiticdisease\n",
      "treat droopy eyelids\n",
      "\n",
      "urinary incontinence prevent thing medicalentity medicalcondition\n",
      "improve incontinence\n",
      "\n",
      "blood type donate thing medicalentity medicalintangible\n",
      "get paid for donating plasma\n",
      "\n",
      "adidas wear thing organization corporation\n",
      "be nammer\n",
      "\n",
      "aol use thing organization corporation\n",
      "connect aol\n",
      "\n",
      "bacardi drink thing organization corporation\n",
      "make rum runner\n",
      "\n",
      "big oil sell thing organization corporation\n",
      "write nonprofit governing board statement\n",
      "\n",
      "bp own thing organization corporation\n",
      "start small business california\n",
      "\n",
      "carolina panthers play thing organization sportsorganization\n",
      "form llc south carolina\n",
      "\n",
      "chicago white sox win thing organization sportsorganization\n",
      "be chicago white sox fan\n",
      "\n",
      "espn watch thing organization broadcastchannel\n",
      "watch fifa world cup online\n",
      "\n",
      "fedex call thing organization corporation\n",
      "get job fedex\n",
      "\n",
      "fifa watch thing organization sportsorganization\n",
      "follow fifa world cup\n",
      "\n",
      "ford motor company sell thing organization corporation\n",
      "become car designer\n",
      "\n",
      "ibm join thing organization corporation\n",
      "install ibm java ubuntu linux\n",
      "\n",
      "isuzu motors build thing organization corporation\n",
      "program vex robotics clawbot\n",
      "\n",
      "liberal party of canada support thing organization governmentorganization\n",
      "find your political standpoint\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little league baseball join thing organization sportsorganization\n",
      "become baseball writer\n",
      "\n",
      "los angeles lakers win thing organization sportsorganization\n",
      "be good orlando magic fan\n",
      "\n",
      "major league baseball announce retirement thing organization sportsorganization\n",
      "become baseball writer\n",
      "\n",
      "maroon 5 perform ballad thing organization performinggroup musicgroup\n",
      "access shared folders windows 7\n",
      "\n",
      "maybelline buy makeup thing organization company\n",
      "buy beauty products budget\n",
      "\n",
      "miami heat win the championship thing organization sportsorganization\n",
      "be good orlando magic fan\n",
      "\n",
      "motorola unlock motorola c975 thing organization corporation\n",
      "remove motorola subsidy password message\n",
      "\n",
      "nvidia take complaint thing organization company\n",
      "update nvidia drivers\n",
      "\n",
      "people for the ethical treatment of animals treat animals thing organization ngo\n",
      "help stop cruelty towards animals\n",
      "\n",
      "pga tour win tournament thing organization sportsorganization\n",
      "win your ncaa basketball tournament pool\n",
      "\n",
      "singapore airlines operate route thing organization airline\n",
      "find cheap flights online\n",
      "\n",
      "tiscali get insurance thing organization corporation\n",
      "start title company\n",
      "\n",
      "visa inc. extend visa thing organization corporation\n",
      "avoid violating your b1 business visa\n",
      "\n",
      "walmart apply for a job thing organization corporation\n",
      "get job walmart\n",
      "\n",
      "university of houston apply for a job  thing organization educationalorganization collegeoruniversity\n",
      "become scholarship consultant\n",
      "\n",
      "abraham lincoln work lawyer thing person\n",
      "make abraham lincoln costume\n",
      "\n",
      "alan turing pass a turing test thing person\n",
      "plant turing sunflowers\n",
      "\n",
      "aristotle discover secrets thing person\n",
      "use whisper\n",
      "\n",
      "barbie hsu study relationship thing person\n",
      "install android htc hd\n",
      "\n",
      "batman play villain thing person\n",
      "play lego batman the videogame\n",
      "\n",
      "british royal family idolise family thing person\n",
      "formally address british royalty aristocracy person\n",
      "\n",
      "diana ross sings jazz thing person\n",
      "act like diana meade secret circle triology\n",
      "\n",
      "jason kidd provide scoring thing person\n",
      "be point guard\n",
      "\n",
      "jean-claude van damme splits  thing person\n",
      "entertain yourself\n",
      "\n",
      "jesus died on a tree thing person\n",
      "get closer with god our holy father\n",
      "\n",
      "johann sebastian bach listen  thing person\n",
      "listen bach\n",
      "\n",
      "kathy griffin watch premiere thing person\n",
      "keep with hipster\n",
      "\n",
      "luther vandross left money thing person\n",
      "model molecule using avogadro software\n",
      "\n",
      "pope benedict xvi visit pope thing person\n",
      "address pope\n",
      "\n",
      "prince charming become man thing person\n",
      "stop waiting prince charming\n",
      "\n",
      "roger federer win season thing person\n",
      "write sports article\n",
      "\n",
      "samuel beckett find hidden talent thing person\n",
      "discover your talents\n",
      "\n",
      "serena williams beat niculescu thing person\n",
      "care premature baby\n",
      "\n",
      "tink sing music thing person\n",
      "make region choir\n",
      "\n",
      "tony robbins live with passion thing person\n",
      "discover what you really want new career\n",
      "\n",
      "vin diesel become warming thing person\n",
      "become diesel mechanic\n",
      "\n",
      "acapulco hold mesino thing place\n",
      "make phone call mexico\n",
      "\n",
      "angola take crudes thing place\n",
      "form roda\n",
      "\n",
      "british columbia establish welfare thing place\n",
      "understand canadian slang\n",
      "\n",
      "chennai fire naphtha thing place\n",
      "remove stickers safely guitar\n",
      "\n",
      "cincinnati buy a beginner drum set thing place\n",
      "buy bongo drum\n",
      "\n",
      "dormitory provide meals thing place accommodation\n",
      "get around venice cheap\n",
      "\n",
      "egypt visit thing place\n",
      "time your trip cairo\n",
      "\n",
      "egyptian pyramids build thing place touristattraction\n",
      "enjoy attractions cairo\n",
      "\n",
      "finland go thing place\n",
      "write position paper model un\n",
      "\n",
      "glasgow look for a job  thing place\n",
      "become licensed taxi driver glasgow\n",
      "\n",
      "greece live in greece thing place\n",
      "get italy greece\n",
      "\n",
      "hawaii use wireless thing place\n",
      "get free cell phone\n",
      "\n",
      "himalayas go on a trek thing place\n",
      "go trekking himalayas\n",
      "\n",
      "iran discuss iran threat thing place\n",
      "visit iran\n",
      "\n",
      "kansas city locate thing place\n",
      "make science fair title\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-294-ad797db9fc45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mproperties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mentityType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm25\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_top_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#     print(tokenizer.tokenize(query), group.entityType.unique(), properties)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/rank_bm25.py\u001b[0m in \u001b[0;36mget_top_n\u001b[0;34m(self, query, documents, n)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The documents given don't match the index corpus!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mtop_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/rank_bm25.py\u001b[0m in \u001b[0;36mget_scores\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mdoc_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mq_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_freqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n\u001b[1;32m    114\u001b[0m                                                (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.7/site-packages/rank_bm25.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mdoc_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mq_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_freqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n\u001b[1;32m    114\u001b[0m                                                (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "for name, group in df.groupby(\"query_id\"):\n",
    "    entity = group.entity.unique()[0]\n",
    "    action = group.action.unique()[0]\n",
    "    entityType = group.entityType.unique()[0]\n",
    "    properties = \" \".join(group.property.unique())\n",
    "    query = entity + \" \" + action + \" \" + entityType\n",
    "    doc = bm25.get_top_n(tokenizer.tokenize(query), corpus, n=10)\n",
    "#     print(tokenizer.tokenize(query), group.entityType.unique(), properties)\n",
    "    print(query)\n",
    "    print(doc[0].split(\"\\t\")[0])\n",
    "    print()\n",
    "#     for d in doc:\n",
    "#         print(d.split(\"\\t\")[0])\n",
    "    \n",
    "    \n",
    "#     data = getVectors(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"entity\", \"action\", \"entityType\"]].drop_duplicates().to_csv(\"tmp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in df.groupby(\"query_id\"):\n",
    "    query = group.entity.iloc[0] + \" \" + group.action.iloc[0] + \" \" + group.entityType.iloc[0] # .replace(\"thing \",\"\").replace(\"thing\",\"\")\n",
    "    query = group.entityType.iloc[0] + \" \" + group.action.iloc[0]# .replace(\"thing \",\"\").replace(\"thing\",\"\")\n",
    "    with open(\"data/cedr/type-action.tsv\", 'a') as fd:\n",
    "        fd.write(\"query\\t\"+str(name)+\"\\t\"+query +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entityType\n",
       "thing                                                               5\n",
       "thing action                                                      139\n",
       "thing creativework                                                 22\n",
       "thing creativework book                                            12\n",
       "thing creativework movie                                          379\n",
       "thing creativework musicrecording                                  42\n",
       "thing creativework tvseries                                        61\n",
       "thing event                                                       157\n",
       "thing event educationevent                                         22\n",
       "thing event sportsevent                                            19\n",
       "thing intangible                                                   59\n",
       "thing intangible computerlanguage                                  11\n",
       "thing intangible language                                           4\n",
       "thing intangible service                                           38\n",
       "thing intangible service financialproduct                          21\n",
       "thing medicalentity anatomicalsystem                               82\n",
       "thing medicalentity medicalcondition                              200\n",
       "thing medicalentity medicalintangible                              23\n",
       "thing medicalentity medicalprocedure                               23\n",
       "thing medicalentity parasiticdisease                                9\n",
       "thing medicalentity substance                                      21\n",
       "thing medicalentity substance drug                                 22\n",
       "thing organization airline                                         25\n",
       "thing organization broadcastchannel                                23\n",
       "thing organization company                                         39\n",
       "thing organization corporation                                    256\n",
       "thing organization educationalorganization collegeoruniversity     19\n",
       "thing organization governmentorganization                          23\n",
       "thing organization ngo                                             20\n",
       "thing organization performinggroup musicgroup                      11\n",
       "thing organization sportsorganization                             150\n",
       "thing person                                                      330\n",
       "thing place                                                       434\n",
       "thing place accommodation                                          22\n",
       "thing place landform bodyofwater                                   20\n",
       "thing place landform mountain                                      33\n",
       "thing place localbusiness entertainmentbusiness                    16\n",
       "thing place touristattraction                                      16\n",
       "thing product                                                     822\n",
       "thing product vehicle                                              53\n",
       "thing product vehicle car                                         113\n",
       "dtype: int64"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"entityType\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['recordedAt', 'musicBy', 'character', 'workExample',\n",
       "       'educationalUse', 'releasedEvent', 'actor', 'author', 'version',\n",
       "       'isFamilyFriendly', 'material', 'publisher', 'producer',\n",
       "       'accountablePerson', 'commentCount', 'alternativeHeadline',\n",
       "       'headline', 'timeRequired', 'potentialAction', 'isPartOf',\n",
       "       'exampleOfWork', 'hasPart', 'isBasedOn', 'position', 'video',\n",
       "       'accessibilityHazard', 'mentions', 'provider', 'name', 'audio',\n",
       "       'copyrightYear', 'contentRating', 'creator', 'award', 'director',\n",
       "       'productionCompany', 'genre', 'schemaVersion', 'trailer',\n",
       "       'publication', 'audience', 'countryOfOrigin', 'comment',\n",
       "       'alternateName', 'inLanguage', 'offers', 'identifier',\n",
       "       'publishingPrinciples', 'text', 'contentLocation',\n",
       "       'locationCreated', 'translator', 'review', 'image'], dtype=object)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type2prop['thing creativework movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entityType\n",
       "thing                                                               5\n",
       "thing action                                                      139\n",
       "thing creativework                                                 22\n",
       "thing creativework book                                            12\n",
       "thing creativework movie                                          379\n",
       "thing creativework musicrecording                                  42\n",
       "thing creativework tvseries                                        61\n",
       "thing event                                                       157\n",
       "thing event educationevent                                         22\n",
       "thing event sportsevent                                            19\n",
       "thing intangible                                                   59\n",
       "thing intangible computerlanguage                                  11\n",
       "thing intangible language                                           4\n",
       "thing intangible service                                           38\n",
       "thing intangible service financialproduct                          21\n",
       "thing medicalentity anatomicalsystem                               82\n",
       "thing medicalentity medicalcondition                              200\n",
       "thing medicalentity medicalintangible                              23\n",
       "thing medicalentity medicalprocedure                               23\n",
       "thing medicalentity parasiticdisease                                9\n",
       "thing medicalentity substance                                      21\n",
       "thing medicalentity substance drug                                 22\n",
       "thing organization airline                                         25\n",
       "thing organization broadcastchannel                                23\n",
       "thing organization company                                         39\n",
       "thing organization corporation                                    256\n",
       "thing organization educationalorganization collegeoruniversity     19\n",
       "thing organization governmentorganization                          23\n",
       "thing organization ngo                                             20\n",
       "thing organization performinggroup musicgroup                      11\n",
       "thing organization sportsorganization                             150\n",
       "thing person                                                      330\n",
       "thing place                                                       434\n",
       "thing place accommodation                                          22\n",
       "thing place landform bodyofwater                                   20\n",
       "thing place landform mountain                                      33\n",
       "thing place localbusiness entertainmentbusiness                    16\n",
       "thing place touristattraction                                      16\n",
       "thing product                                                     822\n",
       "thing product vehicle                                              53\n",
       "thing product vehicle car                                         113\n",
       "dtype: int64"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"entityType\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in prop2pid:\n",
    "    with open(\"data/cedr/doc.tsv\", 'a') as fd:\n",
    "        fd.write(\"doc\\t\"+str(prop2pid[key])+\"\\t\"+key+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    with open(\"data/cedr/qrel.tsv\", 'a') as fd:\n",
    "        fd.write(str(row[\"query_id\"]) + \"\\t0\\t\"+str(row[\"pid\"])+\"\\t\"+str(row[\"rele_label\"])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5fold data generation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "foldNum = 5\n",
    "trainId5fold = {i:[] for i in range(foldNum)}\n",
    "validateId5fold = {i:[] for i in range(foldNum)}\n",
    "testId5fold = {i:[] for i in range(foldNum)}\n",
    "for name, group in df.groupby(\"entityType\"):\n",
    "    if group.query_id.nunique() >= foldNum:\n",
    "        ids = np.array(group.query_id.unique())\n",
    "        \n",
    "        kf = KFold(n_splits=foldNum)\n",
    "        kf.get_n_splits(ids)\n",
    "        for idx, ele in enumerate(kf.split(ids)):\n",
    "            train, test = ids[ele[0]], ids[ele[1]]\n",
    "            train, val, _, _ = train_test_split(train, train, test_size=0.2)\n",
    "            trainId5fold[idx].extend(train)\n",
    "            validateId5fold[idx].extend(val)\n",
    "            testId5fold[idx].extend(test)\n",
    "    else:\n",
    "        ids = list(group.query_id.unique())\n",
    "        for i in range(foldNum):\n",
    "            trainId5fold[i].extend(ids)\n",
    "\n",
    "def writeRunFile(filename, _df):\n",
    "    for name, group in _df.groupby(\"query_id\"):\n",
    "        for idx, row in enumerate(group.iterrows()):\n",
    "            with open(filename, 'a') as fd:\n",
    "                fd.write(str(row[1]['query_id']) +\"\\tQ0\\t\" + str(row[1]['pid']) + \"\\t\" + str(idx+1) + \"\\t\" + str(row[1]['rele_label']) + \"\\trun\\n\")\n",
    "                \n",
    "for fold in range(foldNum):\n",
    "    for idx, group in df[df.query_id.isin(trainId5fold[fold])].groupby(\"query_id\"):\n",
    "        pos = group['property'].tolist()\n",
    "        neg = list(set.difference(set(type2allprop[group.iloc[0]['entityType']]), set(pos)))\n",
    "\n",
    "        for idx, row in group.iterrows():\n",
    "            with open(\"data/cedr/train%d.tsv\" % fold, 'a') as fd:\n",
    "                fd.write(str(row[\"query_id\"]) + \"\\t\"+str(row[\"pid\"])+\"\\n\")\n",
    "        for n in neg:\n",
    "            with open(\"data/cedr/train%d.tsv\" % fold, 'a') as fd:\n",
    "                fd.write(str(row[\"query_id\"]) + \"\\t\"+str(prop2pid[n])+\"\\n\")\n",
    "    \n",
    "    for idx, group in df[df.query_id.isin(validateId5fold[fold])].groupby(\"query_id\"):\n",
    "        pos = group['property'].tolist()\n",
    "        neg = list(set.difference(set(type2allprop[group.iloc[0]['entityType']]), set(pos)))\n",
    "\n",
    "        lines = {}\n",
    "        for idx, row in group.iterrows():\n",
    "            lines[str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[row['property']])+\"\\t0\\t\" +str(row['rele_label'])+\"\\trun\\n\"] = row['rele_label']\n",
    "\n",
    "        for n in neg:\n",
    "            lines[str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[n])+\"\\t0\\t0\\trun\\n\"] = 0\n",
    "        lines = [i[0] for i in sorted(lines.items(), key=lambda x: x[1], reverse=True)]\n",
    "        for l in lines:\n",
    "            with open(\"data/cedr/valid%d.tsv\" % fold, 'a') as fd:\n",
    "                fd.write(l)\n",
    "                \n",
    "    for idx, group in df[df.query_id.isin(testId5fold[fold])].groupby(\"query_id\"):\n",
    "        pos = group['property'].tolist()\n",
    "        neg = list(set.difference(set(type2allprop[group.iloc[0]['entityType']]), set(pos)))\n",
    "\n",
    "        lines = {}\n",
    "        for idx, row in group.iterrows():\n",
    "            lines[str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[row['property']])+\"\\t0\\t\" +str(row['rele_label'])+\"\\trun\\n\"] = row['rele_label']\n",
    "\n",
    "        for n in neg:\n",
    "            lines[str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[n])+\"\\t0\\t0\\trun\\n\"] = 0\n",
    "        lines = [i[0] for i in sorted(lines.items(), key=lambda x: x[1], reverse=True)]\n",
    "        for l in lines:\n",
    "            with open(\"data/cedr/test%d.tsv\" % fold, 'a') as fd:\n",
    "                fd.write(l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test\n",
    "for idx, group in df[df.query_id.isin(trainIds)].groupby(\"query_id\"):\n",
    "    pos = group['property'].tolist()\n",
    "    neg = list(set.difference(set(type2allprop[group.iloc[0]['entityType']]), set(pos)))\n",
    "\n",
    "    for idx, row in group.iterrows():\n",
    "        with open(\"data/cedr/train.tsv\", 'a') as fd:\n",
    "            fd.write(str(row[\"query_id\"]) + \"\\t\"+str(row[\"pid\"])+\"\\n\")\n",
    "    for n in neg:\n",
    "        with open(\"data/cedr/train.tsv\", 'a') as fd:\n",
    "            fd.write(str(row[\"query_id\"]) + \"\\t\"+str(prop2pid[n])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, group in df[df.query_id.isin(testIds)].groupby(\"query_id\"):\n",
    "    pos = group['property'].tolist()\n",
    "    neg = list(set.difference(set(type2allprop[group.iloc[0]['entityType']]), set(pos)))\n",
    "\n",
    "    lines = {}\n",
    "    for idx, row in group.iterrows():\n",
    "#         with open(\"data/cedr/test.run\", 'a') as fd:\n",
    "#             fd.write(str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[n])+\"\\t0\\t\" +str(row['rele_label'])+\"\\trun\\n\")\n",
    "        lines[str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[row['property']])+\"\\t0\\t\" +str(row['rele_label'])+\"\\trun\\n\"] = row['rele_label']\n",
    "            \n",
    "    for n in neg:\n",
    "#         with open(\"data/cedr/test.run\", 'a') as fd:\n",
    "        lines[str(row[\"query_id\"]) + \"\\tQ0\\t\"+str(prop2pid[n])+\"\\t0\\t0\\trun\\n\"] = 0\n",
    "    lines = [i[0] for i in sorted(lines.items(), key=lambda x: x[1], reverse=True)]\n",
    "    for l in lines:\n",
    "        with open(\"data/cedr/test.tsv\", 'a') as fd:\n",
    "            fd.write(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 27)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.query_id == 71]), len(type2allprop2[df[df.query_id == 71].iloc[0].entityType])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229 20 0\n"
     ]
    }
   ],
   "source": [
    "c, c1, c2 = 0, 0, 0\n",
    "for idx, group in df.groupby(\"query_id\"):\n",
    "    pos = group.property.tolist()\n",
    "    neg1 = list(set.difference(set(type2allprop[group.iloc[0]['entityType']]), set(pos)))\n",
    "    neg2 = list(set.difference(set(type2allprop2[group.iloc[0]['entityType']]), set(pos)))\n",
    "    if len(neg1) == 0:\n",
    "        c1 += 1\n",
    "    if len(neg2) == 0:\n",
    "        c2 += 1\n",
    "    c += 1\n",
    "print(c, c1, c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling import BertRanker\n",
    "from Data import _pad_crop, _mask\n",
    "from modeling_util import SimmatModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "br = BertRanker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get(query, doc):\n",
    "    _query_token = br.tokenize(query)\n",
    "    _doc_token = br.tokenize(doc)\n",
    "    query_token = _pad_crop([_query_token], 5)\n",
    "    doc_token = _pad_crop([_doc_token], 5)\n",
    "    query_mask = _mask([_query_token], 5)\n",
    "    doc_mask = _mask([_doc_token], 5)\n",
    "    return br.encode_bert(query_token, query_mask, doc_token, doc_mask)\n",
    "\n",
    "get(\"have benchmarking\", \"startTime\")[1][12].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def bertSim(query, doc):\n",
    "    QLEN = DLEN = 20\n",
    "    query = br.tokenize(query)\n",
    "    doc = br.tokenize(doc)\n",
    "    query_tok = _pad_crop([query], QLEN)\n",
    "    doc_tok = _pad_crop([doc], DLEN)\n",
    "    query_mask = _mask([query], QLEN)\n",
    "    doc_mask =_mask([doc], DLEN)\n",
    "    c, q, d = br.encode_bert(query_tok, query_mask, doc_tok, doc_mask)\n",
    "    return 1 - cosine(torch.mean(q[-1][0], dim=0).detach().numpy(), torch.mean(d[-1][0], dim=0).detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    print(row['query_id'])\n",
    "    bertSim(row['entity'], row['property']), row['rele_label']\n",
    "    print(1 - cosine()\n",
    ")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5887700534759358\t0.5579916815210934\t0.5498811645870469\t0.5370915032679738\t0.2023824341397186\t0.37383273408613293\t0.5205524147829946\t0.6563343661279278\t0.41748150210732166\t0.46722664435583194\t0.5077280078477923\t0.5583809990552392\t0.5483985055151065\t0.5730014618779197\t0.57668781331345\t0.5775744590750268\t0.5401313764702022\t0.5732620320855615\t0.5561200237670827\t0.5415032679738562\t0.5312091503267974\t0.19533384738903942\t0.373239288499416\t0.5152811387331357\t0.6512255028340639\t0.40852621914075843\t0.46013109212750586\t0.4984495450441254\t0.549658459665293\t0.5485529136943489\t0.5740434585853085\t0.578324970616467\t0.5791509041302285\t0.5332822412411633\t"
     ]
    }
   ],
   "source": [
    "def getBertVec(text):\n",
    "    \n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    \n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    # Predict hidden states features for each layer\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "        \n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "        \n",
    "#       second to last\n",
    "        token_vecs = encoded_layers[0][0]\n",
    "#         token_vecs = torch.squeeze(torch.sum(torch.stack(emb), dim=0), dim=0)\n",
    "\n",
    "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "        return sentence_embedding.detach().numpy()\n",
    "def bertSim2(query, doc):\n",
    "    return 1 - cosine(getBertVec(query), getBertVec(doc))\n",
    "\n",
    "# for idx, row in df.iterrows():\n",
    "#     print(row['query_id'])\n",
    "#     print(bertSim2(row['action'] +\" \" +row['entity'], row['property']), row['rele_label'])\n",
    "\n",
    "#     for col in [\"title\", \"overview\", \"sectionLabel\", \"headline\", \"text\"]:\n",
    "for col in [3,4]:\n",
    "    res, res2, res3 = [], [], []\n",
    "    for fold in range(5):\n",
    "        _df = pd.read_csv(\"data/cedr/test%d.tsv\" %fold, sep=\"\\t\", usecols=[0], names=[\"qid\"])\n",
    "        testIds = _df.qid.unique()\n",
    "\n",
    "        for idx, row in df[['query_id', 'entity', 'action', 'entityType']].drop_duplicates().iterrows():\n",
    "            if row['query_id'] not in testIds:\n",
    "                continue\n",
    "#             print(fold, row['query_id'])\n",
    "            qrels = qrel[str(row['query_id'])]\n",
    "            cand_properties = type2prop[row['entityType']]\n",
    "\n",
    "            rank = {}\n",
    "            doc = bm25.get_top_n(tokenizer.tokenize(row['action'] +\" \" +row['entity']), corpus, n=1)[0].split(\"\\t\")[col][:500]\n",
    "\n",
    "            for p in cand_properties:\n",
    "    #             rank[p] = bertSim2(row['action'] +\" \" +row['entity'], p)\n",
    "                rank[p] = bertSim2(doc, p)\n",
    "    #             print(rank[p])\n",
    "            rank = [i[0] for i in sorted(rank.items(), key=lambda x: x[1], reverse=True)]\n",
    "            our = evaluate(qrels, rank)\n",
    "\n",
    "            res.append(our)\n",
    "\n",
    "    keys = {\"%s@%d\" %( i,j): [] for i in [\"p\", \"r\", \"ndcg\", \"nerr\"] for j in [5, 10 ,15, 20]}\n",
    "    keys[\"rp\"] = []\n",
    "    for r in [res]:\n",
    "        for k in keys:\n",
    "            print(np.mean([i[k] for i in r]), end=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entity\n",
       "abraham lincoln          [children, gender, memberOf, hasOfferCatalog, ...\n",
       "acapulco                 [image, logo, event, telephone, review, contai...\n",
       "acupuncture              [relevantSpeciality, anatomicalSystem, medical...\n",
       "adidas                   [globalLocationNumber, name, brand, makesOffer...\n",
       "alan turing              [name, deathPlace, potentialAction, knows, hei...\n",
       "all about my mother      [recordedAt, musicBy, character, workExample, ...\n",
       "angola                   [geo, description, telephone, containsPlace, a...\n",
       "anti-gravity             [potentialAction, additionalType, description,...\n",
       "aol                      [location, numberOfEmployees, description, add...\n",
       "apgar score              [image, potentialAction, description, code, st...\n",
       "apple cider              [isSimilarTo, productionDate, logo, purchaseDa...\n",
       "aristotle                [workLocation, relatedTo, image, nationality, ...\n",
       "asthma                   [associatedAnatomy, primaryPrevention, descrip...\n",
       "automotive battery       [weight, manufacturer, productionDate, model, ...\n",
       "baby shower              [offers, location, identifier, translator, per...\n",
       "bacardi                  [name, address, hasOfferCatalog, logo, award, ...\n",
       "back pain                [guideline, description, cause, possibleCompli...\n",
       "back to the future       [accountablePerson, commentCount, alternativeH...\n",
       "banana bread             [image, purchaseDate, offers, itemCondition, i...\n",
       "barbecue sauce           [isConsumableFor, review, manufacturer, name, ...\n",
       "barbie hsu               [award, performerIn, workLocation, seeks, nati...\n",
       "batman                   [nationality, knows, seeks, potentialAction, s...\n",
       "beetroot                 [category, color, isConsumableFor, identifier,...\n",
       "belief                   [description, additionalType, potentialAction,...\n",
       "benchmarking             [startTime, result, target, name, actionStatus...\n",
       "bibliography             [image, name, identifier, description, potenti...\n",
       "big oil                  [telephone, taxID, review, makesOffer, memberO...\n",
       "blood type               [relevantSpeciality, guideline, study, legalSt...\n",
       "bodybuilding             [potentialAction, participant, startTime, endT...\n",
       "boric acid               [itemCondition, brand, manufacturer, image, is...\n",
       "                                               ...                        \n",
       "tehran                   [telephone, hasMap, description, review, event...\n",
       "terms of endearment      [award, name, actor, educationalUse, releasedE...\n",
       "testosterone             [study, maximumIntake, potentialAction, name, ...\n",
       "the devil wears prada    [version, publisher, isBasedOn, name, director...\n",
       "the exodus               [duration, location, performer, potentialActio...\n",
       "the incredible hulk      [keywords, producer, text, character, trailer,...\n",
       "the paper                [video, review, producer, position, provider, ...\n",
       "the pyramid              [material, contentLocation, educationalUse, te...\n",
       "the sixth sense          [isBasedOn, character, audience, genre, mentio...\n",
       "the tall man             [version, author, musicBy, name, recordedAt, c...\n",
       "the wizard of oz         [timeRequired, countryOfOrigin, award, video, ...\n",
       "thyroid hormone          [guideline, mainEntityOfPage, medicineSystem, ...\n",
       "times square             [maximumAttendeeCapacity, specialOpeningHoursS...\n",
       "tink                     [birthPlace, nationality, sponsor, familyName,...\n",
       "tiscali                  [taxID, faxNumber, name, foundingDate, memberO...\n",
       "today                    [isBasedOn, image, version, award, position, m...\n",
       "tony robbins             [nationality, colleague, owns, familyName, nam...\n",
       "translation              [result, image, name, disambiguatingDescriptio...\n",
       "trichinosis              [potentialAction, medicineSystem, relevantSpec...\n",
       "trinidad and tobago      [hasMap, containedInPlace, globalLocationNumbe...\n",
       "trust me                 [review, position, producer, award, timeRequir...\n",
       "university of houston    [identifier, location, employee, event, areaSe...\n",
       "urinary incontinence     [secondaryPrevention, epidemiology, status, na...\n",
       "vin diesel               [jobTitle, knows, worksFor, image, award, work...\n",
       "visa inc.                [review, logo, faxNumber, employee, telephone,...\n",
       "voip phone               [serviceType, areaServed, serviceOutput, categ...\n",
       "volkswagen passat        [material, image, vehicleInteriorType, vehicle...\n",
       "walmart                  [email, numberOfEmployees, telephone, award, r...\n",
       "west indies              [faxNumber, address, containsPlace, name, maxi...\n",
       "world war ii             [description, actor, endDate, inLanguage, audi...\n",
       "Name: property, Length: 229, dtype: object"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"entity\")[\"property\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'overclock graphics card'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1307, -0.2163, -0.2279,  ...,  0.0138,  0.1159,  0.0976],\n",
      "         [ 2.2972, -0.5218, -0.3770,  ...,  0.7855,  0.5923, -4.9415],\n",
      "         [-0.7612,  0.3292,  0.2475,  ...,  0.3704,  1.8085,  0.0186]]])\n"
     ]
    }
   ],
   "source": [
    "def getBertVec(text):\n",
    "    \n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    \n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    # Predict hidden states features for each layer\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "        \n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "        print(encoded_layers[0] + encoded_layers[1])\n",
    "        \n",
    "#       sum\n",
    "#         token_vecs = torch.sum(encoded_layers, dim=0)[0]\n",
    "#         token_vecs = encoded_layers[-1][0]\n",
    "        token_vecs = torch.sum(torch.stack(emb), dim=0)\n",
    "    \n",
    "#         print(token_vecs.size())\n",
    "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "        return sentence_embedding.detach().numpy(), encoded_layers\n",
    "# _, emb = getBertVec(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1, 3, 768])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 768])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.read_csv(\"data/cedr/question-qq.tsv\", names=[\"q\", \"qid\", \"text\"], sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q</th>\n",
       "      <th>qid</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "      <td>What is internal benchmarking and external ben...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>question</td>\n",
       "      <td>2</td>\n",
       "      <td>Is it OK to do push ups every day? When should...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>question</td>\n",
       "      <td>3</td>\n",
       "      <td>What are the 3 types of brainstorming? What ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>question</td>\n",
       "      <td>4</td>\n",
       "      <td>How many cigarettes does a chain smoker smoke ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>question</td>\n",
       "      <td>5</td>\n",
       "      <td>How do I apply for a charitable grant? What is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          q  qid                                               text\n",
       "0  question    1  What is internal benchmarking and external ben...\n",
       "1  question    2  Is it OK to do push ups every day? When should...\n",
       "2  question    3  What are the 3 types of brainstorming? What ar...\n",
       "3  question    4  How many cigarettes does a chain smoker smoke ...\n",
       "4  question    5  How do I apply for a charitable grant? What is..."
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = []\n",
    "for idx, row in _df.iterrows():\n",
    "    new_texts.append(preprocessingText(row.text, True))\n",
    "_df[\"text\"] = new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = []\n",
    "for idx, row in _df.iterrows():\n",
    "    new_texts.append(\"?\".join(row.text.split(\"?\")[:1]))\n",
    "_df[\"text\"] = new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _df.to_csv(\"data/cedr/question-qq3-nostopword.tsv\", header=False, index=False, sep=\"\\t\")\n",
    "_df.to_csv(\"data/cedr/question-qq1.tsv\", header=False, index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_crawl = [\" \".join(i) for i in df[['action', 'entity']].drop_duplicates().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_scraper import MultiThreadScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_crawl = ['serve barbecue sauce', 'bake banana bread']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:00<00:00, 29.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//div[@class='kno-ftr']//div/following-sibling::a[text()='Feedback']\"}\n",
      "  (Session info: headless chrome=79.0.3945.130)\n",
      "\n",
      "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//div[@class='kno-ftr']//div/following-sibling::a[text()='Feedback']\"}\n",
      "  (Session info: headless chrome=79.0.3945.130)\n",
      "\n",
      "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//div[@class='kno-ftr']//div/following-sibling::a[text()='Feedback']\"}\n",
      "  (Session info: headless chrome=79.0.3945.130)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = MultiThreadScraper(to_crawl, \"data/questions/query2/\", \"fail\")\n",
    "s.run_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 229/229 [38:11<00:00, 10.01s/it]\n"
     ]
    }
   ],
   "source": [
    "title_to_crawl = []\n",
    "for query in to_crawl:\n",
    "    title = df_bm25[df_bm25.text.str.contains(query[:10])].text.values[0].replace(query+\" \", \"\")\n",
    "    title_to_crawl.append(title)\n",
    "s = MultiThreadScraper(title_to_crawl, \"data/questions/wiki/\", \"fail\")\n",
    "s.run_scraper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "notfound = []\n",
    "bugs = {\"Lockheed_C-5_Galaxy\":\"Lockheed C-5 Galaxy\", \"IMac\":\"IMac computer\", \"Diet_Coke\": \"Diet_Coke drink\", \"Copper\": \"Copper science\", \"Prison_Break_(season_2)\": \"Prison Break (season 2)\", \"Showtime_%28film%29\":\"Showtime film\", \"The_Wizard_of_Oz_%281939_film%29\": \"The Wizard of Oz film\", \"The_Pyramid_%28film%29\":\"The Pyramid film\", \"Prom_Night_%281980_film%29\":\"Prom Night film\", \"Law_%26_Order%3A_Special_Victims_Unit\": \"Law Order Special film\", \"Mary_Poppins_%28film%29\": \"Mary Poppins film\", \"Shock_%28circulatory%29\": \"Shock circulatory\", \"DNA\": \"DNA science\", \"Skin\": \"Skin science\", \"BP\":\"BP company\", \"Jason_Kidd\": \"Jason Kidd person\", \"Jean-Claude_Van_Damme\": \"Jean Claude Van Damme\", \"Alberta\": \"Alberta place\", \"Himalayas\": \"Himalayas place\", \"Kings_Island\":\"KingsIsland\", \"Mount_Everest\": \"MountEverest\", \"Predator_%28film%29\": \"Predator_%28film%29\", \"Braveheart\": \"Braveheart film\", \"Brainstorming\": \"Brain storming\", \"Give_Me_Five!\": \"Give_Me_Five! music\"}\n",
    "for idx, row in df[['query_id', 'url', 'entityType']].drop_duplicates().iterrows():\n",
    "    try:\n",
    "        if row['url'] in bugs:\n",
    "            with open(\"data/cedr/wikipedia.tsv\", 'a') as fd:\n",
    "                fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+preprocessingText(wikipedia.summary(urllib.parse.unquote(bugs[row['url']])))+\"\\n\")\n",
    "        else:\n",
    "            with open(\"data/cedr/wikipedia.tsv\", 'a') as fd:\n",
    "                fd.write(\"query\\t\"+str(row['query_id'])+\"\\t\"+preprocessingText(wikipedia.summary(urllib.parse.unquote(row['url'])))+\"\\n\")\n",
    "    except:\n",
    "        print(row[\"url\"])\n",
    "        notfound.append((row['query_id'], row['url'], row['entityType']))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in notfound:\n",
    "    if i[1] not in bugs:\n",
    "        print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youth\n"
     ]
    }
   ],
   "source": [
    "all_categories = \"Arts and Entertainment·Cars & Other Vehicles·Computers and Electronics·Education and Communications·Family Life·Finance and Business·Food and Entertaining·Health·Hobbies and Crafts·Holidays and Traditions·Home and Garden·Personal Care and Style·Pets and Animals·Philosophy and Religion·Relationships·Sports and Fitness·Travel·Work World·Youth\"\n",
    "all_categories = all_categories.lower()\n",
    "all_categories = all_categories.split(\"·\")\n",
    "\n",
    "df = pd.read_csv(\"data/wikihowSep.csv\")\n",
    "df = df[df.sectionLabel != \"Steps\"]\n",
    "df[\"title\"] = df[\"title\"].str.lower()\n",
    "df[\"sectionLabel\"] = df[\"sectionLabel\"].str.lower()\n",
    "\n",
    "wikiCat = pd.read_csv(\"data/cate.csv\", sep=\",\", error_bad_lines=False, names=[\"title\", \"category\"])\n",
    "wikiCat['title'] = wikiCat['title'].str.replace(\"https://www.wikihow.com/\", \"\").str.replace(\"%22\", \"\").str.replace(\n",
    "    \"-\", \" \").str.lower()\n",
    "wikiCat['title'] = [\"how to \" + i for i in wikiCat['title'].tolist()]\n",
    "wikiCat['category'] = wikiCat['category'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arts and entertainment',\n",
       " 'cars & other vehicles',\n",
       " 'computers and electronics',\n",
       " 'education and communications',\n",
       " 'family life',\n",
       " 'finance and business',\n",
       " 'food and entertaining',\n",
       " 'health',\n",
       " 'hobbies and crafts',\n",
       " 'holidays and traditions',\n",
       " 'home and garden',\n",
       " 'personal care and style',\n",
       " 'pets and animals',\n",
       " 'philosophy and religion',\n",
       " 'relationships',\n",
       " 'sports and fitness',\n",
       " 'travel',\n",
       " 'work world',\n",
       " 'youth']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = \"sports and fitness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = wikiCat[wikiCat.category.str.contains(cat)].title.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniformly Sample negative queries from any task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youth\n",
      "4018\n"
     ]
    }
   ],
   "source": [
    "for cat in all_categories:\n",
    "    tasks, queries = [], []\n",
    "    cat = cat.replace(\" \", \"_\")\n",
    "    print(cat)\n",
    "    folder = \"csv/%s/\" % cat\n",
    "    files = []\n",
    "    for file in os.listdir(folder):\n",
    "        if os.stat(folder + file).st_size > 0:\n",
    "            if \".csv\" in file:\n",
    "                task_name = file.split(\".csv\")[0].replace(\"_\", \" \")\n",
    "                tmp = pd.read_csv(\"csv/%s/%s\" %(cat,file), names=[\"q\", \"qq\"], sep=\";\", header=0)\n",
    "                try:\n",
    "                    query = tmp.q.str.lower().str.replace(\"?\", \"\").unique().tolist() + tmp.qq.str.lower().str.replace(\"?\", \"\").unique().tolist()\n",
    "                except:\n",
    "#                   no sub queries\n",
    "                    query = tmp.q.str.lower().str.replace(\"?\", \"\").unique().tolist()\n",
    "                queries.extend(query)\n",
    "                tasks.extend([task_name]*len(query))\n",
    "                files.append(task_name)\n",
    "    print(len(files))\n",
    "    break\n",
    "            \n",
    "#     #   generate query-task pair dataset\n",
    "#     x_task, x_query, label = [], [], []\n",
    "#     for t, pq, nq in zip(tasks, queries, queries[::-1]):\n",
    "#         x_query.append(pq)\n",
    "#         x_query.append(nq)\n",
    "#         x_task.extend([t, t])\n",
    "#         label.extend([1,0])\n",
    "#     _df = pd.DataFrame.from_dict({'task': x_task, \"query\": x_query, \"label\": label})\n",
    "#     _df.to_csv(\"data/query_task_prediction/%s.csv\" % cat, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample negative queries within the same task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arts_and_entertainment\n",
      "cars_&_other_vehicles\n",
      "computers_and_electronics\n",
      "education_and_communications\n",
      "family_life\n",
      "finance_and_business\n"
     ]
    }
   ],
   "source": [
    "def read_scraper_file(df):\n",
    "    try:\n",
    "        query = df.q.str.lower().str.replace(\"?\", \"\").unique().tolist() + df.qq.str.lower().str.replace(\"?\", \"\").unique().tolist()\n",
    "    except:\n",
    "#       no sub queries\n",
    "        query = df.q.str.lower().str.replace(\"?\", \"\").unique().tolist()\n",
    "    return query\n",
    "\n",
    "for cat in all_categories:\n",
    "    tasks = wikiCat[wikiCat.category.str.contains(cat)].title.tolist()\n",
    "    sdf = df[df.title.isin(tasks)].drop_duplicates([\"sectionLabel\"])\n",
    "    subtasks = df[df.title.isin(tasks)].drop_duplicates([\"sectionLabel\"]).sectionLabel.unique().tolist()\n",
    "\n",
    "    cat = cat.replace(\" \", \"_\")\n",
    "    print(cat)\n",
    "    folder = \"csv/%s/\" % cat\n",
    "    scraper_subtasks = []\n",
    "    for file in os.listdir(folder):\n",
    "        if os.stat(folder + file).st_size > 0:\n",
    "            if \".csv\" in file:\n",
    "                task_name = file.split(\".csv\")[0].replace(\"_\", \" \")\n",
    "#                 print(task_name)\n",
    "                if task_name in subtasks:\n",
    "                    scraper_subtasks.append(task_name)\n",
    "            \n",
    "    x_task, x_query, label = [], [], []\n",
    "    for  name, row in sdf.groupby(\"title\"):\n",
    "        check = set.intersection(set(row.sectionLabel.tolist()), set(scraper_subtasks))\n",
    "        if len(check) > 1:\n",
    "            check = list(check)\n",
    "            for i in range(len(check)):\n",
    "                subtask_name = check[i]\n",
    "                _df = pd.read_csv(\"csv/%s/%s.csv\" %(cat,subtask_name.replace(\" \", \"_\")), names=[\"q\", \"qq\"], sep=\";\", header=0)\n",
    "                pos_query = read_scraper_file(_df)\n",
    "\n",
    "                neg_query_pool = []\n",
    "                for _subtask_name in check[:i] + check[i+1:]:\n",
    "                    _df = pd.read_csv(\"csv/%s/%s.csv\" %(cat, _subtask_name.replace(\" \", \"_\")), names=[\"q\", \"qq\"], sep=\";\", header=0)\n",
    "                    neg_query_pool.extend(read_scraper_file(_df))\n",
    "                \n",
    "                if len(pos_query) <= len(neg_query_pool):\n",
    "                    neg_query = random.sample(neg_query_pool, len(pos_query))\n",
    "                else:\n",
    "                    neg_query = []\n",
    "                    while len(neg_query) < len(pos_query):\n",
    "                        \n",
    "                        neg_query.append(random.sample(neg_query_pool, 1))\n",
    "#                     print(len(neg_query_pool), len(pos_query))\n",
    "#                     neg_query = neg_query_pool + random.sample(neg_query_pool, len(pos_query) - len(neg_query_pool))\n",
    "                \n",
    "                for pq, nq in zip(pos_query, neg_query):\n",
    "                    x_query.append(pq)\n",
    "                    x_query.append(nq)\n",
    "                    x_task.extend([subtask_name, subtask_name])\n",
    "                    label.extend([1,0])\n",
    "    _df = pd.DataFrame.from_dict({'task': x_task, \"query\": x_query, \"label\": label})\n",
    "    _df.to_csv(\"data/query_task_prediction/same_task/%s.csv\" % cat, index=False, header=False)\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
